{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx1RbCcjCpuU"
      },
      "source": [
        "# FLAIR BASICS\n",
        "\n",
        "Check the documentation and tutorials: \n",
        "\n",
        "https://github.com/flairNLP/flair/tree/master/resources/docs\n",
        "\n",
        "Flair is a state of the art neural toolkit to perform sequence labelling and text classification.\n",
        "\n",
        "The aim of this lab is to learn how to install Flair, understand the intuitions about the character-based contextual word representations and getting familiar with its API, which is built around the Token and Sentence objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQCUHxFt3GYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73a3d293-adbc-4fa8-ad5e-fc444bdb93aa"
      },
      "source": [
        "!pip install flair"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: flair in /usr/local/lib/python3.8/dist-packages (0.11.3)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from flair) (1.13.1+cu116)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.8/dist-packages (from flair) (1.5.11)\n",
            "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.8/dist-packages (from flair) (0.3)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.8/dist-packages (from flair) (3.2.2)\n",
            "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from flair) (0.3.4)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.8/dist-packages (from flair) (6.1.1)\n",
            "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.8/dist-packages (from flair) (1.2.13)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from flair) (2022.6.2)\n",
            "Requirement already satisfied: gdown==4.4.0 in /usr/local/lib/python3.8/dist-packages (from flair) (4.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from flair) (0.8.10)\n",
            "Requirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from flair) (4.26.1)\n",
            "Requirement already satisfied: pptree in /usr/local/lib/python3.8/dist-packages (from flair) (3.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from flair) (9.0.0)\n",
            "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from flair) (4.6.5)\n",
            "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from flair) (2.1.0)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.8/dist-packages (from flair) (1.0.9)\n",
            "Requirement already satisfied: sentencepiece==0.1.95 in /usr/local/lib/python3.8/dist-packages (from flair) (0.1.95)\n",
            "Requirement already satisfied: hyperopt>=0.2.7 in /usr/local/lib/python3.8/dist-packages (from flair) (0.2.7)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from flair) (4.9.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.8/dist-packages (from flair) (2.8.2)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.8/dist-packages (from flair) (4.64.1)\n",
            "Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.8/dist-packages (from flair) (0.5.8)\n",
            "Requirement already satisfied: janome in /usr/local/lib/python3.8/dist-packages (from flair) (0.4.2)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.8/dist-packages (from flair) (0.12.0)\n",
            "Requirement already satisfied: conllu>=4.0 in /usr/local/lib/python3.8/dist-packages (from flair) (4.5.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from flair) (1.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair) (3.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair) (2.25.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from bpemb>=0.3.2->flair) (1.21.6)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from deprecated>=1.2.4->flair) (1.14.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.4.0->flair) (6.3.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.4.0->flair) (1.7.3)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair) (3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair) (0.16.0)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair) (0.10.9.7)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair) (2.2.1)\n",
            "Requirement already satisfied: importlib-metadata<4.0.0,>=3.7.0 in /usr/local/lib/python3.8/dist-packages (from konoha<5.0.0,>=4.0.0->flair) (3.10.1)\n",
            "Requirement already satisfied: overrides<4.0.0,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from konoha<5.0.0,>=4.0.0->flair) (3.1.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->flair) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->flair) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch!=1.8,>=1.5.0->flair) (4.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.0.0->flair) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.0.0->flair) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.0.0->flair) (0.13.2)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->flair) (0.2.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.12.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.4.0->flair) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.4.0->flair) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.4.0->flair) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.4.0->flair) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.4.0->flair) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIZTbJw_7RNE"
      },
      "source": [
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "from flair.models import TextClassifier"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOK_QbPH7bjL"
      },
      "source": [
        "# \"use_tokenizer\" parameter for tokenizing the input text\n",
        "\n",
        "sentence = Sentence('Washington University, which is located in Missouri, is named after George Washington.', use_tokenizer=False)\n",
        "tokenized_sentence = Sentence('Washington University, which is located in Missouri, is named after George Washington.', use_tokenizer=True)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsoP6Y1lfVm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba3208c0-4bf7-4ed9-d73f-f2403988e500"
      },
      "source": [
        "print(sentence)\n",
        "print(tokenized_sentence)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: \"Washington University, which is located in Missouri, is named after George Washington.\"\n",
            "Sentence: \"Washington University , which is located in Missouri , is named after George Washington .\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feAJDSkZg_ld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07053d2b-186c-4c7d-829c-a0d2d9a154e5"
      },
      "source": [
        "## get_token() function retrieves the token by index (starting from 1)\n",
        "print(sentence.get_token(3))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token[2]: \"which\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmQ1bKIthTEd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db6e7c1b-67a3-45f8-8d3d-a8a3b34f46b0"
      },
      "source": [
        "## indexes to obtain the tokens (starting from 0)\n",
        "print(sentence[2])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token[2]: \"which\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM53HBb9hkx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84fe9436-f999-42a8-adee-40a3c8ea876d"
      },
      "source": [
        "for token in sentence:\n",
        "  print(token)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token[0]: \"Washington\"\n",
            "Token[1]: \"University,\"\n",
            "Token[2]: \"which\"\n",
            "Token[3]: \"is\"\n",
            "Token[4]: \"located\"\n",
            "Token[5]: \"in\"\n",
            "Token[6]: \"Missouri,\"\n",
            "Token[7]: \"is\"\n",
            "Token[8]: \"named\"\n",
            "Token[9]: \"after\"\n",
            "Token[10]: \"George\"\n",
            "Token[11]: \"Washington.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awDYkCnGENuD"
      },
      "source": [
        "# WORD REPRESENTATIONS\n",
        "\n",
        "1. Static Word Embeddings (fastText, Glove, etc.)\n",
        "2. Flair character-based contextual embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmDWPSl4phOQ"
      },
      "source": [
        "from flair.embeddings import WordEmbeddings\n",
        "\n",
        "# init embedding\n",
        "en_embedding = WordEmbeddings('glove')"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTWM9876qHoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07a9e9be-e533-4ee4-b0d4-fe28354c617c"
      },
      "source": [
        "#sentence = Sentence('Washington University, which is located in Missouri, is named after George Washington.')\n",
        "\n",
        "# Obtain vector-based representation from glove pre-trained model\n",
        "en_embedding.embed(sentence)\n",
        "\n",
        "# print the vector representing each word in the sentence\n",
        "for token in sentence:\n",
        "    print(token)\n",
        "    print(token.embedding)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token[0]: \"Washington\"\n",
            "tensor([-2.2048e-01, -1.1316e-01,  9.4277e-01, -3.9024e-01,  2.5004e-01,\n",
            "        -4.1651e-01, -1.4640e-01,  2.3628e-03, -1.2966e-01, -1.1173e-01,\n",
            "        -2.1546e-01, -8.6271e-01,  1.3817e-01,  3.3118e-01, -6.6500e-01,\n",
            "         3.7134e-01,  2.0050e-01, -3.4055e-01, -1.2422e+00, -7.6653e-01,\n",
            "        -1.1253e-02,  3.8440e-01, -5.0105e-02, -1.8869e-01,  1.0785e-01,\n",
            "         1.7502e-01, -1.0167e-01, -5.7925e-01,  2.3529e-01,  3.2626e-02,\n",
            "         3.2353e-01,  9.7457e-01,  4.5231e-01,  4.9740e-01, -8.8874e-01,\n",
            "         4.9170e-01,  1.1230e-01, -2.1484e-01,  9.3187e-02,  4.7039e-01,\n",
            "        -7.8776e-01, -6.8219e-01, -2.3741e-01,  2.2351e-01,  2.0269e-01,\n",
            "        -1.0166e+00,  1.3095e-01, -2.3654e-01,  3.1501e-01, -3.1880e-01,\n",
            "         5.9744e-01, -2.8722e-01,  2.9970e-01,  3.4877e-01, -1.6597e-01,\n",
            "        -2.8483e+00,  3.2219e-01, -7.8469e-01,  1.3754e+00,  1.5050e-01,\n",
            "        -8.5193e-01,  2.5303e-01,  2.0142e-01, -5.9176e-01,  8.9212e-02,\n",
            "        -3.5561e-01,  2.6522e-01,  1.1283e+00, -3.7250e-01,  9.4010e-01,\n",
            "        -5.2708e-01, -4.6361e-01, -8.1034e-01, -3.3479e-01,  1.0260e-01,\n",
            "        -4.1905e-01,  6.3775e-01,  4.5096e-02, -1.2385e+00,  2.1903e-01,\n",
            "         4.9145e-01,  3.6583e-01, -3.2024e-01, -5.1061e-02, -6.5049e-01,\n",
            "        -1.1894e-02,  2.0887e-01,  2.5857e-01, -2.3303e-01, -8.4911e-01,\n",
            "        -1.5466e-01, -3.3714e-01, -3.7272e-01, -2.9143e-01, -1.2799e+00,\n",
            "         2.8781e-01,  2.9473e-01, -3.2733e-01,  6.3016e-01,  3.6249e-01],\n",
            "       device='cuda:0')\n",
            "Token[1]: \"University,\"\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "Token[2]: \"which\"\n",
            "tensor([ 3.0240e-02,  4.4606e-01,  4.3166e-01, -3.7528e-01,  2.9068e-01,\n",
            "         2.3032e-01,  1.8125e-01,  4.0201e-01,  1.3518e-01, -1.9562e-01,\n",
            "         3.0639e-01, -1.3239e-01,  6.7897e-01,  4.2234e-01,  3.2637e-01,\n",
            "        -1.5281e-01,  3.7698e-01, -2.3303e-01, -3.3817e-01,  3.0588e-01,\n",
            "         4.4918e-01, -8.3624e-01,  5.9146e-01,  2.4958e-01,  3.9986e-01,\n",
            "        -5.0172e-01, -2.3544e-01, -1.4696e-01, -3.5144e-01, -5.6852e-01,\n",
            "         8.9540e-02,  8.2612e-01, -2.6586e-01,  3.9030e-01, -3.6849e-02,\n",
            "         4.8257e-01,  7.1664e-01,  1.1004e-01, -5.9354e-01, -3.3216e-01,\n",
            "        -2.5736e-01, -3.4531e-01, -2.6326e-02, -2.3747e-01,  1.9656e-04,\n",
            "        -2.7480e-01,  3.8512e-01, -3.9581e-01,  1.1404e-01, -2.5174e-01,\n",
            "        -3.2470e-01,  8.9608e-02,  2.4929e-01,  1.5127e+00, -1.9762e-01,\n",
            "        -2.8509e+00, -5.3833e-01, -4.7111e-01,  1.7859e+00,  7.8126e-01,\n",
            "        -1.2963e-01,  5.6077e-01,  3.2151e-01,  3.5571e-01,  8.4547e-01,\n",
            "         1.4931e-01,  1.1487e-01,  3.0625e-01,  5.4774e-01, -5.0426e-01,\n",
            "         3.3824e-01, -6.2043e-01, -1.2869e-02,  6.6666e-02,  6.2731e-02,\n",
            "        -4.4534e-01,  1.5541e-01,  2.1801e-01, -1.7320e+00,  4.2054e-01,\n",
            "         3.6319e-01, -7.2258e-02, -7.4811e-01,  1.9888e-01, -1.4461e+00,\n",
            "        -2.7576e-01,  2.6646e-01, -5.7838e-01,  5.6151e-01, -2.8701e-02,\n",
            "        -2.4660e-01, -4.2500e-01, -5.7154e-01,  3.1939e-01, -2.2075e-01,\n",
            "         4.6528e-01, -1.6606e-01, -7.9923e-01,  8.0849e-01,  3.7378e-01],\n",
            "       device='cuda:0')\n",
            "Token[3]: \"is\"\n",
            "tensor([-0.5426,  0.4148,  1.0322, -0.4024,  0.4669,  0.2182, -0.0749,  0.4733,\n",
            "         0.0810, -0.2208, -0.1281, -0.1144,  0.5089,  0.1157,  0.0282, -0.3628,\n",
            "         0.4382,  0.0475,  0.2028,  0.4986, -0.1007,  0.1327,  0.1697,  0.1165,\n",
            "         0.3135,  0.2571,  0.0928, -0.5683, -0.5297, -0.0515, -0.6733,  0.9253,\n",
            "         0.2693,  0.2273,  0.6636,  0.2622,  0.1972,  0.2609,  0.1877, -0.3454,\n",
            "        -0.4263,  0.1398,  0.5634, -0.5691,  0.1240, -0.1289,  0.7248, -0.2610,\n",
            "        -0.2631, -0.4360,  0.0789, -0.8415,  0.5160,  1.3997, -0.7646, -3.1453,\n",
            "        -0.2920, -0.3125,  1.5129,  0.5243,  0.2146,  0.4245, -0.0884, -0.1780,\n",
            "         1.1876,  0.1058,  0.7657,  0.2191,  0.3582, -0.1164,  0.0933, -0.6248,\n",
            "        -0.2190,  0.2180,  0.7406, -0.4374,  0.1434,  0.1472, -1.1605, -0.0505,\n",
            "         0.1268, -0.0144, -0.9868, -0.0913, -1.2054, -0.1197,  0.0478, -0.5400,\n",
            "         0.5246, -0.7096, -0.3253, -0.1346, -0.4131,  0.3343, -0.0072,  0.3225,\n",
            "        -0.0442, -1.2969,  0.7622,  0.4635], device='cuda:0')\n",
            "Token[4]: \"located\"\n",
            "tensor([-0.3259, -0.0977,  0.4929,  0.5785,  0.0600,  0.1727, -0.2408,  1.1679,\n",
            "         0.3066, -0.0685,  0.3392, -0.1444,  0.8911,  0.2538,  0.3281, -0.8504,\n",
            "         0.9990,  0.0704, -0.7729,  0.1591,  0.3662,  1.2885,  0.6803, -0.1153,\n",
            "        -0.4641, -0.8220,  0.5264,  0.0250, -0.3565,  0.3435, -0.4078,  0.1170,\n",
            "         1.0752,  0.8914,  0.5610,  0.1422,  0.2339,  0.0219,  0.2791,  0.0548,\n",
            "         0.4955, -0.7159, -0.2975, -0.4888,  0.7659,  0.4937,  0.9453,  0.4742,\n",
            "         0.1847,  1.0986, -0.2204,  0.0127,  0.2676,  0.3564, -0.8907, -2.6910,\n",
            "        -0.6626, -1.1894,  1.4337,  0.0867,  0.0597,  0.7623,  0.7843,  0.5301,\n",
            "         0.3194,  0.0225,  0.2144,  0.0696,  0.5294,  0.7360, -0.0974, -0.1481,\n",
            "         0.2401, -0.2745,  0.5444, -0.7322,  0.9958,  0.2988, -0.2035,  0.1765,\n",
            "        -0.4184,  0.2860, -1.0497,  0.0569,  0.2900,  0.4441,  0.0429, -0.7129,\n",
            "         1.2021,  0.1814, -0.2566, -0.2002, -0.3269,  0.5788, -0.8189,  0.6666,\n",
            "        -0.2440, -1.0820,  1.1991, -0.0523], device='cuda:0')\n",
            "Token[5]: \"in\"\n",
            "tensor([ 0.0857, -0.2220,  0.1657,  0.1337,  0.3824,  0.3540,  0.0129,  0.2246,\n",
            "        -0.4382,  0.5016, -0.3587, -0.3498,  0.0552,  0.6965, -0.1796,  0.0679,\n",
            "         0.3910,  0.1604, -0.2664, -0.2114,  0.5370,  0.4938,  0.9366,  0.6690,\n",
            "         0.2179, -0.4664,  0.2238, -0.3620, -0.1766,  0.1748, -0.2037,  0.1393,\n",
            "         0.0198, -0.1041, -0.2024,  0.5500, -0.1546,  0.9865, -0.2686, -0.2909,\n",
            "        -0.3287, -0.3419, -0.1694, -0.4200, -0.0467, -0.1633,  0.7082, -0.7491,\n",
            "        -0.0916, -0.9618, -0.1975,  0.1028,  0.5522,  1.3816, -0.6564, -3.2502,\n",
            "        -0.3156, -1.2055,  1.7709,  0.4026, -0.7983,  1.1597, -0.3304,  0.3138,\n",
            "         0.7739,  0.2260,  0.5247, -0.0341,  0.3205,  0.0799,  0.1775, -0.4943,\n",
            "        -0.7005, -0.4457,  0.1724,  0.2028,  0.0233, -0.2068, -1.0158,  0.1832,\n",
            "         0.5675,  0.3182, -0.6501,  0.6828, -0.8658, -0.0594, -0.2926, -0.5567,\n",
            "        -0.3471, -0.3289,  0.4022, -0.1275, -0.2023,  0.8737, -0.5450,  0.7921,\n",
            "        -0.2069, -0.0743,  0.7581, -0.3424], device='cuda:0')\n",
            "Token[6]: \"Missouri,\"\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n",
            "Token[7]: \"is\"\n",
            "tensor([-0.5426,  0.4148,  1.0322, -0.4024,  0.4669,  0.2182, -0.0749,  0.4733,\n",
            "         0.0810, -0.2208, -0.1281, -0.1144,  0.5089,  0.1157,  0.0282, -0.3628,\n",
            "         0.4382,  0.0475,  0.2028,  0.4986, -0.1007,  0.1327,  0.1697,  0.1165,\n",
            "         0.3135,  0.2571,  0.0928, -0.5683, -0.5297, -0.0515, -0.6733,  0.9253,\n",
            "         0.2693,  0.2273,  0.6636,  0.2622,  0.1972,  0.2609,  0.1877, -0.3454,\n",
            "        -0.4263,  0.1398,  0.5634, -0.5691,  0.1240, -0.1289,  0.7248, -0.2610,\n",
            "        -0.2631, -0.4360,  0.0789, -0.8415,  0.5160,  1.3997, -0.7646, -3.1453,\n",
            "        -0.2920, -0.3125,  1.5129,  0.5243,  0.2146,  0.4245, -0.0884, -0.1780,\n",
            "         1.1876,  0.1058,  0.7657,  0.2191,  0.3582, -0.1164,  0.0933, -0.6248,\n",
            "        -0.2190,  0.2180,  0.7406, -0.4374,  0.1434,  0.1472, -1.1605, -0.0505,\n",
            "         0.1268, -0.0144, -0.9868, -0.0913, -1.2054, -0.1197,  0.0478, -0.5400,\n",
            "         0.5246, -0.7096, -0.3253, -0.1346, -0.4131,  0.3343, -0.0072,  0.3225,\n",
            "        -0.0442, -1.2969,  0.7622,  0.4635], device='cuda:0')\n",
            "Token[8]: \"named\"\n",
            "tensor([-0.3516, -0.1663,  0.3655, -0.3684,  0.2747, -0.1548,  0.4861, -0.0217,\n",
            "        -0.5534, -0.0798, -0.3784, -0.1086,  0.8104, -0.0225,  0.3513, -0.3454,\n",
            "         1.4764, -0.1031, -1.0180,  0.6877, -0.3879,  0.2693, -0.1971,  0.0187,\n",
            "         0.4329, -0.2141, -0.1406, -0.5489,  0.1550,  0.5489,  0.0804,  0.6596,\n",
            "         0.2833,  0.2851,  0.4042,  0.1460, -0.3615,  0.2548,  0.1282, -0.2636,\n",
            "        -0.2533,  0.3237,  0.6622, -0.4780,  0.6232,  0.6496, -0.4423,  0.0062,\n",
            "        -0.0396,  0.4655, -0.4053,  0.1261,  0.8087,  0.8531, -0.4349, -2.2792,\n",
            "        -0.8602, -0.1126,  0.5434,  0.6340, -0.0577,  0.7507,  0.1123,  0.2812,\n",
            "         0.9433, -0.7431,  0.2454,  0.7698,  0.5878,  0.7000,  0.1028, -0.1339,\n",
            "        -0.5746, -0.3237, -0.3334,  0.0385,  0.3300, -0.1217, -0.6153, -0.6244,\n",
            "         0.0891, -0.0047, -0.4819,  0.0180, -0.4557, -0.6658, -0.1686, -0.6777,\n",
            "         0.7518, -0.7094,  0.1465, -0.1562, -0.0856,  0.5049, -0.3004,  0.1565,\n",
            "        -0.7611, -0.5038,  0.1150, -0.8237], device='cuda:0')\n",
            "Token[9]: \"after\"\n",
            "tensor([ 3.7711e-01, -3.4471e-01,  1.3405e-01, -1.1710e-02, -1.9427e-01,\n",
            "         4.1464e-01,  4.0608e-01,  4.3063e-01, -5.7060e-02, -1.9921e-01,\n",
            "         4.3267e-01, -1.6269e-02,  2.1710e-01, -2.6149e-03,  3.9424e-01,\n",
            "        -4.2803e-01, -1.7495e-02, -5.6658e-01, -4.4558e-01, -1.8529e-01,\n",
            "         2.6732e-01, -1.5712e-01,  2.1657e-01,  7.9714e-01,  6.9623e-01,\n",
            "         2.0405e-01, -4.9907e-01, -4.5519e-01,  3.8210e-01,  2.0603e-01,\n",
            "        -2.1606e-01,  1.0093e-01, -5.0148e-01, -1.1058e-01, -4.3455e-01,\n",
            "        -2.6785e-01, -2.0234e-01,  3.8320e-03, -4.9108e-01, -1.7642e-01,\n",
            "        -8.8971e-01, -2.7900e-01,  8.6387e-01, -1.7356e-02,  3.1210e-01,\n",
            "         4.1004e-01,  2.3199e-01, -6.0812e-01,  4.4763e-01, -8.9579e-01,\n",
            "        -3.8491e-02, -2.5772e-01,  3.9468e-01,  1.6186e+00, -5.4882e-01,\n",
            "        -3.0291e+00, -7.7845e-01, -3.2463e-01,  1.7658e+00,  9.7303e-01,\n",
            "        -3.9342e-01,  5.4811e-01,  1.3164e-02,  3.7850e-01,  2.4538e-01,\n",
            "         3.1079e-02,  2.3628e-01,  2.8901e-01,  2.7047e-02,  2.8985e-01,\n",
            "        -7.4523e-01,  1.1517e-02, -3.9456e-01, -5.7706e-01, -6.3604e-01,\n",
            "         3.1022e-01, -3.8317e-01, -7.7663e-02, -1.3539e+00,  1.8009e-02,\n",
            "         8.5646e-01,  3.8259e-02, -3.9437e-01,  4.4331e-01, -1.0802e+00,\n",
            "        -4.3159e-01,  1.4391e-01,  1.1854e-01, -5.6459e-01, -4.7966e-01,\n",
            "         2.2860e-01, -2.4369e-01, -4.2823e-01,  1.0366e+00, -8.3071e-01,\n",
            "         1.2460e-01,  2.0630e-01,  5.4232e-01,  1.1425e-01, -6.6927e-01],\n",
            "       device='cuda:0')\n",
            "Token[10]: \"George\"\n",
            "tensor([-4.4295e-01,  5.9694e-02,  7.8306e-02, -2.4619e-01,  2.8411e-02,\n",
            "        -6.3084e-01,  3.7209e-02, -1.6568e-01, -7.3036e-01, -6.8152e-01,\n",
            "        -1.5946e-01, -4.4025e-01, -5.3630e-01,  2.4174e-01,  7.7513e-02,\n",
            "        -3.1366e-01,  3.3539e-01, -1.4214e-01, -1.2390e+00,  1.6170e-01,\n",
            "        -1.3486e-01, -2.2743e-01,  4.4955e-01, -7.2688e-01,  2.5063e-01,\n",
            "         1.1166e-01, -6.2308e-01, -5.5884e-01,  5.1252e-01,  3.6855e-01,\n",
            "         2.9996e-01,  7.0179e-01,  6.7464e-01,  4.0543e-01, -3.0825e-01,\n",
            "         8.3837e-01, -1.8986e-01, -1.7278e-01, -2.1306e-01,  3.5070e-01,\n",
            "        -4.1994e-01, -9.9697e-02,  7.5963e-01,  3.2975e-01, -2.2113e-01,\n",
            "        -1.2372e-01, -5.1361e-01, -6.8517e-01, -6.5545e-02, -1.7642e-01,\n",
            "         6.0858e-01, -7.4822e-02,  9.6863e-01,  5.0805e-01,  2.6777e-01,\n",
            "        -2.7093e+00, -1.0079e-01, -1.2588e-01,  1.2641e-01, -1.2384e-01,\n",
            "         4.0576e-01,  5.5333e-01,  1.5053e-01, -9.6734e-02,  2.8402e-01,\n",
            "        -2.3382e-01,  4.7452e-02,  1.4050e+00, -2.7471e-01,  6.4243e-01,\n",
            "        -5.1403e-01,  1.8748e-01, -9.9874e-01, -3.7344e-01, -1.4193e-01,\n",
            "        -2.7153e-01,  3.0842e-01,  9.3234e-01, -3.8388e-01,  3.6608e-01,\n",
            "         7.3780e-01, -1.7734e-01,  2.0051e-02, -2.4361e-01, -5.7102e-01,\n",
            "        -2.7266e-01, -5.6562e-01, -7.4104e-01,  1.6745e-01, -1.3076e+00,\n",
            "        -3.5900e-01, -7.5738e-01, -1.0948e-01,  5.3662e-01, -1.0914e+00,\n",
            "        -1.0287e-01,  5.8263e-04, -4.5603e-01, -4.7270e-02, -6.7599e-01],\n",
            "       device='cuda:0')\n",
            "Token[11]: \"Washington.\"\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC2ydyh9GEpG",
        "outputId": "e96a1628-632a-494d-b5d9-7393db9bc718"
      },
      "source": [
        "# Washington embedding \"Washington University\"\n",
        "sentence[0].get_embedding()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-2.2048e-01, -1.1316e-01,  9.4277e-01, -3.9024e-01,  2.5004e-01,\n",
              "        -4.1651e-01, -1.4640e-01,  2.3628e-03, -1.2966e-01, -1.1173e-01,\n",
              "        -2.1546e-01, -8.6271e-01,  1.3817e-01,  3.3118e-01, -6.6500e-01,\n",
              "         3.7134e-01,  2.0050e-01, -3.4055e-01, -1.2422e+00, -7.6653e-01,\n",
              "        -1.1253e-02,  3.8440e-01, -5.0105e-02, -1.8869e-01,  1.0785e-01,\n",
              "         1.7502e-01, -1.0167e-01, -5.7925e-01,  2.3529e-01,  3.2626e-02,\n",
              "         3.2353e-01,  9.7457e-01,  4.5231e-01,  4.9740e-01, -8.8874e-01,\n",
              "         4.9170e-01,  1.1230e-01, -2.1484e-01,  9.3187e-02,  4.7039e-01,\n",
              "        -7.8776e-01, -6.8219e-01, -2.3741e-01,  2.2351e-01,  2.0269e-01,\n",
              "        -1.0166e+00,  1.3095e-01, -2.3654e-01,  3.1501e-01, -3.1880e-01,\n",
              "         5.9744e-01, -2.8722e-01,  2.9970e-01,  3.4877e-01, -1.6597e-01,\n",
              "        -2.8483e+00,  3.2219e-01, -7.8469e-01,  1.3754e+00,  1.5050e-01,\n",
              "        -8.5193e-01,  2.5303e-01,  2.0142e-01, -5.9176e-01,  8.9212e-02,\n",
              "        -3.5561e-01,  2.6522e-01,  1.1283e+00, -3.7250e-01,  9.4010e-01,\n",
              "        -5.2708e-01, -4.6361e-01, -8.1034e-01, -3.3479e-01,  1.0260e-01,\n",
              "        -4.1905e-01,  6.3775e-01,  4.5096e-02, -1.2385e+00,  2.1903e-01,\n",
              "         4.9145e-01,  3.6583e-01, -3.2024e-01, -5.1061e-02, -6.5049e-01,\n",
              "        -1.1894e-02,  2.0887e-01,  2.5857e-01, -2.3303e-01, -8.4911e-01,\n",
              "        -1.5466e-01, -3.3714e-01, -3.7272e-01, -2.9143e-01, -1.2799e+00,\n",
              "         2.8781e-01,  2.9473e-01, -3.2733e-01,  6.3016e-01,  3.6249e-01],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5U-FL5l8lit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a511e9-a19e-4d60-ef4b-b2a7e7973a1a"
      },
      "source": [
        "# Washington embedding in \"George Washington\"\n",
        "sentence[11].get_embedding()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvFXLP5kH5xZ"
      },
      "source": [
        "## ASSIGNMENT 1\n",
        "\n",
        "In theory, the representations for tokens sentence[0] and sentence[11] should be the same (same glove vector representation).\n",
        "\n",
        "+ Write code to establish whether the vectors are actually the same. The output should look like the one below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in sentence:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlDuXnwapLYm",
        "outputId": "a2258d0b-368b-4a2b-f142-42d644857a0c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token[0]: \"Washington\"\n",
            "Token[1]: \"University,\"\n",
            "Token[2]: \"which\"\n",
            "Token[3]: \"is\"\n",
            "Token[4]: \"located\"\n",
            "Token[5]: \"in\"\n",
            "Token[6]: \"Missouri,\"\n",
            "Token[7]: \"is\"\n",
            "Token[8]: \"named\"\n",
            "Token[9]: \"after\"\n",
            "Token[10]: \"George\"\n",
            "Token[11]: \"Washington.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The two vectors are not same because Token[0] is \"washington\" where as Token[11] is \"washington\" + \".\", so they are not same.\n",
        "sentence[11].get_embedding() == sentence[0].get_embedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcAI5xrRMCGw",
        "outputId": "85c5dddc-70fc-46c5-99a9-5fdc3c9411df"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([False, False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False, False],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2asTF-XjImdt"
      },
      "source": [
        "+ TODO: You need to find out why this is the case.\n",
        "+ TODO: Once you find out, write code to obtain the embeddings again and to establish that they are indeed the same representations (for both occurrences of 'Washington')."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_embedding.embed(tokenized_sentence)\n",
        "for token in tokenized_sentence:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jguuaM4v5IM",
        "outputId": "57d46d19-6a2a-47e3-e498-a40f8081468d"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token[0]: \"Washington\"\n",
            "Token[1]: \"University\"\n",
            "Token[2]: \",\"\n",
            "Token[3]: \"which\"\n",
            "Token[4]: \"is\"\n",
            "Token[5]: \"located\"\n",
            "Token[6]: \"in\"\n",
            "Token[7]: \"Missouri\"\n",
            "Token[8]: \",\"\n",
            "Token[9]: \"is\"\n",
            "Token[10]: \"named\"\n",
            "Token[11]: \"after\"\n",
            "Token[12]: \"George\"\n",
            "Token[13]: \"Washington\"\n",
            "Token[14]: \".\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vector representation of  each word in the sentence\n",
        "for token in tokenized_sentence:\n",
        "    print(token)\n",
        "    print(token.embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i6bY13ZwE-P",
        "outputId": "7402458b-cfa6-470b-978c-b95939722781"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token[0]: \"Washington\"\n",
            "tensor([-2.2048e-01, -1.1316e-01,  9.4277e-01,  ..., -2.2946e-03,\n",
            "         1.4192e-03,  2.5719e-04], device='cuda:0')\n",
            "Token[1]: \"University\"\n",
            "tensor([ 6.9580e-01, -1.9334e-01, -7.8134e-01,  ..., -7.0233e-05,\n",
            "        -1.3960e-03,  7.4919e-04], device='cuda:0')\n",
            "Token[2]: \",\"\n",
            "tensor([-1.0767e-01,  1.1053e-01,  5.9812e-01,  ...,  2.0763e-05,\n",
            "        -1.9901e-02,  1.0584e-02], device='cuda:0')\n",
            "Token[3]: \"which\"\n",
            "tensor([ 0.0302,  0.4461,  0.4317,  ..., -0.0035, -0.0067,  0.0031],\n",
            "       device='cuda:0')\n",
            "Token[4]: \"is\"\n",
            "tensor([-0.5426,  0.4148,  1.0322,  ..., -0.0019,  0.1306,  0.0056],\n",
            "       device='cuda:0')\n",
            "Token[5]: \"located\"\n",
            "tensor([-3.2592e-01, -9.7740e-02,  4.9287e-01,  ...,  2.8786e-04,\n",
            "        -1.3166e-03, -1.2682e-03], device='cuda:0')\n",
            "Token[6]: \"in\"\n",
            "tensor([ 8.5703e-02, -2.2201e-01,  1.6569e-01,  ...,  6.4110e-05,\n",
            "         6.5429e-03,  2.6719e-02], device='cuda:0')\n",
            "Token[7]: \"Missouri\"\n",
            "tensor([ 5.8746e-01, -4.1809e-02,  5.4232e-01,  ..., -3.1905e-05,\n",
            "         7.0416e-03,  7.9550e-03], device='cuda:0')\n",
            "Token[8]: \",\"\n",
            "tensor([-1.0767e-01,  1.1053e-01,  5.9812e-01,  ...,  5.4086e-05,\n",
            "         3.7566e-03,  9.4838e-03], device='cuda:0')\n",
            "Token[9]: \"is\"\n",
            "tensor([-5.4264e-01,  4.1476e-01,  1.0322e+00,  ..., -9.1483e-04,\n",
            "         1.0065e-02,  4.0604e-03], device='cuda:0')\n",
            "Token[10]: \"named\"\n",
            "tensor([-0.3516, -0.1663,  0.3655,  ...,  0.0006, -0.0128,  0.0092],\n",
            "       device='cuda:0')\n",
            "Token[11]: \"after\"\n",
            "tensor([ 0.3771, -0.3447,  0.1340,  ..., -0.0021,  0.0015,  0.0126],\n",
            "       device='cuda:0')\n",
            "Token[12]: \"George\"\n",
            "tensor([-0.4430,  0.0597,  0.0783,  ..., -0.0006,  0.0128,  0.0035],\n",
            "       device='cuda:0')\n",
            "Token[13]: \"Washington\"\n",
            "tensor([-2.2048e-01, -1.1316e-01,  9.4277e-01,  ..., -3.0783e-04,\n",
            "         5.8905e-03,  4.3093e-03], device='cuda:0')\n",
            "Token[14]: \".\"\n",
            "tensor([-3.3979e-01,  2.0941e-01,  4.6348e-01,  ...,  4.3887e-04,\n",
            "         5.4742e-03,  6.3518e-03], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp6WF1ngyzjr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0b90c08-e00d-4c6e-ec6c-9dd77bd3978c"
      },
      "source": [
        "# checking if the Token[0] is same as Token[13]\n",
        "tokenized_sentence[13].get_embedding() == tokenized_sentence[0].get_embedding()"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ True,  True,  True,  ..., False, False, False], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHRuHJaGKXJh"
      },
      "source": [
        "## ASSIGNMENT 2\n",
        "\n",
        "In this assigment we will show how the representations obtained for both occurrences of 'Washington' are different when they are obtained from Flair contextual-based embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "XFy2wbXQvUbN"
      },
      "outputs": [],
      "source": [
        "from flair.embeddings import FlairEmbeddings\n",
        "\n",
        "# init Flair embedding\n",
        "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
        "tokenized_sentence = Sentence('Washington University, which is located in Missouri, is named after George Washington.', use_tokenizer=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4x3tR9lLlsB"
      },
      "source": [
        "+ TODO: compare the representations obtained for 'Washington' for both sentences, tokenized and raw."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ajmr-0JJLK4G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f65b34fd-228c-4122-87d2-cb7e4152daf0"
      },
      "source": [
        "# TODO compare the Flair embeddings obtained for 'Washington'\n",
        "#tokenized sentence\n",
        "flair_embedding_forward.embed(tokenized_sentence)\n",
        "\n",
        "for token in tokenized_sentence:\n",
        "  print(token)\n",
        "  print(token.embedding)\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token[0]: \"Washington\"\n",
            "tensor([ 3.6137e-03, -3.7161e-06, -2.9144e-02,  ..., -2.2946e-03,\n",
            "         1.4192e-03,  2.5719e-04], device='cuda:0')\n",
            "Token[1]: \"University\"\n",
            "tensor([ 4.9652e-04, -6.9346e-04, -2.1376e-01,  ..., -7.0233e-05,\n",
            "        -1.3960e-03,  7.4919e-04], device='cuda:0')\n",
            "Token[2]: \",\"\n",
            "tensor([-4.0296e-05,  2.9115e-05,  6.2106e-02,  ...,  2.0763e-05,\n",
            "        -1.9901e-02,  1.0584e-02], device='cuda:0')\n",
            "Token[3]: \"which\"\n",
            "tensor([-0.0005,  0.0004,  0.0358,  ..., -0.0035, -0.0067,  0.0031],\n",
            "       device='cuda:0')\n",
            "Token[4]: \"is\"\n",
            "tensor([ 0.0045, -0.0022,  0.1370,  ..., -0.0019,  0.1306,  0.0056],\n",
            "       device='cuda:0')\n",
            "Token[5]: \"located\"\n",
            "tensor([-0.0110, -0.0005,  0.0741,  ...,  0.0003, -0.0013, -0.0013],\n",
            "       device='cuda:0')\n",
            "Token[6]: \"in\"\n",
            "tensor([-1.7291e-02,  2.4153e-05,  7.0391e-02,  ...,  6.4110e-05,\n",
            "         6.5429e-03,  2.6719e-02], device='cuda:0')\n",
            "Token[7]: \"Missouri\"\n",
            "tensor([ 4.3458e-07,  8.4390e-05,  1.8153e-02,  ..., -3.1905e-05,\n",
            "         7.0416e-03,  7.9550e-03], device='cuda:0')\n",
            "Token[8]: \",\"\n",
            "tensor([-1.3976e-04,  2.2443e-03,  3.1917e-02,  ...,  5.4086e-05,\n",
            "         3.7566e-03,  9.4838e-03], device='cuda:0')\n",
            "Token[9]: \"is\"\n",
            "tensor([ 0.0039, -0.0014,  0.1891,  ..., -0.0009,  0.0101,  0.0041],\n",
            "       device='cuda:0')\n",
            "Token[10]: \"named\"\n",
            "tensor([ 1.3887e-03, -7.9731e-05,  6.1766e-02,  ...,  6.4618e-04,\n",
            "        -1.2783e-02,  9.1788e-03], device='cuda:0')\n",
            "Token[11]: \"after\"\n",
            "tensor([ 3.7910e-02, -7.4848e-05,  9.4208e-02,  ..., -2.0584e-03,\n",
            "         1.4700e-03,  1.2642e-02], device='cuda:0')\n",
            "Token[12]: \"George\"\n",
            "tensor([ 7.6139e-05,  3.2164e-05, -2.4055e-02,  ..., -6.3394e-04,\n",
            "         1.2783e-02,  3.5248e-03], device='cuda:0')\n",
            "Token[13]: \"Washington\"\n",
            "tensor([ 4.1531e-04,  9.6629e-06, -1.4776e-03,  ..., -3.0783e-04,\n",
            "         5.8905e-03,  4.3093e-03], device='cuda:0')\n",
            "Token[14]: \".\"\n",
            "tensor([-8.5550e-05,  2.3345e-04,  2.2249e-01,  ...,  4.3887e-04,\n",
            "         5.4742e-03,  6.3518e-03], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#camparing the embeddings of wasington\n",
        "tokenized_sentence[0].get_embedding() == tokenized_sentence[13].get_embedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lxliwN4qbMV",
        "outputId": "8af5e620-53d6-4cf1-b512-e9fd59c4fad8"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([False, False, False,  ..., False, False, False], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNf91MDYQybN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e753d6fa-7bc2-4fee-af24-e08c7fa3839b"
      },
      "source": [
        "#Raw sentence\n",
        "flair_embedding_forward.embed(sentence)\n",
        "for token in sentence:\n",
        "  print(token)\n",
        "  print(token.embedding)\n",
        " "
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token[0]: \"Washington\"\n",
            "tensor([-2.2048e-01, -1.1316e-01,  9.4277e-01,  ..., -2.2946e-03,\n",
            "         1.4192e-03,  2.5719e-04], device='cuda:0')\n",
            "Token[1]: \"University,\"\n",
            "tensor([0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0303, 0.0230], device='cuda:0')\n",
            "Token[2]: \"which\"\n",
            "tensor([ 0.0302,  0.4461,  0.4317,  ..., -0.0089, -0.0048,  0.0054],\n",
            "       device='cuda:0')\n",
            "Token[3]: \"is\"\n",
            "tensor([-0.5426,  0.4148,  1.0322,  ..., -0.0027,  0.0913,  0.0074],\n",
            "       device='cuda:0')\n",
            "Token[4]: \"located\"\n",
            "tensor([-3.2592e-01, -9.7740e-02,  4.9287e-01,  ...,  4.2787e-04,\n",
            "        -1.6303e-03, -9.5898e-04], device='cuda:0')\n",
            "Token[5]: \"in\"\n",
            "tensor([ 8.5703e-02, -2.2201e-01,  1.6569e-01,  ...,  5.2130e-05,\n",
            "         7.9720e-03,  3.0658e-02], device='cuda:0')\n",
            "Token[6]: \"Missouri,\"\n",
            "tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.0006,  0.0282,  0.0026],\n",
            "       device='cuda:0')\n",
            "Token[7]: \"is\"\n",
            "tensor([-5.4264e-01,  4.1476e-01,  1.0322e+00,  ..., -7.2745e-04,\n",
            "         1.0725e-02,  8.1581e-03], device='cuda:0')\n",
            "Token[8]: \"named\"\n",
            "tensor([-0.3516, -0.1663,  0.3655,  ...,  0.0004, -0.0129,  0.0087],\n",
            "       device='cuda:0')\n",
            "Token[9]: \"after\"\n",
            "tensor([ 0.3771, -0.3447,  0.1340,  ..., -0.0018,  0.0016,  0.0137],\n",
            "       device='cuda:0')\n",
            "Token[10]: \"George\"\n",
            "tensor([-0.4430,  0.0597,  0.0783,  ..., -0.0005,  0.0111,  0.0037],\n",
            "       device='cuda:0')\n",
            "Token[11]: \"Washington.\"\n",
            "tensor([0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0139, 0.0491], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#camparing the embeddings of washington\n",
        "tokenized_sentence[0].get_embedding() == tokenized_sentence[11].get_embedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rh46riprJAN",
        "outputId": "4976bbeb-8bc8-449b-af42-67acd20cb91f"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([False, False, False,  ..., False, False, False], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj3_18iBim-F"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Tagging\n",
        "\n",
        "Now we will learn how to tag our sentence using Flair pre-trained models for the following tasks:\n",
        "\n",
        "1. POS tagging\n",
        "3. Named Entity Recognition\n",
        "4. Frame Semantics (event detection)\n",
        "5. Polarity classification\n",
        "\n",
        "** Check the following link to see the list of available models and languages:**\n",
        "[Flair Tagging Info](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_2_TAGGING.md)\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9zjAbXbkMzS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47d5d065-fcdd-42de-f820-f611a38f5978"
      },
      "source": [
        "pos_tagger = SequenceTagger.load('pos')"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:629: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-13 21:45:39,704 loading file /root/.flair/models/pos-english/a9a73f6cd878edce8a0fa518db76f441f1cc49c2525b2b4557af278ec2f0659e.121306ea62993d04cd1978398b68396931a39eb47754c8a06a87f325ea70ac63\n",
            "2023-02-13 21:45:39,965 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnCl3opclmH0"
      },
      "source": [
        "pos_tagger.predict(sentence)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5poo-5Ql1mZ"
      },
      "source": [
        "for postag in sentence.get_spans('pos'):\n",
        "  print(postag)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqaR_aymmKw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e54e877b-09bf-4cef-c526-5a7de6a81bbe"
      },
      "source": [
        "print(sentence.to_tagged_string())"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: \"Washington University, which is located in Missouri, is named after George Washington.\" → [\"Washington\"/NNP, \"University,\"/NNP, \"which\"/WDT, \"is\"/VBZ, \"located\"/VBN, \"in\"/IN, \"Missouri,\"/NNP, \"is\"/VBZ, \"named\"/VBN, \"after\"/IN, \"George\"/NNP, \"Washington.\"/NNP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Smhnbxoojwdi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef500e9-2549-49c9-e714-1494e828b6cb"
      },
      "source": [
        "chunker = SequenceTagger.load('chunk')"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:629: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-13 21:45:49,513 loading file /root/.flair/models/chunk-english/5b53097d6763734ee8ace8de92db67a1ee2528d5df9c6d20ec8e3e6f6470b423.d81b7fd7a38422f2dbf40f6449b1c63d5ae5b959863aa0c2c1ce9116902e8b22\n",
            "2023-02-13 21:45:49,706 SequenceTagger predicts: Dictionary with 45 tags: <unk>, O, B-NP, E-NP, I-NP, S-PP, S-VP, S-SBAR, S-ADVP, S-NP, S-ADJP, B-VP, E-VP, B-PP, E-PP, I-VP, S-PRT, B-ADVP, E-ADVP, B-ADJP, E-ADJP, B-CONJP, I-CONJP, E-CONJP, I-ADJP, B-SBAR, E-SBAR, S-INTJ, I-ADVP, I-PP, B-UCP, I-UCP, E-UCP, S-LST, B-PRT, I-PRT, E-PRT, S-CONJP, B-INTJ, E-INTJ, I-INTJ, B-LST, E-LST, <START>, <STOP>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbXQaTaMj56O"
      },
      "source": [
        "chunker.predict(sentence)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIy91gzlkGfU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a56d3bd4-a116-4dfd-c8eb-2a1b033a0454"
      },
      "source": [
        "for chunktag in sentence.get_spans('np'):\n",
        "  print(chunktag)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Span[0:2]: \"Washington University,\" → NP (0.7408)\n",
            "Span[2:3]: \"which\" → NP (0.9995)\n",
            "Span[3:5]: \"is located\" → VP (0.8574)\n",
            "Span[5:6]: \"in\" → PP (1.0)\n",
            "Span[6:7]: \"Missouri,\" → NP (0.9999)\n",
            "Span[7:9]: \"is named\" → VP (0.9624)\n",
            "Span[9:10]: \"after\" → PP (0.9981)\n",
            "Span[10:12]: \"George Washington.\" → NP (0.798)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUJEv1Tlj-MG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9c01e32-fb44-4039-c3c0-7afbb6a3c3f4"
      },
      "source": [
        "print(sentence.to_tagged_string())"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: \"Washington University, which is located in Missouri, is named after George Washington.\" → [\"Washington\"/NNP, \"Washington University,\"/NP, \"University,\"/NNP, \"which\"/WDT, \"which\"/NP, \"is\"/VBZ, \"is located\"/VP, \"located\"/VBN, \"in\"/IN, \"in\"/PP, \"Missouri,\"/NNP, \"Missouri,\"/NP, \"is\"/VBZ, \"is named\"/VP, \"named\"/VBN, \"after\"/IN, \"after\"/PP, \"George\"/NNP, \"George Washington.\"/NP, \"Washington.\"/NNP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7iEd1O87dR1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7fabf96-e307-45cb-8616-e06df335a8ea"
      },
      "source": [
        "ner_tagger = SequenceTagger.load('ner')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:629: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-13 21:45:59,001 loading file /root/.flair/models/ner-english/4f4cdab26f24cb98b732b389e6cebc646c36f54cfd6e0b7d3b90b25656e4262f.8baa8ae8795f4df80b28e7f7b61d788ecbb057d1dc85aacb316f1bd02837a4a4\n",
            "2023-02-13 21:46:01,603 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_BqJpjM7nS0"
      },
      "source": [
        "ner_tagger.predict(sentence)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKsRwzPn85XV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "228294ba-975e-4773-e539-0d0dc4bd3cdd"
      },
      "source": [
        "# iterate over entities and print\n",
        "for entity in sentence.get_spans('ner'):\n",
        "    print(entity)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Span[0:2]: \"Washington University,\" → NP (0.7408); ORG (0.8766)\n",
            "Span[6:7]: \"Missouri,\" → NP (0.9999); LOC (0.9987)\n",
            "Span[10:12]: \"George Washington.\" → NP (0.798); PER (0.9916)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sucDODzD9lud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac94764-ea00-4ee1-9ea0-26ca3b1a64ea"
      },
      "source": [
        "print(sentence.to_tagged_string())"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: \"Washington University, which is located in Missouri, is named after George Washington.\" → [\"Washington\"/NNP, \"Washington University,\"/NP/ORG, \"University,\"/NNP, \"which\"/WDT, \"which\"/NP, \"is\"/VBZ, \"is located\"/VP, \"located\"/VBN, \"in\"/IN, \"in\"/PP, \"Missouri,\"/NNP, \"Missouri,\"/NP/LOC, \"is\"/VBZ, \"is named\"/VP, \"named\"/VBN, \"after\"/IN, \"after\"/PP, \"George\"/NNP, \"George Washington.\"/NP/PER, \"Washington.\"/NNP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcTqR2-w-Iu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62bfdfea-a729-4b29-f7bc-beaa86f4bedc"
      },
      "source": [
        "sem_tagger = SequenceTagger.load('frame')"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:629: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-13 21:46:13,008 loading file /root/.flair/models/frame-english/c397b8bbddf56e35a7d4b64295712a42a1a9b7ccf430dff76d03c8c7e26b9707.fd7786a36026b383ca73a1413c0a29aa1e67551621b805a0d28ca547636353b9\n",
            "2023-02-13 21:46:13,644 SequenceTagger predicts: Dictionary with 5196 tags: <unk>, O, _, do.01, get.01, kid.01, know.01, be.01, send.01, seem.01, fold.03, have.03, want.01, say.01, pass.08, play.01, be_like.04, be.03, record.01, hear.01, speak.01, go.04, mean.01, let.01, go.01, see.01, drive.01, pull.01, look.01, start.01, come.01, get.06, pay.01, go.02, miss.01, know.02, know.06, forget.01, ask.02, mail.01, wait.01, be.02, make.02, make.01, think.01, live.01, care.01, smoke.02, put_off.07, mind.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbZY7xMQiBZL"
      },
      "source": [
        "sem_tagger.predict(sentence)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmddT1dziPfx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9f057f5-c893-4100-a707-f0af691bc7f9"
      },
      "source": [
        "for event in sentence.get_spans('frame'):\n",
        "  print(event)\n",
        "print(sentence.to_tagged_string())"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: \"Washington University, which is located in Missouri, is named after George Washington.\" → [\"Washington\"/NNP, \"Washington University,\"/NP/ORG, \"University,\"/NNP, \"which\"/WDT, \"which\"/NP, \"is\"/VBZ/be.03, \"is located\"/VP, \"located\"/VBN/locate.01, \"in\"/IN, \"in\"/PP, \"Missouri,\"/NNP, \"Missouri,\"/NP/LOC, \"is\"/VBZ/be.03, \"is named\"/VP, \"named\"/VBN/name.01, \"after\"/IN, \"after\"/PP, \"George\"/NNP, \"George Washington.\"/NP/PER, \"Washington.\"/NNP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yxv9Xfxiz4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08488acb-362f-4380-a269-66a30f2ba4f8"
      },
      "source": [
        "print(sentence.to_dict(tag_type='pos'))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'Washington University, which is located in Missouri, is named after George Washington.', 'pos': [{'value': 'NNP', 'confidence': 1.0}, {'value': 'NNP', 'confidence': 0.9999886751174927}, {'value': 'WDT', 'confidence': 0.9999895095825195}, {'value': 'VBZ', 'confidence': 0.9999998807907104}, {'value': 'VBN', 'confidence': 0.9999347925186157}, {'value': 'IN', 'confidence': 1.0}, {'value': 'NNP', 'confidence': 0.999966025352478}, {'value': 'VBZ', 'confidence': 1.0}, {'value': 'VBN', 'confidence': 0.9999909400939941}, {'value': 'IN', 'confidence': 0.9993758797645569}, {'value': 'NNP', 'confidence': 0.9999997615814209}, {'value': 'NNP', 'confidence': 0.9999992847442627}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Szh3q5AlaTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1dd677d-96db-4694-848f-665f67c5768c"
      },
      "source": [
        "print(sentence.to_dict(tag_type='chunk'))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'Washington University, which is located in Missouri, is named after George Washington.', 'chunk': []}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6WEuLoljCR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6443574a-43af-469d-c16e-12e7c6e28f7e"
      },
      "source": [
        "print(sentence.to_dict(tag_type='ner'))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'Washington University, which is located in Missouri, is named after George Washington.', 'ner': [{'value': 'ORG', 'confidence': 0.8765585720539093}, {'value': 'LOC', 'confidence': 0.9987462759017944}, {'value': 'PER', 'confidence': 0.9916445016860962}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0z0nr2QjG5Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d5e2f69-13bb-409d-e17d-0a400f7d9302"
      },
      "source": [
        "print(sentence.to_dict(tag_type='frame'))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'Washington University, which is located in Missouri, is named after George Washington.', 'frame': [{'value': 'be.03', 'confidence': 0.9955000281333923}, {'value': 'locate.01', 'confidence': 0.9831220507621765}, {'value': 'be.03', 'confidence': 0.9986500144004822}, {'value': 'name.01', 'confidence': 0.7090935707092285}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2DRh1z5mZPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "324b906f-7e5b-42a8-e68e-2bd0e39c1d59"
      },
      "source": [
        "polarity_classifier = TextClassifier.load('en-sentiment')"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-13 21:46:29,812 loading file /root/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEWoXN31m90v"
      },
      "source": [
        "polarity_classifier.predict(sentence)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfEK7YplnB24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa04886f-194a-416b-d738-47f8fa915632"
      },
      "source": [
        "print(sentence.to_tagged_string())\n",
        "print(sentence.labels)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: \"Washington University, which is located in Missouri, is named after George Washington.\" → POSITIVE (0.9722) → [\"Washington\"/NNP, \"Washington University,\"/NP/ORG, \"University,\"/NNP, \"which\"/WDT, \"which\"/NP, \"is\"/VBZ/be.03, \"is located\"/VP, \"located\"/VBN/locate.01, \"in\"/IN, \"in\"/PP, \"Missouri,\"/NNP, \"Missouri,\"/NP/LOC, \"is\"/VBZ/be.03, \"is named\"/VP, \"named\"/VBN/name.01, \"after\"/IN, \"after\"/PP, \"George\"/NNP, \"George Washington.\"/NP/PER, \"Washington.\"/NNP]\n",
            "['Token[0]: \"Washington\"'/'NNP' (1.0), 'Token[1]: \"University,\"'/'NNP' (1.0), 'Token[2]: \"which\"'/'WDT' (1.0), 'Token[3]: \"is\"'/'VBZ' (1.0), 'Token[4]: \"located\"'/'VBN' (0.9999), 'Token[5]: \"in\"'/'IN' (1.0), 'Token[6]: \"Missouri,\"'/'NNP' (1.0), 'Token[7]: \"is\"'/'VBZ' (1.0), 'Token[8]: \"named\"'/'VBN' (1.0), 'Token[9]: \"after\"'/'IN' (0.9994), 'Token[10]: \"George\"'/'NNP' (1.0), 'Token[11]: \"Washington.\"'/'NNP' (1.0), 'Span[0:2]: \"Washington University,\"'/'NP' (0.7408), 'Span[2:3]: \"which\"'/'NP' (0.9995), 'Span[3:5]: \"is located\"'/'VP' (0.8574), 'Span[5:6]: \"in\"'/'PP' (1.0), 'Span[6:7]: \"Missouri,\"'/'NP' (0.9999), 'Span[7:9]: \"is named\"'/'VP' (0.9624), 'Span[9:10]: \"after\"'/'PP' (0.9981), 'Span[10:12]: \"George Washington.\"'/'NP' (0.798), 'Span[0:2]: \"Washington University,\"'/'ORG' (0.8766), 'Span[6:7]: \"Missouri,\"'/'LOC' (0.9987), 'Span[10:12]: \"George Washington.\"'/'PER' (0.9916), 'Token[3]: \"is\"'/'be.03' (0.9955), 'Token[4]: \"located\"'/'locate.01' (0.9831), 'Token[7]: \"is\"'/'be.03' (0.9987), 'Token[8]: \"named\"'/'name.01' (0.7091), 'Sentence: \"Washington University, which is located in Missouri, is named after George Washington.\"'/'POSITIVE' (0.9722)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8dStO99pjXN"
      },
      "source": [
        "## ASSIGNMENT 3\n",
        "\n",
        "Check out the following list of sentences and perform the following tasks using the Flair system and models:\n",
        "\n",
        "1. Perform POS tagging and Named Entity Recognition on sentences 1-4.\n",
        "2. Chunking and Frame detection for sentences 5-6.\n",
        "3. Sentiment Analysis for sentences 7-8.\n",
        "\n",
        "**Do not repeat the instructions, use the loop structure to annotate and display the annotations of every sentence in one step per task.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogtPRzoL-1m9"
      },
      "source": [
        "sentence_1 = Sentence('Jackson is placed in Microsoft located in Redmond .')\n",
        "sentence_2 = Sentence('Redmond is coming to New York city .')\n",
        "sentence_3 = Sentence('Redmond is coming to New York City .')\n",
        "sentence_4 = Sentence('Redmond is coming to New York City.')\n",
        "sentence_5 = Sentence('Redmond returned to New York City to return his hat .')\n",
        "sentence_6 = Sentence('He had a look at different hats .')\n",
        "sentence_7 = Sentence('This film hurts.')\n",
        "sentence_8 = Sentence('It is so bad that I am confused.')"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# POS tagging and Named Entity Recognition on sentences 1-4.\n",
        "sent_ner_pos = [sentence_2, sentence_1, sentence_3, sentence_4]\n",
        "#Chunking and Frame detection for sentences 5-6.\n",
        "sent_chu_fra = [sentence_5, sentence_6]\n",
        "#Sentiment Analysis for sentences 7-8.\n",
        "sent_analysis = [sentence_8, sentence_7]"
      ],
      "metadata": {
        "id": "vLtEIy5XGJuK"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sent_ner_pos:\n",
        "  pos_tagger.predict(sentence)\n",
        "  ner_tagger.predict(sentence)\n",
        "  print(sentence.to_tagged_string())\n",
        "  print(sentence.to_original_text())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n86yU3S8GM2s",
        "outputId": "5f761d42-9e0a-4025-82fc-55b5fce41a29"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: \"Redmond is coming to New York city .\" → [\"Redmond\"/NNP, \"Redmond\"/PER, \"is\"/VBZ, \"coming\"/VBG, \"to\"/IN, \"New\"/NNP, \"New York\"/LOC, \"York\"/NNP, \"city\"/NN, \".\"/.]\n",
            "Redmond is coming to New York city .\n",
            "Sentence: \"Jackson is placed in Microsoft located in Redmond .\" → [\"Jackson\"/NNP, \"Jackson\"/PER, \"is\"/VBZ, \"placed\"/VBN, \"in\"/IN, \"Microsoft\"/NNP, \"Microsoft\"/ORG, \"located\"/VBN, \"in\"/IN, \"Redmond\"/NNP, \"Redmond\"/LOC, \".\"/.]\n",
            "Jackson is placed in Microsoft located in Redmond .\n",
            "Sentence: \"Redmond is coming to New York City .\" → [\"Redmond\"/NNP, \"Redmond\"/PER, \"is\"/VBZ, \"coming\"/VBG, \"to\"/IN, \"New\"/NNP, \"New York City\"/LOC, \"York\"/NNP, \"City\"/NNP, \".\"/.]\n",
            "Redmond is coming to New York City .\n",
            "Sentence: \"Redmond is coming to New York City .\" → [\"Redmond\"/NNP, \"Redmond\"/PER, \"is\"/VBZ, \"coming\"/VBG, \"to\"/IN, \"New\"/NNP, \"New York City\"/LOC, \"York\"/NNP, \"City\"/NNP, \".\"/.]\n",
            "Redmond is coming to New York City.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sent_chu_fra:\n",
        "  chunker.predict(sentence)\n",
        "  sem_tagger.predict(sentence)\n",
        "  print(sentence.to_tagged_string())\n",
        "  print(sentence.to_original_text())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUwkUA6tGSVX",
        "outputId": "e91557c9-e829-40d5-c70d-15f56af1b081"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: \"Redmond returned to New York City to return his hat .\" → [\"Redmond\"/NP, \"returned\"/VP, \"returned\"/return.01, \"to\"/PP, \"New York City\"/NP, \"to return\"/VP, \"return\"/return.02, \"his hat\"/NP]\n",
            "Redmond returned to New York City to return his hat .\n",
            "Sentence: \"He had a look at different hats .\" → [\"He\"/NP, \"had\"/VP, \"had\"/have.03, \"a look\"/NP, \"look\"/look.01, \"at\"/PP, \"different hats\"/NP]\n",
            "He had a look at different hats .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sent_analysis:\n",
        "  polarity_classifier.predict(sentence)\n",
        "  print(sentence.to_original_text())\n",
        "  print(sentence.labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQJN4-JDGUAQ",
        "outputId": "b744d2bd-d108-4b64-b901-fafd51e7d18c"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It is so bad that I am confused.\n",
            "['Sentence: \"It is so bad that I am confused .\"'/'NEGATIVE' (0.9999)]\n",
            "This film hurts.\n",
            "['Sentence: \"This film hurts .\"'/'NEGATIVE' (0.9999)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENKOC4Q6x6Vc"
      },
      "source": [
        "## ASSIGNMENT 4 (BONUS)\n",
        "\n",
        "In this task you will be annotating a movie review at document and sentence level.\n",
        "\n",
        "1. Open the text in this file: '/content/drive/My Drive/Colab Notebooks/2023-ILTAPP/resources/movie-review.txt'\n",
        "2. Predict Named Entities and Sentiment for the **whole document**.\n",
        "3. Predict Named Entities and Sentiment for each of the sentences in the document.\n",
        "> 3.1 Hint: You will need to segment the document at sentence level using the segtok segmenter and store each sentence as a Sentence object. The final result must be a list of Sentence objects. The segtok segmenter is used as in the following code snippet:\n",
        "\n",
        "```\n",
        "from segtok.segmenter import split_single\n",
        "split_single(docText)\n",
        "```\n",
        "\n",
        "4. Print both the sentiment classification output and Named Entities.\n",
        "5. Spot the differences in the annotations when performed at document and at sentence level. Write the difference at the end of this notebook.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "613ff68d-2c10-4448-8fdb-666a849f82bf",
        "id": "r-LleWY0v9Rx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: \"Once again Mr. Costner has dragged out a movie for far longer than necessary . Aside from the terrific sea rescue sequences , of which there are very few I just did not care about any of the characters . Most of us have ghosts in the closet , and Costner 's character are realized early on , and then forgotten until much later , by which time I did not care . The character we should really care about is a very cocky , overconfident Ashton Kutcher . The problem is he comes off as kid who thinks he 's better than anyone else around him and shows no signs of a cluttered closet . His only obstacle appears to be winning over Costner . Finally when we are well past the half way point of this stinker , Costner tells us all about Kutcher 's ghosts . We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing . No magic here , it was all I could do to keep from turning it off an hour in .\" → NEGATIVE (1.0) → [\"Costner\"/PER, \"Costner\"/PER, \"Ashton Kutcher\"/PER, \"Costner\"/PER, \"Costner\"/PER, \"Kutcher\"/PER, \"Kutcher\"/PER]\n",
            "['Sentence: \"Once again Mr. Costner has dragged out a movie for far longer than necessary . Aside from the terrific sea rescue sequences , of which there are very few I just did not care about any of the characters . Most of us have ghosts in the closet , and Costner 's character are realized early on , and then forgotten until much later , by which time I did not care . The character we should really care about is a very cocky , overconfident Ashton Kutcher . The problem is he comes off as kid who thinks he 's better than anyone else around him and shows no signs of a cluttered closet . His only obstacle appears to be winning over Costner . Finally when we are well past the half way point of this stinker , Costner tells us all about Kutcher 's ghosts . We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing . No magic here , it was all I could do to keep from turning it off an hour in .\"'/'NEGATIVE' (1.0), 'Span[3:4]: \"Costner\"'/'PER' (0.9991), 'Span[50:51]: \"Costner\"'/'PER' (1.0), 'Span[86:88]: \"Ashton Kutcher\"'/'PER' (0.9445), 'Span[124:125]: \"Costner\"'/'PER' (0.9983), 'Span[140:141]: \"Costner\"'/'PER' (1.0), 'Span[145:146]: \"Kutcher\"'/'PER' (0.9997), 'Span[153:154]: \"Kutcher\"'/'PER' (1.0)]\n",
            "2023-01-09 13:31:01,900 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "Sentence: \"Once again Mr. Costner has dragged out a movie for far longer than necessary .\" → NEGATIVE (0.999) → [\"Costner\"/PER]\n",
            "['Sentence: \"Once again Mr. Costner has dragged out a movie for far longer than necessary .\"'/'NEGATIVE' (0.999), 'Span[3:4]: \"Costner\"'/'PER' (0.9994)]\n",
            "Sentence: \"Aside from the terrific sea rescue sequences , of which there are very few I just did not care about any of the characters .\" → NEGATIVE (1.0)\n",
            "['Sentence: \"Aside from the terrific sea rescue sequences , of which there are very few I just did not care about any of the characters .\"'/'NEGATIVE' (1.0)]\n",
            "Sentence: \"Most of us have ghosts in the closet , and Costner 's character are realized early on , and then forgotten until much later , by which time I did not care .\" → NEGATIVE (1.0) → [\"Costner\"/PER]\n",
            "['Sentence: \"Most of us have ghosts in the closet , and Costner 's character are realized early on , and then forgotten until much later , by which time I did not care .\"'/'NEGATIVE' (1.0), 'Span[10:11]: \"Costner\"'/'PER' (1.0)]\n",
            "Sentence: \"The character we should really care about is a very cocky , overconfident Ashton Kutcher .\" → NEGATIVE (0.9915) → [\"Ashton Kutcher\"/PER]\n",
            "['Sentence: \"The character we should really care about is a very cocky , overconfident Ashton Kutcher .\"'/'NEGATIVE' (0.9915), 'Span[13:15]: \"Ashton Kutcher\"'/'PER' (0.9799)]\n",
            "Sentence: \"The problem is he comes off as kid who thinks he 's better than anyone else around him and shows no signs of a cluttered closet .\" → NEGATIVE (0.9198)\n",
            "['Sentence: \"The problem is he comes off as kid who thinks he 's better than anyone else around him and shows no signs of a cluttered closet .\"'/'NEGATIVE' (0.9198)]\n",
            "Sentence: \"His only obstacle appears to be winning over Costner .\" → POSITIVE (0.9816) → [\"Costner\"/PER]\n",
            "['Sentence: \"His only obstacle appears to be winning over Costner .\"'/'POSITIVE' (0.9816), 'Span[8:9]: \"Costner\"'/'PER' (0.9929)]\n",
            "Sentence: \"Finally when we are well past the half way point of this stinker , Costner tells us all about Kutcher 's ghosts .\" → NEGATIVE (0.9605) → [\"Costner\"/PER, \"Kutcher\"/PER]\n",
            "['Sentence: \"Finally when we are well past the half way point of this stinker , Costner tells us all about Kutcher 's ghosts .\"'/'NEGATIVE' (0.9605), 'Span[14:15]: \"Costner\"'/'PER' (1.0), 'Span[19:20]: \"Kutcher\"'/'PER' (0.9998)]\n",
            "Sentence: \"We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing .\" → POSITIVE (0.9991) → [\"Kutcher\"/PER]\n",
            "['Sentence: \"We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing .\"'/'POSITIVE' (0.9991), 'Span[4:5]: \"Kutcher\"'/'PER' (1.0)]\n",
            "Sentence: \"No magic here , it was all I could do to keep from turning it off an hour in .\" → NEGATIVE (0.9999)\n",
            "['Sentence: \"No magic here , it was all I could do to keep from turning it off an hour in .\"'/'NEGATIVE' (0.9999)]\n",
            "Sentence: \"\"\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "# TODO add code here to obtain an output similar to the one below\n"
      ]
    }
  ]
}