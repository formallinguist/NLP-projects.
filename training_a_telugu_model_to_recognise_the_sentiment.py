# -*- coding: utf-8 -*-
"""Training_a Telugu_model_to_recognise_the_sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_0lqt8UfJRBwxAG-7vi4uIUv53alxcBV
"""

import pandas as pd

data = pd.read_excel("/content/Telugu.xlsx")

data.head()

#converting the labels to numbers
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
data['labels'] = le.fit_transform(data.label.values)

data.head()

from sklearn.utils import shuffle
data = shuffle(data)

sentences = data['text'].tolist()
labels = data['labels'].tolist()

!pip install Keras-Preprocessing
from keras_preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer

#from tensorflow.keras.preprocessing.sequence import pad_Sequences
tokenizer = Tokenizer(oov_token = "<OOV>")
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index

print(word_index)

sequences = tokenizer.texts_to_sequences(sentences)
padded = pad_sequences(sequences,padding = 'post')
print(padded[0])
print(padded.shape)

"""#Splitting of the data to train and test, howeveer there is a problem with the tokenizer as it has to see only the train data not the test data."""

training_size = 210

training_set = sentences[0:training_size]
testing_set = sentences[training_size:]
training_labels = labels[0:training_size]
testing_labels = labels[training_size:]

print(training_set)

len(training_set)

print(training_labels)

"""#In this modified code we see that the tokenizer is trained on the train data."""

vocab_size = 210
embedding_dim = 16
max_length = 100
trunc_type='post'
padding_type='post'
oov_tok = "<OOV>"
training_size = 210

tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)
tokenizer.fit_on_texts(training_set)
word_index = tokenizer.word_index

training_sequences = tokenizer.texts_to_sequences(training_set)
training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

testing_sequences = tokenizer.texts_to_sequences(testing_set)
testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

# Need this block to get it to work with TensorFlow 2.x
import numpy as np
training_padded = np.array(training_padded)
training_labels = np.array(training_labels)
testing_padded = np.array(testing_padded)
testing_labels = np.array(testing_labels)

"""#Context of the words is attained by embeddings."""

import tensorflow as tf
import keras
#model = keras.models.load_model('my_model.h5', custom_objects={'tf': tf})

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(24, activation = 'relu'),
    tf.keras.layers.Dense(1, activation = 'sigmoid')
])
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

model.summary()

type(training_padded)

type(training_labels)

type(testing_padded)

type(testing_labels)

num_epochs = 30
history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)

sentence = ["మధ్య తరగతి వాళ్ళకి ఉపయోగకరమైనది కాని అందులో USB కనెక్షన్ తేడాగా ఉంది  USB 1 కు కనెక్ట్ అయ్యినప్పుడు అది స్పందిచలేదు USB 2 కి కనెక్ట్ చేసినప్పుడు tv  కనెక్ట్ అయ్యింది  ఎందుకంటే ఇది 2 కి కనెక్ట్ చేయబడింది  తారుమారుగా ఉంది కాని ఇది బడ్జెట్ వినియోగదారుల కోరకు  మంచి స్పష్టత  మంచి సౌండ్  మంచి ధర "]

sequences = tokenizer.texts_to_sequences(sentence)

padded = pad_sequences(sequences,maxlen = max_length,padding = padding_type, truncating = trunc_type)

print(model.predict(padded))