{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/formallinguist/NLP-projects./blob/main/PDF_reading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cbgwZWWfWpp"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fitz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQdoTG4PHl9B",
        "outputId": "f3a79704-5b50-4bba-a6c0-31bc178ce61b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fitz\n",
            "  Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
            "Collecting configparser\n",
            "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
            "Collecting pyxnat\n",
            "  Downloading pyxnat-1.4.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (from fitz) (3.0.2)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.7/dist-packages (from fitz) (0.17.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fitz) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fitz) (1.1.5)\n",
            "Collecting configobj\n",
            "  Downloading configobj-5.0.6.tar.gz (33 kB)\n",
            "Collecting nipype\n",
            "  Downloading nipype-1.7.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 11.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fitz) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from configobj->fitz) (1.15.0)\n",
            "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (7.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (2.8.2)\n",
            "Collecting etelemetry>=0.2.0\n",
            "  Downloading etelemetry-0.3.0-py3-none-any.whl (6.3 kB)\n",
            "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (1.3.0)\n",
            "Collecting traits!=5.0,>=4.6\n",
            "  Downloading traits-6.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.1 MB 70.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (2.6.3)\n",
            "Collecting prov>=1.5.2\n",
            "  Downloading prov-2.0.0-py3-none-any.whl (421 kB)\n",
            "\u001b[K     |████████████████████████████████| 421 kB 81.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (3.4.2)\n",
            "Collecting rdflib>=5.0.0\n",
            "  Downloading rdflib-6.1.1-py3-none-any.whl (482 kB)\n",
            "\u001b[K     |████████████████████████████████| 482 kB 92.3 MB/s \n",
            "\u001b[?25hCollecting simplejson>=3.8.0\n",
            "  Downloading simplejson-3.17.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 79.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from etelemetry>=0.2.0->nipype->fitz) (2.23.0)\n",
            "Collecting ci-info>=0.2\n",
            "  Downloading ci_info-0.2.0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: lxml>=3.3.5 in /usr/local/lib/python3.7/dist-packages (from prov>=1.5.2->nipype->fitz) (4.2.6)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot>=1.2.3->nipype->fitz) (3.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib>=5.0.0->nipype->fitz) (57.4.0)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 594 kB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib>=5.0.0->nipype->fitz) (4.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib>=5.0.0->nipype->fitz) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib>=5.0.0->nipype->fitz) (3.7.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->fitz) (2018.9)\n",
            "Collecting lxml>=3.3.5\n",
            "  Downloading lxml-4.7.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 87.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.7/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Requirement already satisfied: future>=0.16 in /usr/local/lib/python3.7/dist-packages (from pyxnat->fitz) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->etelemetry>=0.2.0->nipype->fitz) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->etelemetry>=0.2.0->nipype->fitz) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->etelemetry>=0.2.0->nipype->fitz) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->etelemetry>=0.2.0->nipype->fitz) (2021.10.8)\n",
            "Building wheels for collected packages: configobj, pyxnat\n",
            "  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configobj: filename=configobj-5.0.6-py3-none-any.whl size=34547 sha256=bc8815ef54fe328c57e93fdbf16787172b559be2e77be8a17f3afa85c8b2986c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/c4/19/13d74440f2a571841db6b6e0a273694327498884dafb9cf978\n",
            "  Building wheel for pyxnat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyxnat: filename=pyxnat-1.4-py3-none-any.whl size=92687 sha256=948ddedb7203c79e5140d0ea54d3db591cedaff5e7c47ea78e639f7a793b6469\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/d3/8f/51847ef95ec6448ff7599bf269145f8290fa82f1ddf00ee90c\n",
            "Successfully built configobj pyxnat\n",
            "Installing collected packages: isodate, rdflib, lxml, ci-info, traits, simplejson, prov, etelemetry, pyxnat, nipype, configparser, configobj, fitz\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed ci-info-0.2.0 configobj-5.0.6 configparser-5.2.0 etelemetry-0.3.0 fitz-0.0.1.dev2 isodate-0.6.1 lxml-4.7.1 nipype-1.7.0 prov-2.0.0 pyxnat-1.4 rdflib-6.1.1 simplejson-3.17.6 traits-6.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzoDmG7MJItb",
        "outputId": "1d57382e-a8b4-4c8a-e17c-2131a315d9c2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 4.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.19.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz"
      ],
      "metadata": {
        "id": "gHIhNR9BIQW9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = fitz.open('/content/math.pdf')"
      ],
      "metadata": {
        "id": "lwQ8x8seJN94"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNyr1Jc4JtH9",
        "outputId": "15f6cac6-84e4-4f79-874c-062df4c57ca4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "194"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \" \"\n",
        "for page in doc:\n",
        "  text = text+str(page.getText())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6WeMZh_Jzy9",
        "outputId": "ff6af28e-832f-4dbb-98eb-57071b8baeae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Deprecation: 'getText' removed from class 'Page' after v1.19 - use 'get_text'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "XldcGHtdMtA1",
        "outputId": "a6a88071-d434-4adf-958e-c488042f113c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Introduction to the Mathematics of Language\\nMichael Hammond\\nU. of Arizona\\nDecember 24, 2015\\nContents\\n1\\nOverview\\n5\\n1.1\\nA Scientiﬁc Approach . . . . . . . . . . . . . . . . . . . . . . .\\n5\\n1.2\\nOther Reasons for Formal Theories . . . . . . . . . . . . . . .\\n7\\n1.3\\nAn example . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n8\\n1.4\\nOrganization of the Book . . . . . . . . . . . . . . . . . . . . .\\n8\\n1.5\\nAcknowledgements . . . . . . . . . . . . . . . . . . . . . . . .\\n9\\n2\\nLanguage\\n10\\n2.1\\nCommon Misconceptions . . . . . . . . . . . . . . . . . . . . .\\n10\\n2.1.1\\nLearning is memorization . . . . . . . . . . . . . . . . .\\n10\\n2.1.2\\nCorrection is necessary to learn language . . . . . . . .\\n11\\n2.1.3\\nSome people have bad grammar . . . . . . . . . . . . .\\n12\\n2.1.4\\nTwo negatives make a positive . . . . . . . . . . . . . .\\n13\\n2.1.5\\nI don’t have an accent\\n. . . . . . . . . . . . . . . . . .\\n14\\n2.1.6\\nSome languages are logical . . . . . . . . . . . . . . . .\\n15\\n2.1.7\\nSome languages are primitive\\n. . . . . . . . . . . . . .\\n15\\n2.2\\nKey Notions . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n16\\n2.2.1\\nWhat is Language? . . . . . . . . . . . . . . . . . . . .\\n17\\n2.2.2\\nCreativity . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n2.2.3\\nPrescriptive vs. Descriptive Grammar . . . . . . . . . .\\n19\\n2.2.4\\nCompetence and Performance . . . . . . . . . . . . . .\\n19\\n2.3\\nGeneral Areas . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\n2.3.1\\nStructures of Language . . . . . . . . . . . . . . . . . .\\n21\\n2.3.2\\nOther Areas . . . . . . . . . . . . . . . . . . . . . . . .\\n28\\n2.4\\nSummary\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n29\\n2.5\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n30\\n1\\nCONTENTS\\n2\\n3\\nSet Theory\\n31\\n3.1\\nSets\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n31\\n3.2\\nMembership . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n34\\n3.3\\nOperations . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n35\\n3.4\\nFundamental Set-theoretic Equalities . . . . . . . . . . . . . .\\n36\\n3.5\\nTheorems\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n38\\n3.6\\nOrdered Pairs, Relations, and Functions\\n. . . . . . . . . . . .\\n39\\n3.7\\nLanguage examples . . . . . . . . . . . . . . . . . . . . . . . .\\n41\\n3.7.1\\nExamples of Sets . . . . . . . . . . . . . . . . . . . . .\\n41\\n3.7.2\\nExamples of Relations and Functions . . . . . . . . . .\\n42\\n3.8\\nSummary\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n44\\n3.9\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n4\\nSentential Logic\\n46\\n4.1\\nThe intuition\\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\\n46\\n4.2\\nBasic Syntax\\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\\n46\\n4.2.1\\nSimplicity . . . . . . . . . . . . . . . . . . . . . . . . .\\n48\\n4.2.2\\nLack of Ambiguity\\n. . . . . . . . . . . . . . . . . . . .\\n49\\n4.2.3\\nArtiﬁciality\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n4.3\\nBasic Semantics . . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n4.4\\nThe Meanings of the Connectives . . . . . . . . . . . . . . . .\\n53\\n4.4.1\\nNegation . . . . . . . . . . . . . . . . . . . . . . . . . .\\n54\\n4.4.2\\nConjunction . . . . . . . . . . . . . . . . . . . . . . . .\\n54\\n4.4.3\\nDisjunction\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n55\\n4.4.4\\nConditional . . . . . . . . . . . . . . . . . . . . . . . .\\n55\\n4.4.5\\nBiconditional\\n. . . . . . . . . . . . . . . . . . . . . . .\\n55\\n4.5\\nHow Many Connectives? . . . . . . . . . . . . . . . . . . . . .\\n56\\n4.6\\nTautology, Contradiction, Contingency . . . . . . . . . . . . .\\n58\\n4.7\\nProof . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n59\\n4.8\\nRules of Inference . . . . . . . . . . . . . . . . . . . . . . . . .\\n68\\n4.9\\nConditional Proof . . . . . . . . . . . . . . . . . . . . . . . . .\\n71\\n4.10 Indirect Proof . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n73\\n4.11 Language\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n74\\n4.12 Summary\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n75\\n4.13 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n76\\nCONTENTS\\n3\\n5\\nPredicate Logic\\n77\\n5.1\\nSyntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n77\\n5.2\\nSemantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n79\\n5.3\\nLaws and Rules . . . . . . . . . . . . . . . . . . . . . . . . . .\\n84\\n5.3.1\\nLaws . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n84\\n5.3.2\\nRules of Inference . . . . . . . . . . . . . . . . . . . . .\\n88\\n5.4\\nProofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n89\\n5.4.1\\nIndirect Proof . . . . . . . . . . . . . . . . . . . . . . .\\n91\\n5.4.2\\nConditional Proof . . . . . . . . . . . . . . . . . . . . .\\n92\\n5.5\\nSummary\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n92\\n5.6\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n93\\n6\\nFormal Language Theory\\n96\\n6.1\\nLanguages . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n96\\n6.2\\nGrammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n97\\n6.3\\nFinite State Automata . . . . . . . . . . . . . . . . . . . . . . 105\\n6.4\\nRegular Languages . . . . . . . . . . . . . . . . . . . . . . . . 112\\n6.5\\nAutomata and Regular Languages . . . . . . . . . . . . . . . . 113\\n6.6\\nRight-linear Grammars and Automata\\n. . . . . . . . . . . . . 115\\n6.7\\nClosure Properties of Regular Languages . . . . . . . . . . . . 115\\n6.8\\nPumping Lemma for Regular Languages\\n. . . . . . . . . . . . 117\\n6.9\\nRelevance of Regular Languages . . . . . . . . . . . . . . . . . 119\\n6.10 Summary\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\\n6.11 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\\n7\\nFormal Language Theory Continued\\n124\\n7.1\\nContext-free Languages . . . . . . . . . . . . . . . . . . . . . . 124\\n7.2\\nPushdown Automata . . . . . . . . . . . . . . . . . . . . . . . 126\\n7.3\\nEquivalence of Non-deterministic PDAs and CFGs . . . . . . . 129\\n7.3.1\\nCFG to PDA . . . . . . . . . . . . . . . . . . . . . . . 130\\n7.3.2\\nPDA to CFG . . . . . . . . . . . . . . . . . . . . . . . 132\\n7.4\\nClosure Properties of Context-free Languages\\n. . . . . . . . . 135\\n7.5\\nPumping Lemma for Context-free Languages . . . . . . . . . . 136\\n7.6\\nNatural Language Syntax\\n. . . . . . . . . . . . . . . . . . . . 138\\n7.7\\nDeterminism . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\\n7.8\\nOther Machines . . . . . . . . . . . . . . . . . . . . . . . . . . 140\\n7.9\\nSummary\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\\n7.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\\nCONTENTS\\n4\\n8\\nProbability\\n147\\n8.1\\nWhat is it and why should we care? . . . . . . . . . . . . . . . 147\\n8.2\\nBasic Probability . . . . . . . . . . . . . . . . . . . . . . . . . 149\\n8.3\\nCombinatorics . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\\n8.4\\nLaws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\\n8.5\\nA Tricky Example . . . . . . . . . . . . . . . . . . . . . . . . . 156\\n8.6\\nConditional Probability . . . . . . . . . . . . . . . . . . . . . . 157\\n8.7\\nDistributions\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 160\\n8.8\\nSummary\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165\\n8.9\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165\\n9\\nProbabilistic Language Models\\n167\\n9.1\\nThe Chain Rule . . . . . . . . . . . . . . . . . . . . . . . . . . 167\\n9.2\\nN-gram models\\n. . . . . . . . . . . . . . . . . . . . . . . . . . 168\\n9.2.1\\nUnigrams\\n. . . . . . . . . . . . . . . . . . . . . . . . . 168\\n9.2.2\\nBigrams . . . . . . . . . . . . . . . . . . . . . . . . . . 170\\n9.2.3\\nHigher-order N-grams\\n. . . . . . . . . . . . . . . . . . 175\\n9.2.4\\nN-gram approximation . . . . . . . . . . . . . . . . . . 175\\n9.3\\nHidden Markov Models . . . . . . . . . . . . . . . . . . . . . . 177\\n9.3.1\\nMarkov Chains . . . . . . . . . . . . . . . . . . . . . . 178\\n9.3.2\\nHidden Markov Models . . . . . . . . . . . . . . . . . . 179\\n9.3.3\\nFormal HMM properties . . . . . . . . . . . . . . . . . 180\\n9.3.4\\nBigrams and HMMs\\n. . . . . . . . . . . . . . . . . . . 181\\n9.3.5\\nHigher-order N-grams\\n. . . . . . . . . . . . . . . . . . 182\\n9.4\\nProbabilistic Context-free Grammars . . . . . . . . . . . . . . 183\\n9.5\\nSummary\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\\n9.6\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\\nChapter 1\\nOverview\\nThe investigation of language can be carried out in many diﬀerent ways. For\\nexample, we might learn to speak a language as a way to understand it.\\nWe might also study literature and poetry to the same end. We might even\\nwrite novels and poetry to understand the intricacies of expression that some\\nlanguage provides.\\nThese are valuable pursuits, not to be denigrated in any way, but they\\ndo not provide for a scientiﬁc understanding of language, one where we can\\nmake falsiﬁable or testable claims about our object of study. A falsiﬁable\\nclaim is one that can be disproven with real data. For example, if we foolishly\\nhypothesize that Shakespeare wrote good poetry because he wrote in English,\\nwe would need some objective way to assess how “good” some piece of poetry\\nis independent of the language the author wrote in. If the hypothesis rested\\ninstead entirely on our own ideas about how good individual poems are, then\\nit would surely not be falsiﬁable, and thus not be science.\\n1.1\\nA Scientiﬁc Approach\\nThere are, of course, a number of ways to do science with language. We\\nmight investigate the range of sentence types that some speaker can produce\\nor that some corpus contains. For example, do male or female characters in\\nShakespeare have longer sentences? We might look at the set of sounds that\\ncan occur in any one language, or the set of sounds used in some speciﬁc\\npoetic context.\\nDo the languages of Europe have more vowels than the\\nlanguages of India? We might want to investigate the time course of language\\n5\\nCHAPTER 1. OVERVIEW\\n6\\nacquisition or of language change. Do children learn all the vowels of English\\nbefore or after they learn all the consonants? Are vowels or consonants more\\n“stable” over the history of the English language?\\nAs we lay out these diﬀerent questions, we beg the question of why we\\nmight expect diﬀerent outcomes. Why do we believe that language should\\nwork in any particular way? Our expectations about how language should\\nbehave are our theory of language. For example, if we hypothesize, for ex-\\nample, that all languages have the vowel [a], as in English father, then we\\nhave an implicit theory about how language works and about the vowel [a].\\nIf we believe that male or female characters have longer sentences in Shake-\\nspeare’s plays, then we have a theory of sentence length and its relationship\\nto gender.\\nThese expectations can be fairly informal and intuitive. For example, we\\nmight believe that [a] is a very easy vowel to produce and that languages\\nmake use of easier sounds before making use of harder sounds. With respect\\nto sentence length, we might have some idea about the role female characters\\nplayed in Shakespeare and how that would be reﬂected in the language of\\nthose characters.\\nAs we proceed along these lines, as we make, test, and reﬁne our hy-\\npotheses about language, it behoves us to make our theory of language more\\nexplicit. The more explicit our theory of language is, the more falsiﬁable it\\nis. As scientists, we want our theory of language to be as testable as possible.\\nAs our theory becomes more explicit, it tends to become more formal.\\nA formal theory is one where the components of the theory are cast in a\\nrestricted metalanguage, e.g. math or logic.\\nThe languages of math and\\nlogic are quite precise. A theory cast in those terms can make very speciﬁc\\npredictions that are quite falsiﬁable. A theory cast in normal English can\\nbe quite vague. An analogy would be the language of contracts. Typically,\\nsuch documents are quite hard to make out for laymen as they are written\\nusing very speciﬁc language that lawyers have very speciﬁc interpretations\\nfor. While this kind of language can be quite frustrating for the rest of us,\\nit is essential. The legal metalanguage of contracts provides a mechanism\\nwhereby our rights and commitments can be negotiated clearly in contracts.\\nLikewise, mathematical and logical formalisms can appear daunting as\\nparts of a theory of language, but they can be indispensable to making a\\ntheory maximally falsiﬁable.\\nUnderstanding the formalisms used in theories of language requires spe-\\ncialized knowledge of various formal domains. That is the point of this book.\\nCHAPTER 1. OVERVIEW\\n7\\n1.2\\nOther Reasons for Formal Theories\\nIntuitively, formalism is all the symbols, all the stuﬀ that looks like math. As\\nwe’ve discussed in the previous section, the formalism is just a mechanism\\nfor being maximally explicit. If we want to build theories that we can test in\\nprecise ways, then we need to have theories that are as precise as possible.\\nA proper formalization enables us to do this.\\nFormalization also enables us to implement our theories computationally.\\nWe might want to write a computer program that mimics some aspect of\\nthe grammar of a language, its sentences, words, or sounds. For example,\\nimagine we wanted to write a program that would produce poetry. We would\\nsupply a vocabulary and the program would spit back a random poem. This\\nmay seem rather silly, but is, in fact, a hugely complicated task. We would\\nhave to provide the program with some deﬁnition of what constitutes poetry.\\nTo the extent that this deﬁnition had any content, the task becomes quite\\ncomplex.\\nFor example, how do we deﬁne “rhyme”?\\nHow do we get the\\nprogram to produce grammatical sentences and not just random strings of\\nwords? How do we get the program to ﬁgure out how words are actually\\npronounced? Computer programs are merciless in requiring speciﬁc answers\\nto questions like these and an inexplicit theory of language will be of little\\nhelp.\\nWhy would we want to write such programs? There are two broad rea-\\nsons. The ﬁrst is that it is another way to test our understanding of our\\ntheory of language. For example, if we have the wrong theory of rhyme, then\\nour program would produce bad poetry. The second is that we may actually\\nwant to do something useful in the outside world with our theories, use them\\nfor some purpose other than the pursuit of “truth”. While a program that\\nwrote poetry would seem of little use, a program that examined text and\\nidentiﬁed it as poetry or not might be of great use. In either case, being as\\nexplicit as possible in our formalization of our theory makes implementing\\nthat theory as painless as possible.\\nAnother reason why we formalize theories of language is to understand\\nthe general character of formalization better. For example, if we challenge\\nourselves to characterize some aspect of language in terms of ﬁrst-order logic1,\\nwe may ﬁnd out something about logic too. Thus formalization of theories\\nof language can also tell us about math and logic more generally.\\n1More on this in chapters 4 and 5 below.\\nCHAPTER 1. OVERVIEW\\n8\\n1.3\\nAn example\\nLet’s consider a simple example. Pretty much anyone would agree that “two\\ntimes two equals four” is the same thing as 2 × 2 = 4. In a relatively simple\\ndomain like multiplication, there does not appear to be much to be gained by\\ntranslating words into an equation. Consider on the other hand a sentence\\nlike the following:\\n(1.1)\\nTwo women saw two men.\\nIf we are interested in the meanings of sentences, then a bit of formalism\\nmight be helpful in characterizing what such a sentence can mean. In the\\ncase at hand, this sentence has at least two diﬀerent interpretations. First,\\nthe sentence could mean that there are precisely two women and precisely\\ntwo men and each of the two men were seen by at least one of the women.\\nHowever, there is another possible interpretation where there are only two\\nwomen, but up to four men. Each woman saw two men, but they may not\\nhave been the same men. We can make this a little clearer if we assign names\\nto the individuals: Mary, Molly, Mark, Mike, Mitch, and Matt.\\n(1.2)\\nFirst interpretation: Mary and Molly saw Mark and Mike.\\n(1.3)\\nSecond interpretation: Mary saw Mark and Mike, and Molly\\nsaw Mitch and Matt.\\nThe distinction is a subtle one and can get quite intricate with more complex\\nsentences. Thus characterizing the meanings of sentences might require some\\nfairly elaborate formal machinery.\\n1.4\\nOrganization of the Book\\nThe main topics we cover in this book are the following.\\nSet Theory Abstract theory of elements and groups. Underlines virtually\\neverything else we’ll treat.\\nLogic A formal system with which we can argue and reason.\\nThere are\\ntwo ways to look at this. One way to look at it is as a formal system\\nCHAPTER 1. OVERVIEW\\n9\\nthat underlies how we make arguments. However, another way to look\\nat it is as a “perfect language” with syntax and semantics rigorously\\ndeﬁned.\\nFormal Language Theory An explicit way to look at languages and the\\ncomplexity of their description with an eye to how we might compute\\nthings about those languages.\\nProbability How to be explicit about likelihood, essential for experimental\\ndisciplines, linguistic behavior, language change, sociolinguistics, . . . .\\n1.5\\nAcknowledgements\\nThanks to Andrew Carnie, Chris Cronenberg, Heidi Harley, Shiloh Mossie,\\nand Diane Ohala for useful feedback and discussion.\\nThanks also to the\\nstudents in Linguistics 178 and 501 at the University of Arizona. All errors\\nare, of course, my own.\\nChapter 2\\nLanguage\\nIn this chapter, we outline how language works in very general terms. We\\nbegin with a tour of some of the more common and egregious misunderstand-\\nings about language and then progress to the key notions of any theory of\\nlanguage. Finally, we brieﬂy review the principal areas of language study.\\n2.1\\nCommon Misconceptions\\nLanguage is a wonderful domain to do science with. All humans have lan-\\nguage in some sense, so the data one might need to test one’s hypotheses\\nare ready to hand. Unfortunately, there is a great deal of misunderstanding\\nabout how language works. We need to break through these misunderstand-\\nings before we can appreciate the formal structures of language.\\n2.1.1\\nLearning is memorization\\nLet’s begin with a fairly common notion: children learn language by memo-\\nrizing words and sentences. The basic idea here is that all there is to learning\\na language is memorizing the words and sentences one hears.\\nThere are several problems with this view of language learning. First,\\nit is not altogether clear what is intended by memorization, but the most\\nobvious interpretation of this term would have to be incorrect. Memorization\\nimplies a conscious active eﬀort to retain something in memory, but language\\nlearning seems to happen in a rather passive fashion. Children do not appear\\nto spend any eﬀort in acquiring the languages they are exposed to, yet they\\n10\\nCHAPTER 2. LANGUAGE\\n11\\nwill learn any number of them simultaneously.\\nMemorization is certainly a tool by which things can be learned, but it\\nis much more appropriate as a way of characterizing how we learn, say, the\\nmultiplication tables, rather than how we learn language.\\nAnother problem with the memorization story is that children produce\\nthings that they haven’t been exposed to. For example, it is quite typical that\\nat an early stage of acquisition children will produce forms like foots for the\\nadult form feet, or goed for adult went, etc. This has no explanation on the\\nmemorization view. The child has presumably only heard the correct adult\\nforms feet, went, etc., yet produces forms that they’ve never been exposed to.\\nPresumably, in the case of foots, the child is attempting to produce the plural\\nform by the general rule of adding an –s, despite having heard the irregular\\nplural form feet. Likewise, in the case of goed, the child is attempting to\\nproduce the past tense by means of the general rule that says to add –ed.\\nSimilar examples can be constructed at the sentence level. Imagine the\\nchild has been exposed to a novel situation, say, a new toy or a new person in\\ntheir life. The child isn’t stumped by these sorts of situations, but constructs\\nsentences appropriate to the new individuals.\\nThat is, the child has no\\ntrouble saying that Ernie is in the kitchen, even if the child has just met\\nErnie and it’s the ﬁrst time he’s been in the kitchen.\\nBoth sorts of example suggest that learning is at least partially a process\\nof generalizing from what the child has been exposed to, not simple rote\\nmemorization of that exposure.\\n2.1.2\\nCorrection is necessary to learn language\\nAnother common misconception related to language learning is that children\\nneed to be corrected by adults to learn their language properly. We’ve already\\ncited examples where children produce forms that are incorrect in the adult\\nlanguage, e.g. foots. It is reasonable—though incorrect—to suppose that they\\nneed explicit correction to produce the correct form.\\nThere are two glaring problems with this view. First, not all parents cor-\\nrect their children, yet these sorts of errors disappear in the adult language,\\ntypically by age three or four.\\nAnother perhaps rather surprising fact is that children typically ignore\\nany attempt at correction. The literature is rife with anecdotes of children\\nbeing painstakingly corrected for such forms and then blithely continuing to\\nuse them.\\nCHAPTER 2. LANGUAGE\\n12\\nThe point is that these sorts of “errors” are a natural part of language\\nacquisition. They develop and fall away of their own accord, and parental\\ncorrection plays essentially no role in this.\\n2.1.3\\nSome people have bad grammar\\nMost of us believe that some people have “bad grammar”. This is a very\\ntricky notion so it will be useful to go through some examples. Consider the\\nfollowing pair of sentences.\\n(2.1)\\nYou and me are studying logic.\\nYou and I are studying logic.\\nThe two sentences mean the same thing. Both are used in English. Some\\nEnglish speakers prefer to use one over the other and some speakers will use\\nboth in diﬀerent contexts. If a speaker does use both, then the diﬀerence is\\none of register or modality. The ﬁrst is more typical of speech, rather than\\nwriting. The ﬁrst is also more informal than the second.\\nIs either form better than the other? No. It is certainly the case that\\nthey are contextually restricted, but there is no objective scientiﬁc reason to\\nmaintain that either structure is “better” than the other.\\nOne might note that the second sentence type is older while the ﬁrst type\\nis newer. This is indeed correct, but it does not make either structure better\\nor worse than the other. In fact, there are cases that work the other way.\\nFor example, the word ain’t is generally thought to be “bad grammar”, but\\nis well attested in the history of English.\\nThe idea that some constructions are “bad” has led to some interesting\\ndevelopments in the history of English. In the case of the X and me pattern\\nabove, the idea that X and I is to be preferred to X and me has led to\\nsome strange reversals. For example, the ﬁrst sentence type below is what is\\nattested historically, yet the second sentence type now shows up quite often.\\n(2.2)\\nBetween you and me, logic is interesting.\\nBetween you and I, logic is interesting.\\nIn fact, for some speakers, the avoidance of X and I goes further, including\\npairs like the following.\\n(2.3)\\nLogic interests you and me.\\nLogic interests you and I.\\nCHAPTER 2. LANGUAGE\\n13\\nHere, as with the previous pair, the ﬁrst of the two sentences is what is\\nattested historically. What’s happened is that ﬁrst X and I extends to new\\nenvironments, as in the ﬁrst pair. Presumed “language authorities” decry this\\ndevelopment and speakers “overcorrect” to avoid the supposed bad structure.\\nThis is called hypercorrection.\\nIn all these cases, it is important to keep in mind that neither structure\\nis intrinsically better or worse than the other.\\nThere are contextual and\\nstylistic diﬀerences, especially in writing, but these are matters of style and\\ncustom, not a matter of goodness or badness.\\n2.1.4\\nTwo negatives make a positive\\nConsider a sentence like the following:\\n(2.4)\\nErnie didn’t see nothing.\\nSentences of this sort are often cited as examples of the illogic of bad gram-\\nmar.\\nA sentence like this—with the interpretation that Ernie didn’t see\\nanything—is claimed to be an instance of bad grammar. It is bad grammar\\nbecause it is said to be illogical. It is taken to be illogical because there is an\\nextra negative, didn’t and nothing, and the extra negative should make the\\nsentence positive.\\nIt is indeed the case that two negatives sometimes make a positive. For\\nexample. If I assert that Ernie saw no chickens when he was in the yard and\\nyou know this to be false, you might then say:\\n(2.5)\\nErnie did not see no chickens when he was in the yard.\\nHere, the ﬁrst negative not denies the second negative no. This is in contrast\\nto the preceding example, where the ﬁrst negative didn’t emphasizes the\\nsecond negative nothing.\\nIt is certainly the case that using a second negative for emphasis is stylis-\\ntically marked in English. It is indicative of speech, rather than writing, and\\nit is more casual. But is there something illogical about using a negative\\nword for emphasis? No. Words and structures in a language can be used\\nfor many purposes and there is no particular reason why a word that has a\\nnegative meaning in one context cannot have another meaning in some other\\ncontext.\\nCHAPTER 2. LANGUAGE\\n14\\nIn fact, this pattern is quite common in other languages. French, Spanish,\\nand Russian all use two separate negative words to express nothing.\\n(2.6)\\nFrench:\\nErnie n’a vu rien.\\nSpanish:\\nErnie no vio nada.\\nRussian:\\nErnie nichego ne videl.\\nThus using two negative words to express a single negative idea is quite\\nnormal crosslinguistically.\\nIt is true that the logical system we introduce in chapter 4 has the prop-\\nerty that stacking up two negatives is equivalent to having no negatives, e.g.\\n(¬¬p ↔ p). This simply means that logical negation is more restricted in\\nhow it is to be interpreted than negative words in human languages.1\\n2.1.5\\nI don’t have an accent\\nEveryone speaks their own language diﬀerently from other speakers of the\\nsame language. These diﬀerences can be in what words a person uses, how\\nthey pronounce those words, how words are combined to form sentences, etc.\\nSome of these diﬀerences are completely idiosyncratic and reﬂect individual\\nvariation. Some of these diﬀerences reﬂect a speaker’s geographic or social\\norigins. A person’s completely unique language is termed an idiolect; a set\\nof features that reﬂect a particular geographic area and/or social distinction\\nis called a dialect.\\nLet’s ﬁrst look at some pronunciation diﬀerences. Some speakers in the\\nsouthern US pronounce words like pen and pin the same, while most north-\\nern speakers keep these distinct. Likewise, some northern speakers make a\\ndistinction between the initial sounds of words like witch and which, while\\nother speakers do not.\\nThere are lots of regional diﬀerences in terms of word choice. For example,\\ndialects vary dramatically in how they refer to a “soft drink”: soda, pop, or\\neven, used generically, coke. A very interesting example is bubbler which\\nmeans a drinking fountain. This term is only used in western Wisconsin and\\nNew England. The term apparently was the brand name for a particular\\ndrinking fountain that was sold in just those areas of the country.\\n1As you might predict, our logical system has no mechanism for expressing the emphasis\\nthat the double negative sometimes expresses in English.\\nCHAPTER 2. LANGUAGE\\n15\\nThere are also diﬀerences in terms of word order or grammar. For exam-\\nple, there are dialects of English in which the following are acceptable and\\nmean diﬀerent things:\\n(2.7)\\nErnie eat it.\\nErnie be eating it.\\nThe ﬁrst simply means that Ernie is eating something. The second means\\nthat Ernie habitually eats something. The latter structure is quite interesting\\nbecause the distinction is not one that most dialects of English make and\\nbecause the construction is stigmatized.\\nAnother construction that shows up in some dialects is the double modal\\nconstruction, e.g. I might could do that. Finally, there are a number of dialects\\nthat distinguish direct address to one person you vs. direct address to more\\nthan one person: you all, y’all, you guys, yous, etc.\\nThe point here is that everyone’s language reﬂects their geographic and\\nsocial origins. Diﬀerent dialects exhibit diﬀerent degrees of social acceptabil-\\nity, but there is no intrinsic qualitative diﬀerence between dialects, nor does\\nanyone not have a dialect.\\n2.1.6\\nSome languages are logical\\nIt is common to ascribe diﬀerent qualities to languages. For example: Ger-\\nman is ‘logical’, French is ‘beautiful’, or Russian is ‘harsh’. These terms have\\nnothing to do with the language in question and typically have much more\\nto do with our own prejudices about the people and culture.\\n2.1.7\\nSome languages are primitive\\nVarious languages and dialects are often decried as ‘primitive’. Usually, this\\nis an instance of the type of misconception just above. For example, Native\\nAmerican languages like Navajo or Tohono O’odham (Papago) are described\\nas primitive languages, not because there is anything especially primitive\\nabout them, but because of unfortunate ambient attitudes that Native Amer-\\nican cultures are primitive in some way.\\nSometimes the argument is more sophisticated. For example, the dialect\\nof English cited above where a sentence like Ernie eat it is acceptable is cited\\nas primitive because it is “missing” the –s suﬃx that occurs in more accepted\\nCHAPTER 2. LANGUAGE\\n16\\ndialects of English. Such arguments are specious and opportunistic. Thus,\\nin the case at hand, the dialect is cited as more primitive than other dialects\\nbecause it is missing the –s suﬃx, yet it could, by parity of argument, be\\ncited as less primitive than other dialects as it makes an aspectual distinction\\nwith be that other dialects do not make.\\nSometimes the argument is even put in the opposite direction! Eskimo is\\ncited as more primitive because it very quaintly has many words for snow,\\nwhile English presumably has fewer. In fact, it’s been shown that Eskimo\\ndoes not have any more words for snow than other languages (Pullum, 1991).\\nMoreover, people like skiers and snowplow drivers, whose jobs or recreation\\ndepend on snow, have many more words for snow. Is their English somehow\\nmore primitive?\\nOccasionally, the argument is made in terms of communicative limits. For\\nexample, one might argue that French is a highly evolved language since it\\nhas words for concepts like d´etente or ennui. This is specious too, however.\\nFrench does indeed have the words cited, but the concepts are not diﬃcult\\nto express in words in English: “a release from tension” or “weariness and\\ndissatisfaction”. Moreover, there are equivalent concepts that appear to be\\nhard to express in French, but are easy in English: weekend, parking (lot),\\netc.\\nEvery language will express some concepts as individual words and others\\nas combinations of words. It’s not at all apparent that there is any rational\\nbasis to which is which in any particular language.\\nFinally, the character or existence of a writing system is sometimes cited\\nas evidence of a language’s primitive nature. Thus, Navajo might be cited\\nas “primitive” because it did not have a writing system until fairly recently,\\nwhile English has had one for hundreds of years.\\nWriting systems are certainly a valuable piece of cultural technology, but\\nthat is probably the best way to describe them: as technology.\\nThus a\\nlanguage with a writing system has a tool at its disposal that is quite useful.\\nWhat’s important to keep in mind, however, is that the presence of a writing\\nsystem does not appear to correlate with any aspect of language structure.\\n2.2\\nKey Notions\\nIn the previous section, we spent a fair amount of time showing what language\\nis not. In this section, we proceed to deﬁne language in a positive fashion.\\nCHAPTER 2. LANGUAGE\\n17\\n2.2.1\\nWhat is Language?\\nWhat is language? Most people would deﬁne it as some sort of communica-\\ntion system. This is certainly true, but it is general enough to include other\\ncommunication systems as well. For example, is Morse Code an instance of\\nlanguage? Are traﬃc lights—red, yellow, and green—language?\\nTo distinguish what we think of as language from these other systems,\\nwe need to take account of the fact that language is a more complete system.\\nIn fact, language is arguably capable of expressing virtually any idea that a\\nhuman being is capable of thinking of. We can term this power expressive\\ncompleteness and the notion will rear its head again in chapters 4 and 5.\\nIn fact, it has occasionally been argued that language determines what we\\ncan think about. That is, it is impossible to think of things that we cannot\\nput into words. This latter position is somewhat diﬀerent from expressive\\ncompleteness and highly controversial. It is referred to as the Sapir–Whorf\\nHypothesis.\\nNotice that expressive completeness also rules out inadvertent communi-\\ncation systems. For example, we might conclude that Ernie is sick from him\\ncoughing or sneezing, but we would certainly not want to characterize those\\nas instances of language.\\nExpressive completeness also rules out facial gestures as an instance of\\nlanguage, whether those gestures be unintentional, like a smile, or deliberate,\\nlike a wink.\\nThus we characterize language as an expressively complete convention-\\nalized communication system. The requirement of expressive completeness\\nrules out miniature or “toy” systems. Conventionalization rules out uninten-\\ntional communication systems. Language, on this deﬁnition, is a subcase of\\na more general notion of communication.\\nNotice that this deﬁnition does not entail that a language must include\\na written form. In fact, the deﬁnition is neutral with respect to modality,\\nallowing for signed languages like American Sign Language (ASL). The lan-\\nguage also allows for other modalities as well, e.g. an exclusively written\\nlanguage.2\\nGiven this deﬁnition of language, we can ask whether animals have lan-\\nguage. The question is actually a rather odd one and rests on what we mean\\nby expressive completeness. If we mean that a language is complete with re-\\nspect to any message a speaker of it might want to purvey, then surely some\\n2This departs from the usual linguistic notion of language.\\nCHAPTER 2. LANGUAGE\\n18\\nanimals have language in this sense. If, on the other hand, we mean that a\\nlanguage must be complete with respect to any message that we might want\\nto convey, then probably not, assuming that there are no animals capable\\nof communicating about the full range of topics that humans communicate\\nabout.\\n2.2.2\\nCreativity\\nIn the previous section, we showed that language learning cannot be reduced\\nto memorizing words and phrases. Rather, learning a language involves learn-\\ning patterns and then exploiting those patterns in potentially novel ways.\\nThus, for example, our knowledge about what constitutes a well-formed sen-\\ntence of English does not evaporate when we are confronted with a new name.\\nA child confronted with a new individual with a name she hasn’t heard before\\nis perfectly capable of uttering novel sentences describing the new individual,\\ne.g. Ernie is in the kitchen.\\nThus knowledge of a language is knowledge of patterns.\\nOne might believe that these patterns are simply the sensible ones, but\\nthis would be incorrect. One piece of evidence against this idea is that the\\npatterns that work in a language are diﬀerent from the patterns that work\\nin other languages. For example, we saw that some languages use double\\nnegatives, and others do not.\\nAnother argument against this idea comes from the fact that we have\\ncontrasts like the following:\\n(2.8)\\nColorless green ideas sleep furiously.\\nFuriously sleep ideas green colorless.\\nNeither of these sentences makes sense in any obvious way, yet the ﬁrst is\\nvastly more acceptable than the second. Our knowledge of English separates\\nthe status of sentences of these types.\\nBoth arguments support the idea that the patterns that are acceptable\\nin our language are not governed by what makes sense.\\nThese patterns are quite complex. For example, most speakers of English\\nwill prefer one of the following sentences to the other.\\n(2.9)\\nWho did Ernie believe Minnie claimed that Bob saw?\\nWho did Ernie believe Minnie’s claim that Bob saw?\\nCHAPTER 2. LANGUAGE\\n19\\nBoth sentences, though long and a little complicated, make perfect sense,\\nyet one is much better than the other.3 The point is that our knowledge of\\nour language depends on knowing some rather subtle generalizations about\\nwhat makes a sentence acceptable. These generalizations have to do with\\nthe structures in a sentence and not with whether the sentence makes sense\\nin some intuitive way.\\n2.2.3\\nPrescriptive vs. Descriptive Grammar\\nAnother key notion in understanding language is understanding the diﬀerence\\nbetween what a speaker knows about their own language and what so-called\\nexperts tell us about language. For example, as a speaker of English, I know\\nthat I can say You and me are studying logic.\\nHowever, as an educated\\nEnglish speaker, I know that we are not supposed to write such things down\\nand instead are supposed to write: You and I are studying logic.\\nWhen we study what speakers actually do, we are interested in descrip-\\ntive grammar.\\nIf, on the other hand, we are interested in the rules that\\nare imposed on speakers, then we are interested in prescriptive grammar.\\nBoth are quite reasonable areas of study. The ﬁrst is more about individual\\npsychology, what it is a person actually knows about their language, albeit\\nunconsciously. The second is more about social systems, what aspects of\\nlanguage are valued or not in the society at large.\\n2.2.4\\nCompetence and Performance\\nAn extremely important but controversial distinction is that between knowl-\\nedge of a language, or competence, and the use of that language, or perfor-\\nmance.\\nWe can make sense of this distinction by imagining what it would be\\nlike to study language if we did not make this diﬀerence. We would study\\nlanguage by observing what people said. There are two problems with this.\\nFirst, people occasionally make mistakes when they speak, or occasionally\\nchange their minds in the middle of a sentence. For example, in conversation,\\none frequently hears sentences like these:\\n3Most prefer the ﬁrst to the second.\\nCHAPTER 2. LANGUAGE\\n20\\n(2.10)\\nI think. . . , uh, what did you say?\\nDid you read the. . . , oh yeah, now I remember.\\nErnie likes. . . , oh, hey, Hortence!\\nI bed that rook, I mean, I read that book.\\nAny speaker of English would recognize these as not acceptable instances of\\nEnglish word order, yet we utter these sorts of things all the time. If our\\ntheory of language was based purely on what we observed in the real world,\\nwe would have to account for these.\\nAnother problem is that there are sentences that we ﬁnd acceptable, yet\\ndo not utter. We’ve already treated some of these. There are acceptable, yet\\nnonsensical sentences like the following.\\n(2.11)\\nColorless green ideas sleep furiously.\\nThere are also sentences that refer to possible situations that we simply\\nhaven’t been confronted with:\\n(2.12)\\nHortence loves Ernie.\\nIn this latter case, we may simply not know individuals with those names,\\nor not have contemplated their emotional attachment.\\nIn fact, one can argue that there are an inﬁnite number of these sentences.\\nConsider the following series of sentences:\\n(2.13)\\nSophie likes logic.\\nErnie knows Sophie likes logic.\\nSophie knows Ernie knows Sophie likes logic.\\nErnie knows Sophie knows Ernie knows Sophie likes logic.\\n. . .\\nOr this one:\\n(2.14)\\nWhat is one and one?\\nWhat is one and one and one?\\nWhat is one and one and one and one?\\n. . .\\nIn each case, the series begins with a completely acceptable sentence. We\\ncan add words in a simple way up to inﬁnity. Of course, eventually, these\\nCHAPTER 2. LANGUAGE\\n21\\nbecome too long to understand or too long to utter before falling asleep, but\\nthere is no principled upper bound on acceptability. If we were to force our-\\nselves to restrict our language data to only the set of observed utterances, we\\nwould completely miss the fact that the set of possible utterances is inﬁnite.\\nThese problems are what leads to the distinction between language com-\\npetence and language performance. A science of language can then be based\\non either sort of data. We might choose to investigate language competence\\nby looking at the judgments of well-formedness that a speaker is capable of.\\nWe might instead investigate language performance by looking at what type\\nof utterances speakers actually produce and comprehend.\\nThe distinction between competence and performance has held sway in\\nlinguistics for almost ﬁfty years and is still generally thought to be useful. It\\nis, however, becoming more and more controversial. An intuitive concern is\\nthat it seems suspicious to some that a science of language should be based\\non intuitions, rather than more “direct” data. A more substantive problem\\nis that closer investigation of judgment data shows that those data exhibit\\nquite a bit of variability.\\n2.3\\nGeneral Areas\\nLet’s now consider the basic areas of language study. We can divide these\\ninto two broad categories. First, there are the areas that concern the actual\\nstructure of a language. Then there are the areas that concern how those\\nstructures play out in various ways.\\n2.3.1\\nStructures of Language\\nThe areas of language study that focus on the structures of language include:\\nphonetics, phonology, morphology, syntax, and semantics.\\nPhonetics\\nPhonetics is concerned with the sounds of language. The empirical goal is\\nto discover what sounds are possible in language and to try to explain why\\nonly certain speech sounds occur and not others.\\nThe method of explanation is the physical setting of speech. Patterns of\\nspeech sounds are explained in terms of how the articulatory system or the\\nauditory system works.\\nCHAPTER 2. LANGUAGE\\n22\\nLet’s consider a couple of examples of phonetic facts and phonetic expla-\\nnations.\\nOne phonetic fact that we have already mentioned is that all languages\\nappear to include the vowel sound [a], as in the ﬁrst syllable of English\\nfather. The usual explanation for this is that this is an extremely simple\\nsound to produce. Oversimplifying somewhat, it is produced by just opening\\nthe mouth fully and voicing. If we assume that languages make use of sounds\\nthat are easy to produce before they make use of sounds that are hard to\\nproduce, it follows that many, if not all, languages will have [a].\\nAnother kind of phonetic fact concerns [t]-like sounds. In English, the\\nsound [t], as in toe, is produced by putting the tip of the tongue against\\nthe alveolar ridge, the bony ridge behind the upper teeth. In Spanish and\\nRussian, on the other hand, [t] is produced by putting the tip of the tongue\\nslightly forward, against the back of the upper front teeth.\\nOther languages include other [t]-like sounds where the tip of the tongue\\nis placed further back on the roof of the mouth against the hard palate. There\\nare no languages, however, where [t]-like sounds are produced by placing the\\ntip of the tongue against the soft palate.\\n(2.15)\\nThis should not be too surprising and follows directly from the physiology of\\narticulation. The connections between the tongue and the ﬂoor of the mouth\\nCHAPTER 2. LANGUAGE\\n23\\nprevent the tip from reaching that far back (unless someone is unusually\\ngifted).\\nLet’s consider one more example.\\nEnglish includes the sound [v], for\\nexample, in the word van. This sound is produced by bringing the upper\\nteeth in close proximity to the lower lip and voicing. Spanish, on the other\\nhand includes the sound [B], as in the word cabo [kaBo] ‘end’. This sound\\nis very similar to English [v] except that one brings the two lips in close\\nproximity, rather than lip and teeth.\\nWhat’s striking is that there is only one language in the world that ap-\\npears to have both sounds: Ewe, spoken in in Ghana and Togo. Why is this\\ncombination so rare? The explanation is that the sounds are so very similar\\nthat it is too hard to distinguish them. The sounds of a language tend to be\\ndistributed so that they are maximally distinct acoustically.\\nPhonology\\nPhonology is similar to phonetics, except the focus is on the distribution\\nof sounds and sound patterns as instances of cognitive organization, rather\\nthan physiology. There is therefore a natural tension between phonetics and\\nphonology in terms of explanation. Both disciplines deal with sounds and\\ngeneralizations about sounds, but diﬀer in their modes of explanation.\\nThe two disciplines diﬀer in their methodologies as well. Phonetics is\\nan experimental ﬁeld and makes liberal use of technology to understand the\\ndetails of articulation, audition, and acoustics.\\nPhonetics largely focuses\\non the performance of sound systems. Phonology, on the other hand, makes\\nmuch less use of technology and largely focuses on the competence underlying\\nsound systems.\\nWe’ve already discussed a range of sound system facts that seem quite\\namenable to phonetic explanation. What kinds of facts require a phonological\\nstory?\\nOne very good candidate for phonology is syllable structure. All lan-\\nguages parse words into syllables.\\nIn English, for example, hat has one\\nsyllable, table two syllables, banana three, etc. It’s not at all clear how to\\ndeﬁne a syllable phonetically—some phoneticians even deny that there are\\nsyllables—so the syllable seems a reasonable candidate for a cognitive unit.\\nThe idea is that our psychology requires that we break words up into these\\nunits.\\nThere are some interesting typological generalizations about how sylla-\\nCHAPTER 2. LANGUAGE\\n24\\nbles work. For example, while all languages have syllables that begin with\\nconsonants, e.g. in both syllables of English happy, not all languages have\\nsyllables that begin with vowels, e.g. in both syllables of eon. So there are two\\nbroad categories of language along this dimension. First, there are languages\\nwhere all syllables must begin with a consonant, e.g. Hawaiian. There are\\nalso languages where syllables can begin with either a consonant or a vowel,\\ne.g. English. There are, however, no languages where all syllables must be-\\ngin with vowels. This generalization too is thought to be a fact about our\\ncognitive organization.\\nPhonology is also concerned with relations between sounds in utterances.\\nFor example, there is a process in many dialects of (American) English\\nwhereby a [t] or [d] sound is pronounced as a ﬂap when it occurs between\\nappropriate vowels.\\nA ﬂap is produced by passing the tip of the tongue\\nquickly past the alveolar ridge and sounds much like the [r]-sound of Spanish\\nor Russian. We will transcribe a ﬂap like this: [R]. This process causes items\\nto be pronounced diﬀerently in diﬀerent contexts. For example, write and\\nride in isolation are pronounced with [t] and [d] respectively, but when they\\noccur before a vowel, these sounds are pronounced as ﬂaps.\\n(2.16)\\nwrite\\nride\\nin isolation\\n[rayt]\\n[rayd]\\nbefore a vowel-initial word\\n[rayR] a letter\\n[rayR] a horse\\nbefore a vowel-initial suﬃx\\n[rayR]er\\n[rayR]er\\nNot all languages do this and so this cannot simply be a function of the\\nphysiology. Characterizing these sorts of generalizations and the search for\\nexplanation in the domain of cognition are part of phonology.\\nMorphology\\nMorphology is concerned with the combination of meaningful elements to\\nmake words. It can be opposed to phonology, which we can characterize\\nas the combination of meaningless elements to make words. For example,\\na word like hat is composed of three sounds, three meaningless elements:\\n[h], [æ], and [t]. There is only one meaningful element: the word itself. A\\nword like unhappy has a complex phonology—it has six sounds: [2], [n], [h],\\n[æ], [p], [i]—and a complex morphology: it is composed of two meaningful\\nelements: un– and happy.\\nThe element un– is a preﬁx, an element that\\nCHAPTER 2. LANGUAGE\\n25\\ncannot occur alone, but can be attached to the left of another element. It\\nexpresses negation. The element happy is a stem and can occur alone.\\nThere are also suﬃxes. For example, the word books is composed of a\\nstem book and a suﬃx –s, which expresses plurality. Preﬁxes, suﬃxes, stems,\\netc. are called morphemes.\\nThe most important thing to keep in mind about morphology is that\\nit can, in some cases, be boundless. Hence the set of possible words in a\\nlanguage with boundless morphology is inﬁnite. Here is an example from\\nEnglish where a set of suﬃxes can be attached without bound.\\n(2.17)\\nnation\\nnational\\nnationalize\\nnationalization\\nnationalizational\\nnationalizationalize\\n. . .\\nAnother extremely important point is that elements—morphemes—are\\nnot combined in a linear fashion, but are nested. For example, a word like\\nnationalize has three morphemes that are grouped together as represented\\nin the tree below.\\n(2.18)\\nnation\\nal\\nize\\nThis structure can be important to the meaning of a word. Consider a\\ncompound word like budget bottle brush. This has two diﬀerent meanings\\nassociated with two diﬀerent structures.\\n(2.19)\\nbudget\\nbottle\\nbrush\\nbudget\\nbottle\\nbrush\\nThe structure on the left is associated with the meaning where the combi-\\nnation refers to an inexpensive brush for bottles; the structure on the right\\nis associated with the meaning where it refers to a brush for inexpensive\\nbottles.\\nCHAPTER 2. LANGUAGE\\n26\\nThe two ideas come together in the following examples.\\nA word like\\nunforgivable has a single meaning: not able to be forgiven.\\nA word like\\nunlockable actually has two meanings: not able to be locked and able to be\\nunlocked. The ambiguity of the second word correlates with the two diﬀerent\\npossible structures for it.\\n(2.20)\\nun\\nlock\\nable\\nun\\nlock\\nable\\nThe word unforgivable only has one meaning because only one structure is\\npossible:\\n(2.21)\\nun\\nforgiv\\nable\\nNo other meaning is possible because the other structure below is not possi-\\nble.\\n(2.22)\\nun\\nforgiv\\nable\\nThe reason this latter tree is not possible is that to produce it, we would\\nhave to ﬁrst combine un– and forgive into unforgive, and that is not a word\\nof English.\\nSyntax\\nSyntax is concerned with how words are combined to make sentences. We’ve\\nalready cited examples in sections 2.1 and 2.2 above that establish key prop-\\nerties we’ve ascribed to language. First, syntax showed us that language\\nlearning is more than memorization. Second, it established that knowledge\\nof language is knowledge of the generalizations that underlie what is well-\\nformed in the language. Third, it established that what is well-formed in a\\nlanguage is not determined by what “makes sense”.\\nCHAPTER 2. LANGUAGE\\n27\\nA remaining essential point is that words are not combined in a linear\\nfashion. Rather, as with morphology, words are combined in a nested fashion.\\nConsider, for example, a sentence like the following:\\n(2.23)\\nErnie saw the man with the binoculars.\\nThere are two possible meanings for this sentence. First, it could be that\\nErnie used binoculars to see the man. Alternatively, the man has binoculars\\nand Ernie saw him. These two meanings are based on two diﬀerent structures.\\nThe ﬁrst has the man and with the binoculars as sisters within a larger\\ngrouping, labeled VP here for Verb Phrase.\\n(2.24)\\nErnie\\nVP\\nsaw\\nNP\\nthe\\nman\\nwith\\nthe\\nbinoculars\\nThe other structure groups with the binoculars directly within the same\\nphrase as the man, labeled NP here for Noun Phrase.\\n(2.25)\\nErnie\\nVP\\nsaw\\nNP\\nthe\\nman\\nwith\\nthe\\nbinoculars\\nCHAPTER 2. LANGUAGE\\n28\\nIn the ﬁrst structure, with the binoculars modiﬁes the verb; in the second\\nstructure, it modiﬁes the noun.\\nThe point is that words are grouped together into structures and those\\nstructures contribute to the meaning of sentences.\\nSyntactic competence\\nincludes knowledge of what structures are possible in the formation of sen-\\ntences.\\nSemantics\\nSemantics is concerned with the meanings of words and sentences. One kind\\nof semantics deals with how the meaning of a sentence is computed from its\\nwords and the way those words are grouped together. As we have already\\nshown, the groupings can make dramatic contributions to the meaning of a\\nsentence or of a word, as in the binocular example above. We also considered\\nexamples where there are multiple meaning diﬀerences that are quite subtle,\\nbut are not superﬁcially associated with structural diﬀerences: the example\\ntreated in chapter 1 of Two women saw two men.\\nWe will see that logic, treated in chapters 4 and 5, is quite useful in\\nsemantics.\\n2.3.2\\nOther Areas\\nThere are a number of other really interesting areas of language study and\\nwe list just a few of them here (in no particular order).\\nPsycholinguistics is the study of performance, how language is actually\\nused.\\nThe methodology is typically experimental.\\nPsycholinguists\\nstudy language production, language comprehension, speech percep-\\ntion, lexical access, etc.\\nNeurolinguistics is concerned with language in the brain. At the theo-\\nretical level, it focuses on how language is processed in actual brain\\nstructures. At a more applied level, it deals with various sorts of cog-\\nnitive disorders involving language.4\\n4There is something called Neurolinguistic Programming, but this is a misnomer. This\\nis not a ﬁeld of language study. Occasionally, one sees the abbreviation NLP for this ﬁeld.\\nNLP is also used for Natural Language Processing, which is a ﬁeld of language study.\\nCHAPTER 2. LANGUAGE\\n29\\nSociolinguistics is concerned with the relationship between language and\\nsociety. It is concerned with how social forces are reﬂected in language\\nand with how language aﬀects social variables.\\nLanguage Acquisition One can study how language is acquired. This is\\ndone in several ways: following the development of individual children\\nor running experiments on sets of children.\\nWriting Systems can be studied for their own intrinsic interest, but also\\nas a window into the structure of a language or as evidence for how\\nlanguage changes over time.\\nLiterature also provides an interesting vantage point on language. Again, it\\ncan be studied for its own intrinsic interest, but also for the information\\nit provides about the structure of language.\\nApplied Linguistics refers to several diﬀerent ﬁelds, all of which use the\\nstudy of language in some concrete application.\\nThese include, for\\nexample, language teaching and forensic linguistics.\\nComputational Linguistics includes two broad domains. One is the use\\nof language in computational tasks, e.g. machine translation, text min-\\ning, speech understanding, speech synthesis, etc. The other area is the\\nmathematical modeling of language. Many of the foundational areas\\nwe treat in this book are part of this latter area.\\nDiscourse Analysis is the investigation of how units larger than a sentence\\nare constructed. This can include conversation or texts. The latter can\\nbring this area close to the study of literature.\\nHistorical Linguistics is the investigation of language change. This can\\nbe done by comparing modern languages, looking at historical records,\\nor looking at the language-internal residue of historical changes.\\n2.4\\nSummary\\nThis chapter has introduced the study of language generally. We began with\\na refutation of some of the more egregious misconceptions about language.\\nWe then established a number of key properties of language. These in-\\nclude a deﬁnition of language as an expressively complete conventionalized\\nCHAPTER 2. LANGUAGE\\n30\\nsystem of communication. We also argued at several points that the set of\\nwords and sentences is inﬁnite and that knowledge of language is more than\\njust knowing what those words and sentences are.\\nFinally, we reviewed the main areas of language structure and many of\\nthe areas of language study.\\n2.5\\nExercises\\n1. We’ve seen that languages are sometimes characterized in almost an-\\nthropomorphic terms, e.g. “harsh”, “beautiful”, “logical”, etc. Explain\\nwhy.\\n2. We’ve seen that sometimes a concept that is encoded with a word in\\none language is expressed with a phrase in another language. Why do\\nyou think this happens?\\n3. Does the diﬀerence between competence and performance apply to writ-\\nten language? Explain why or why not.\\n4. We’ve cited several structures in English that show that the set of words\\nand the set of sentences are inﬁnite. Find another that’s diﬀerent from\\nthe ones cited in the text.\\n5. Find a description of how the “nothing” construction works in some\\nlanguage we have not discussed.\\nDescribe the pattern and provide\\nsome examples. Does the language have double negation?\\nChapter 3\\nSet Theory\\nWhat is Set Theory and why do we care?\\nSet Theory is—as we would\\nexpect—the theory of sets. It’s an explicit way of talking about elements,\\ntheir membership in groups, and the operations and relationships that can\\napply between elements and groups.\\nSet Theory is important to language study for several reasons. First, it\\nis even more foundational than all the other topics we cover subsequently.\\nThat is, many of the other topics we will treat are grounded in set-theoretic\\nterms.\\nA second reason set theory is important to know about is that there\\nare language issues that can be treated directly in terms of set theory, e.g.\\nfeatures, issues of semantic entailment, and constraint logic.\\n3.1\\nSets\\nA set is an abstract collection or grouping of elements. Those elements can\\nbe anything, e.g. words, sounds, sentences, aﬃxes, etc. In the following, we\\nwill represent the names of sets in all capital letters: A, B, C, etc.\\nSets can be deﬁned in several diﬀerent ways. The simplest is to simply\\nlist the members of the set. For example, we might deﬁne the set A as being\\ncomposed of the elements x, y, and z. This can be expressed as:\\n(3.1) A = {x, y, z}\\nThe ordering of elements in the curly braces is irrelevant; a set is deﬁned\\n31\\nCHAPTER 3. SET THEORY\\n32\\nby what elements it contains, not by any ordering or priority among those\\nelements. Thus the following are equivalent to the preceding.\\n(3.2)\\nA = {x, z, y}\\nA = {y, x, z}\\nA = {y, z, x}\\nA = {z, x, y}\\nA = {z, y, x}\\nNotice too that it makes no sense to repeat an element. Thus the set A =\\n{a, b, c} is the same as A = {a, b, c, b}.\\nAs above, the elements of a set can be anything. For example:\\n(3.3)\\nA = {æ, N, n}\\nB = {French, O’odham, Abkhaz, Spanish}\\nC = {N, 78, The Amazon}\\nSets can also be deﬁned by the properties their elements bear. For exam-\\nple, the set of nasal consonants in English is deﬁned as the set of consonants\\nin which air ﬂows out the nose: [m], [n], and [N] in tam [tæm], tan [tæn], and\\ntang [tæN], respectively. The set can be deﬁned by listing the elements:\\n(3.4) A = {m, n, N}\\nor by specifying that the elements of the set are those elements that bear the\\nrelevant properties:\\n(3.5) A = {x | x is a nasal consonant of English}\\nThis expression is read as: A is composed of any x, where x is a nasal\\nconsonant of English.\\nSimilar sets can be deﬁned in morphology. We can deﬁne the set com-\\nposed of the singular members of the present tense indicative mood of the\\nSpanish verb cantar to ‘sing’ in the same two ways:\\nCHAPTER 3. SET THEORY\\n33\\n(3.6) A = {canto, cantas, canta}\\n(3.7) A = {x | x is a singular form of the present tense of cantar.}\\nBe careful though. Deﬁning a set in terms of the properties of its elements\\ntakes us onto dangerous ground. We want to be as explicit as possible in\\nspecifying what can qualify as a property that can be used to pick out the\\nelements of a set.\\nThe third way to deﬁne sets is by recursive rules. For example, the set of\\npositive integers (1, 2, 3, . . .) can be deﬁned as follows:\\n1. 1 is a positive integer.\\n2. If n is a positive integer, then so is n + 1.\\n3. Nothing else is a positive integer.\\nThis method of deﬁning a set has several interesting properties. First, notice\\nthat the set thus deﬁned is inﬁnite in size. It has an unbounded number of\\nelements. Second, a recursive deﬁnition typically has three parts, just as this\\none does. First, there is a base case, establishing directly the membership of\\nat least one element. Second, there is the recursive case, establishing that\\nadditional members of the set are deﬁned in terms of elements we already\\nknow are members. Finally, there is a bounding clause, limiting membership\\nto the elements accommodated by the other two clauses.\\nThis may seem excessively “mathematical”, but the same method must\\nbe used to deﬁne “possible word” and—in a more sophisticated form—must\\nbe used to deﬁne “possible sentence”. For example, to get a sense of this,\\nhow would you deﬁne the set of all possible strings using the letters/sounds\\n[a] and [b]?1 (Assume that any sequence of these is legal.)\\nFinally, we deﬁne the size of some set A as |A|. If, for example, A =\\n{x, y, z}, then |A| = 3.\\n1This is given as an exercise at the end of the chapter.\\nCHAPTER 3. SET THEORY\\n34\\n3.2\\nMembership\\nIf an element x is a member of the set A, we write x ∈ A; if it is not, we\\nwrite x ̸∈ A. The membership relation is one between a potential element of\\na set and some set.\\nWe have a similar relation between sets: subset.\\nDeﬁnition 1 (Subset) Set A is a subset of set B if every member of A is\\nalso a member of B.\\nNotice that subset is deﬁned in terms of membership. For example, given\\nthe set A = {y, x} and the set B = {x, y, z}, the former is a subset of the\\nlatter—A ⊆ B—because all members of the former—x and y—are members\\nof the latter. If set A is not a subset of set B, we write A ̸⊆ B. Notice that\\nit follows that any set A is a subset of itself, i.e. A ⊆ A.\\nThere is also a notion of proper subset.\\nDeﬁnition 2 (Proper Subset) Some set A is a proper subset of some set\\nB if all elements of A are elements of B, but not all elements of B are\\nelements of A.\\nIn this case, we write A ⊂ B. If A is not a proper subset of B, we write\\nA ̸⊂ B.\\nThe diﬀerence between the notions of subset and proper subset is the\\npossibility of identity. Sets are identical when they have the same members.\\nDeﬁnition 3 (Set extensionality) Two sets are identical if and only if\\nthey have the same members.\\nTo say that some set A is a subset of some set B says that either A is a\\nproper subset of B or A is identical to B. When two sets are identical, we\\nwrite A = B. It then follows that if A ⊆ B and B ⊆ A, that A = B.\\nNote that ⊂ and ∈ are diﬀerent. For example, {a} is a subset of {a, b}, but\\nnot a member of it, e.g. {a} ⊆ {a, b}, but {a} ̸∈ {a, b}. On the other hand,\\n{a} is a member of {{a}, b, c}, but not a subset of it, e.g. {a} ∈ {{a}, b, c},\\nbut {a} ̸⊆ {{a}, b, c}.\\nWe also have the empty set ∅, the set that has no members. It follows\\nthat the empty set is a subset of all other sets. Finally, we have the power\\nset of A: 2A or ℘(A): all possible subsets of A. The designation 2A reﬂects\\nCHAPTER 3. SET THEORY\\n35\\nthe fact that a set with n elements has 2n possible subsets. For example, the\\nset {x, y, z} has 23 = 8 possible subsets.2\\n(3.8)\\n{x, y, z}\\n{x, y}\\n{x}\\n∅\\n{x, z}\\n{y}\\n{y, z}\\n{z}\\n3.3\\nOperations\\nThere are various operations that can be applied to sets. The union operation\\ncombines all elements of two sets. For example, the union of A = {x, y} and\\nB = {y, z} is A∪B = {x, y, z}. Notice how—as we should expect—elements\\nthat occur in both sets only occur once in the new set. One way to think of\\nunion is as disjunction. If A ∪ B = C, then an element that occurs in either\\nA or B (or both) will occur in C.\\nThe other principal operation over sets is intersection, which can be\\nthought of as conjunction. For example, the intersection of A = {x, y} and\\nB = {y, z} is A ∩ B = {y}. That is, if an element is a member of both A\\nand B, then it is a member of their intersection.\\nWe have already deﬁned the empty set ∅: the set that contains no el-\\nements. We can also deﬁne the universal set U: the set that contains all\\nelements of the universe. Similarly, we can deﬁne diﬀerence and comple-\\nment. Thus A′, the complement of A, is deﬁned as all elements that are not\\nin A. That is, all elements that are in U, but not in A. For example, if\\nA = {x, y} and U = {x, y, z}, then A′ = {z}.\\nSet complement and intersection can be used to deﬁne diﬀerence. The\\ndiﬀerence of two sets A and B is deﬁned as all elements of A minus any\\nelements of B, i.e. A − B. This can also be expressed as the intersection of\\nA with the complement of B, i.e. A ∩ B′. Likewise, the complement of A,\\nA′, can be deﬁned as U − A. Notice that not all elements of B need to be\\nin A for us to calculate A − B. For example, if A = {x, y} and B = {y, z},\\nthen A − B = {x}.\\n2Recall that 23 = 2 × 2 × 2 = 8.\\nCHAPTER 3. SET THEORY\\n36\\n3.4\\nFundamental Set-theoretic Equalities\\nThere are a number of beautiful and elegant properties that hold of sets,\\ngiven the operations that we’ve discussed. We go through a few of these in\\nthis section.\\nIdempotency has it that when a set is unioned or intersected with itself,\\nnothing happens.\\n(3.9)\\nIdempotency\\nX ∪ X = X\\nX ∩ X = X\\nCommutativity expresses that the order of arguments is irrelevant for union\\nand intersection.\\n(3.10)\\nCommutativity\\nX ∪ Y = Y ∪ X\\nX ∩ Y = Y ∩ X\\nAssociativity holds that the order with which sets are successively unioned\\nor successively intersected is irrelevant.\\n(3.11)\\nAssociativity\\n(X ∪ Y ) ∪ Z = X ∪ (Y ∪ Z)\\n(X ∩ Y ) ∩ Z = X ∩ (Y ∩ Z)\\nDistributivity governs the interaction between union and intersection.\\n(3.12)\\nDistributivity\\nX ∪ (Y ∩ Z) = (X ∪ Y ) ∩ (X ∪ Z)\\nX ∩ (Y ∪ Z) = (X ∩ Y ) ∪ (X ∩ Z)\\nIdentity governs how the universal set U and the empty set ∅ can be inter-\\nsected or unioned with other sets.\\nCHAPTER 3. SET THEORY\\n37\\n(3.13)\\nIdentity\\nX ∩ U = X\\nX ∪ ∅ = X\\nDomination includes several relationships rather similar to Identity.\\n(3.14)\\nDomination\\nX ∩ ∅ = ∅\\nX ∪ U = U\\nThe Complement Laws govern complements and diﬀerences.\\n(3.15)\\nComplement Laws\\nX ∪ X′ = U\\nX ∩ X′ = ∅\\nThe Double Complement Law says that the complement of the complement\\nof a set is the same as the original set.\\n(3.16)\\nDouble Complement Law\\n(X′)′) = X\\nWe also have the Relative Complement Law, which is basically a deﬁnition\\nof set diﬀerence.\\n(3.17)\\nRelative Complement Law\\nX − Y = X ∩ Y ′\\nFinally, DeMorgan’s Law shows how complement allows us to deﬁne inter-\\nsection and union in terms of each other.\\n(3.18)\\nDeMorgan’s Law\\n(X ∪ Y )′ = X′ ∩ Y ′\\n(X ∩ Y )′ = X′ ∪ Y ′\\nCHAPTER 3. SET THEORY\\n38\\nWhen we get to the Laws of Sentential Logic in Section 4.7, we will see a\\nlot of similarity with these.\\n3.5\\nTheorems\\nLet’s try to understand and prove some things in set theory.3 Here are a few\\ntheorems of set theory.\\n1. If A ⊆ B then A ∪ C ⊆ B ∪ C\\n2. If A ⊆ B then A ∩ C ⊆ B ∩ C\\n3. If A ⊆ B then C − B ⊆ C − A\\n4. A ∩ (B − A) = ∅\\nLet’s consider how we might prove the ﬁrst one. We can do this informally\\nat this stage using what we will call Indirect Proof in section 4.10.\\nFirst, let’s make sure we understand the theorem by stating it in words.\\nIf the set A is a subset of the set B, then the union of A with any set C is\\na subset of the union of B with the same set C. A more informal rewording\\nmight make this more intuitive. If A is a subset of B, then if you add some\\nelements to A and add the same elements to B, then A is still a subset of B.\\nLet’s look at an example. Let’s deﬁne A = {a, b}, and B = {a, b, c}. The\\nﬁrst set is certainly a subset of the second, A ⊆ B, as every member of A is\\nalso a member of B.\\nLet’s now union A and B with some other set C = {g, h}. We now have\\nthat A ∪ C = {a, b, g, h} and B ∪ C = {a, b, c, g, h}. The union operation\\nhas added the same elements to both sets, so the ﬁrst unioned set is still a\\nsubset of the second unioned set: A ∪ C ⊆ B ∪ C.\\nWe will make use of a proof technique called Indirect Proof, or Proof by\\nContradiction. The basic idea is that if you want to prove some statement\\nS, you show instead that if S were not true, a contradiction would result.\\nThis sounds quite formal, but we use this technique all the time. Imagine\\nthat S is some political candidate you are committed to and you are arguing\\nto a friend that they should vote for S too.\\nYou might very well try to\\nmake the case by making dire predictions about what would happen if S is\\n3We will consider proof and proof strategies in more detail in chapters 4 and 5.\\nCHAPTER 3. SET THEORY\\n39\\nnot elected, or by enumerating the shortcomings of the candidates that S is\\nrunning against. These are both instances of essentially the same technique.\\nIn the case at hand, we will attempt to prove a contradiction from these\\ntwo statements: A ⊆ B and A ∪ C ̸⊆ B ∪ C. If the latter is true, then there\\nmust be at least one element, call it x, that is a member of A ∪ C, but is not\\na member of B ∪ C. If it is a member of A ∪ C, then by the deﬁnition of\\nunion, it must be a member of A or C or both. If it is a member of A, then\\nit must be a member of B ∪ C because all members of A are members of\\nB and all members of B are, by the deﬁnition of union, members of B ∪ C.\\nThus x cannot be a member of A. It must then be a member of C. However,\\nif it is a member of C, then it must be a member of B ∪ C, because, by the\\ndeﬁnition of union, all members of C are members of B ∪C. Hence x cannot\\nbe a member of C.\\nIt now follows that there can be no element x. This must be the case\\nbecause x would have to be a member of A or C and neither is possible.\\nHence, we have a contradiction and it cannot be the case that A ⊆ B and\\nA ∪ C ̸⊆ B ∪ C.4 2\\nWe leave the remaining theorems as exercises.\\n3.6\\nOrdered Pairs, Relations, and Functions\\nSets have no order, but we can deﬁne groups with an ordering, e.g. an ordered\\npair. For example, if we want to say that x and y are in an ordered pair,\\nwe write ⟨x, y⟩. We’ve already seen that, for example, {x, y} = {y, x}, but\\n⟨x, y⟩ ̸= ⟨y, x⟩. Ordered groups with more elements are deﬁned in the obvious\\nway, e.g. ⟨x, y, z, w⟩, etc. Notice that, while {x, x} makes no sense, ⟨x, x⟩\\ndoes.\\nOrdered pairs may seem like something that should be “outside” set the-\\nory, but they can be deﬁned in set-theoretic terms as follows. An ordered\\npair ⟨x, y⟩ is deﬁned as the set composed of the set of its ﬁrst member plus\\nthe set composed of both members: {{x}, {x, y}}. Larger ordered groups\\ncan be deﬁned recursively. Thus ⟨x, y, z⟩ is deﬁned as ⟨⟨x, y⟩, z⟩.\\nA relation R pairs elements of one set A with elements of the same set A\\nor a diﬀerent set B. If the relation R pairs element a to element b, then we\\nwrite aRb or, for legibility, R(a, b). For example, we might deﬁne a relation\\n4All proofs in the text will be marked with a subsequent box.\\nCHAPTER 3. SET THEORY\\n40\\nbetween active sentences and passive sentences, or one that relates oral vowels\\nto nasal ones, e.g. aR˜a.\\nWe can deﬁne a relation as a set of ordered pairs. If, for example, aRb and\\ncRd, then we can represent these as ⟨a, b⟩ ∈ R and ⟨c, d⟩ ∈ R. If these are\\nall the pairs that R gives us, then we can represent R as R = {⟨a, b⟩, ⟨c, d⟩}.\\nWe can also have n-place relations, where two or more elements are paired\\nwith another element. These are of course ordered triples: ⟨a, b, c⟩, and the\\nwhole relation is the set of these triples.\\nThe domain of a two-place relation is deﬁned as the ﬁrst member of the\\nordered pair and the range is deﬁned as the second member of the pair.\\nLet’s adopt some additional notation.\\nIf some relation R pairs every\\nelement of some set A with every element of the same set, then we write\\nR = A × A. Thus, if R = A × A and A = {x, y, z}, then R = {⟨x, x⟩,\\n⟨x, y⟩, ⟨x, z⟩, ⟨y, x⟩, ⟨y, y⟩, ⟨y, z⟩, ⟨z, x⟩, ⟨z, y⟩, ⟨z, z⟩}.\\nWe can see that\\n|R| = |A| × |A| = 3 × 3 = 9.\\nSince relations can pair elements of diﬀerent sets, the size of the relation\\nvaries accordingly. For example, if R = A × B and A = {x, y, z} and B =\\n{a, b}, then R = {⟨x, a⟩, ⟨x, b⟩, ⟨y, a⟩, ⟨y, b⟩, ⟨z, a⟩, ⟨z, b⟩}.\\nIn this case,\\n|R| = |A| × |B| = 2 × 3 = 6.\\nThe complement R′ of a relation R is deﬁned as every pairing over the\\nsets that is not included in R. Thus if R ⊆ A×B, then R′ =def (A×B)−R.\\nFor example, if R relates A = {x, y, z} and B = {a, b} (that is R ⊆ A × B),\\nand R = {⟨x, a⟩, ⟨x, b⟩, ⟨y, a⟩, ⟨y, b⟩}, then R′ = {⟨z, a⟩, ⟨z, b⟩}.\\nThe inverse R−1 of a relation R reverses the domain and range of R.\\nThus, if R ⊆ A × B, then R−1 ⊆ B × A. For example, if R = {⟨x, a⟩, ⟨x, b⟩,\\n⟨y, a⟩, ⟨y, b⟩}, then R−1 = {⟨a, x⟩, ⟨b, x⟩, ⟨a, y⟩, ⟨b, y⟩}.\\nA function is a special kind of relation where every element of the domain\\nis paired with just one element of the range. For example, if the function\\nF pairs a with b, we write F(a) = b. If the domain of F is A = {x, y} and\\nthe range is B = {a, b}, then F ⊆ A × B. It follows from the deﬁnition of a\\nfunction, that F can only be one of the following:\\n(3.19)\\na.\\nF = {⟨x, a⟩, ⟨y, a⟩}\\nb.\\nF = {⟨x, b⟩, ⟨y, b⟩}\\nc.\\nF = {⟨x, a⟩, ⟨y, b⟩}\\nd.\\nF = {⟨x, b⟩, ⟨y, a⟩}\\nCHAPTER 3. SET THEORY\\n41\\nThere is no other function from A to B.\\nFinally, relations exhibit a variety of properties that are quite useful.\\nReﬂexivity A relation R is reﬂexive if and only if, for every element x in\\nR, ⟨x, x⟩ ∈ R.\\nSymmetry A relation R is symmetric when ⟨x, y⟩ ∈ R if and only if ⟨y, x⟩ ∈\\nR.\\nTransitivity A relation R is transitive if and only if for all pairs ⟨x, y⟩ and\\n⟨y, z⟩ in R, ⟨x, z⟩ is in R.\\nThe terminology gets heinous when we begin to consider the way these prop-\\nerties might not hold of some relation. For example, a relation that contains\\nno instances of ⟨x, x⟩ is irreﬂexive, but if it is simply missing some instances\\nof ⟨x, x⟩, then it is nonreﬂexive.\\n3.7\\nLanguage examples\\nLet’s now look at some simple examples of these notions in the domain of\\nlanguage.\\n3.7.1\\nExamples of Sets\\nLet’s start with the words of English. This is, of course, a set, e.g. W =\\n{run, Ernie, of , . . .}. Any word of English will be a member of this set. For\\nexample: hat ∈ W.\\nThe verbs of English are also a set: V = {run, sat, envelops, . . .}. As\\nwe would expect, the verbs of English are a subset of the words of English,\\nV ⊆ W, and, in fact, a proper subset: V ⊂ W.\\nLet’s assume that nouns are also a set and a proper subset of the set of\\nEnglish words: N ⊂ W. The union of nouns and verbs, N ∪ V , would then\\nbe the set of any word that is a noun or a verb. Is this true: N ∪ V ⊂ W?\\nWhat is the intersection of N and V , i.e. N ∩ V ? Can you give an example?\\nIf some word is a member of N, does it follow that it is also a member of\\nN ∪ V ? Of N ∩ V ?\\nWe can also use set theory to characterize syntax or morphology. For\\nexample, a sentence like Ernie likes logic can be treated as an ordered triple:\\nCHAPTER 3. SET THEORY\\n42\\n⟨Ernie, likes, logic⟩. Why would it be a mistake to view sentences as normal\\nsets?\\nWe can go further, in fact. Recall that a speaker’s knowledge of what\\nconstitutes a well-formed sentence is best characterized in terms of hierar-\\nchical structures. Thus a sentence isn’t a ﬂat string of words, but organized\\ninto nested phrases. We can incorporate this by viewing a sentence not as\\na ﬂat ordered tuple, but as composed of nested tuples. For example, if we\\nwere to say that likes logic is a VP, we could represent Ernie likes logic as:\\n⟨Ernie, ⟨likes, logic⟩⟩.\\n3.7.2\\nExamples of Relations and Functions\\nRelations are ubiquitous in language. For example, we can posit a relation\\nR between (unmarked) verbs in the present tense:\\n(3.20) {jump, break, ﬁnish, ring, . . .}\\nand verbs marked for the past tense:\\n(3.21) {jumped, broke, ﬁnished, rang, . . .}\\nThus:\\n(3.22) R =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n⟨jump, jumped⟩,\\n⟨break, broke⟩,\\n⟨ﬁnish, ﬁnished⟩,\\n⟨ring, rang⟩,\\n. . .\\n\\uf8fc\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8fd\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8fe\\nThe inverse of this is then:\\n(3.23) R−1 =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n⟨jumped, jump⟩,\\n⟨broke, break⟩,\\n⟨ﬁnished, ﬁnish⟩,\\n⟨rang, ring⟩,\\n. . .\\n\\uf8fc\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8fd\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8fe\\nCHAPTER 3. SET THEORY\\n43\\nIs R a function? Recall, that a relation must satisfy two properties to be\\na relation. First, does every verb of English have a past tense? Yes. Second,\\nis there one and only one past tense for for every verb? This is tricky.\\nFirst, there are verbs where the past tense form looks just like the present\\ntense for, e.g. R(hit, hit). On the other hand, there are verbs that seem to\\nhave more than one past tense form. For example, the verb ring has the\\npast tense forms rang and ringed. These seem to have diﬀerent meanings,\\nhowever.\\n(3.24)\\nThe bell rang.\\nThe wall ringed the city.\\nTo accommodate cases like this, we might want to say that there are two\\nverbs ring1 and ring2, each with unique past tense forms.\\nAnother problem, however, is that there are verbs like dive where multiple\\npast tense forms are possible: dived and dove. Some speakers prefer one, some\\nthe other, and some vacillate. If R(dive, dove) and R(dive, dived), then R is\\nnot a function.\\nOn the other hand, the dive problem does not prevent R−1 from being a\\nfunction.\\nLet’s consider some relations that are relevant to sound systems. For\\nexample, we might posit a relation R from words to numbers that says how\\nmany letters are in a word, e.g. R(hat, 3), R(orange, 6), R(appropriate, 11).\\nWe could also do this with respect to the number of sounds in a word:\\nR([hæt], 3), R([Or@nˇ\\uf6be], 5), R([@propri@t], 9). This is clearly a function. Every\\nsingle word has one and only one length. The inverse R−1 is not a function\\nas many words have the same length.\\nWe might also posit a relation L that indicates that the ﬁrst element is\\nlonger than the second element, again, in either letters or sounds. Thus, if\\nwe deﬁne L in terms of letters, we get instances like these: L(orange, hat),\\nL(appropriate, hat), L(appropriate, orange), etc.\\nNotice that neither this\\nrelation nor its inverse are functions.\\nLet’s consider now whether any of these relations exhibit the properties\\nwe discussed above.\\nFirst, none of them are reﬂexive.\\nRecall that for a\\nrelation to be reﬂexive, every element in the domain must be paired with\\nitself. An example of a reﬂexive relation might be the notion of at least as\\nlong as: L1. We would then have L1(hat, hat), L1(orange, orange), etc.\\nOnly the last relation is transitive. For example, from the fact that the\\nCHAPTER 3. SET THEORY\\n44\\nword orange is longer than the word hat, L(orange, hat), and the fact that\\nthe word appropriate is longer than the word orange, L(appropriate, orange),\\nit follows that the word appropriate is longer than the word hat. That is:\\nL(appropriate, hat).\\nFinally, none of the relations are symmetric. Recall that for a symmetric\\nrelation, the domain and range are interchangeable. An example of a sym-\\nmetric relation might be the notion of the same length as: L2. Thus if the\\nword hat is the same length as the word pan, L2(hat, pan), then it follows\\nthat the word pan is the same length as the word hat: L2(pan, hat).\\n3.8\\nSummary\\nThis chapter introduced basic set theory. We began with a deﬁnition of what\\na set is and provided three mechanisms for deﬁning a set: by enumeration,\\nby properties, and by recursive rules. We also deﬁned a notion of set size\\nsuch that |A| represents the size of some set A.\\nWe went on to consider notions of set membership. An element x can\\nbe a member of a set A, e.g. x ∈ A. Membership allows us to deﬁne the\\nrelations subset and proper subset between sets. We also deﬁned the very\\nimportant notion of set extensionality which holds that two sets are identical\\nif they share the same members. In general terms, this means that sets are\\ndeﬁned solely by their members.\\nWe deﬁned three special sets. First, there is the empty set ∅ which has no\\nmembers. There is also the universal set U, which has all members.5 Finally,\\nwe deﬁned the notion of a power set of any set A, 2A or ℘(A), which is the\\nset of all possible subsets of A.\\nThere are several diﬀerent operations which can be applied to sets. Union\\nmerges two sets. Intersection ﬁnds the overlap of two sets. Complement ﬁnds\\neverything that is not in some set, and diﬀerence removes the elements of\\none set from another.\\nWe presented a number of general laws about sets: Idempotency, Com-\\nmutativity, Associativity, Distributivity, Identity, Domination, Complement\\nLaws, Double Complement Law, Relative Complement Law, and DeMorgan’s\\nLaw. We also showed a few theorems about set theory and showed how we\\nmight prove them using indirect proof.\\n5This is actually a very tricky notion. For example, if the universal set contains all\\nmembers, then does it contain itself?\\nCHAPTER 3. SET THEORY\\n45\\nFinally, we developed a notion of ordered tuple and use it to deﬁne rela-\\ntions and functions. We showed what inverse and complement relations are\\nand deﬁned reﬂexivity, transitivity, and symmetry with respect to relations.\\n3.9\\nExercises\\n1. Provide a recursive deﬁnition of the set of all possible sequences of a\\nand b.\\n2. if A = {a, a, a, a}, what is |A|?\\n3. As we noted on page 34, it follows that if A ⊆ B and B ⊆ A, that\\nA = B. Explain why this is so. Prove that this is so.\\n4. It follows that the empty set is a subset of all other sets. Explain why.\\n5. The empty set is a proper subset of all sets except one. Which one and\\nwhy?\\n6. We gave a number of theorems on page 38. Prove these.\\n7. Ordered groups larger than two are deﬁned recursively in terms of pairs.\\nThus ⟨x, y, z⟩ is deﬁned as ⟨⟨x, y⟩, z⟩.\\nHow do we encode this with\\nsimple sets?\\n8. Use set theory to characterize the diﬀerent meanings we discussed in\\nchapter 1 for Two women saw two men.\\n9. Why would it be a mistake to formalize sentences directly in terms of\\nsets?\\n10. Find a symmetric relation in the syntax of a language that is not your\\nnative language (or English), and that is diﬀerent from the examples\\ndiscussed in the text. Show how it works.\\n11. Give an example of a proper subset relation in language diﬀerent from\\nthe ones discussed in the text.\\nChapter 4\\nSentential Logic\\nIn this chapter, we treat sentential logic, logical systems built on sentential\\nconnectives like and, or, not, if, and if and only if.\\nOur goals here are\\nthreefold:\\n1. To lay the foundation for a later treatment of full predicate logic.\\n2. To begin to understand how we might formalize linguistic theory in\\nlogical terms.\\n3. To begin to think about how logic relates to natural language syntax\\nand semantics (if at all!).\\n4.1\\nThe intuition\\nThe basic idea is that we will deﬁne a formal language with a syntax and\\na semantics.\\nThat is, we have a set of rules for how statements can be\\nconstructed and then a separate set of rules for how those statements can\\nbe interpreted. We then develop—in precise terms—how we might prove\\nvarious things about sets of those statements.\\n4.2\\nBasic Syntax\\nWe start with an ﬁnite set of letters: {p, q, r, s, . . .}.\\nThese become the\\ninﬁnite set of atomic statements when we add in primes, e.g. p′, p′′, p′′′, . . ..\\nThis inﬁnite set can be deﬁned recursively.\\n46\\nCHAPTER 4. SENTENTIAL LOGIC\\n47\\nDeﬁnition 4 (Atomic statements) Recursive deﬁnition:\\n1. Any letter {p, q, r, s, . . .} is an atomic statement.\\n2. Any atomic statement followed by a prime, e.g. p′, q′′, . . . is an atomic\\nstatement.\\n3. There are no other atomic statements.\\nThe ﬁrst clause provides for a ﬁnite number of atomic statements: 26. The\\nsecond clause is the recursive one, allowing for an inﬁnite number of atomic\\nstatements built on the ﬁnite set provided by the ﬁrst clause. The third\\nclause is the limiting case.\\nUsing the sentential connectives, these can be combined into the set of\\nwell-formed formulas: WFFs. We also deﬁne WFF recursively.\\nDeﬁnition 5 (WFF) Recursive deﬁnition:\\n1. Any atomic statement is a WFF.\\n2. Any WFF preceded by ‘¬’ is a WFF.\\n3. Any two WFFs can be made into another WFF by writing one of these\\nsymbols between them, ‘∧’, ‘∨’, ‘→’, or ‘↔’, and enclosing the result\\nin parentheses.\\n4. Nothing else is a WFF.\\nThe ﬁrst clause is the base case. Notice that it already provides for an inﬁnite\\nnumber of WFFs since there are an inﬁnite number of atomic statements.\\nThe second clause is recursive and provides for an inﬁnite number of WFFs\\ndirectly, e.g. ¬p, ¬¬p, ¬¬¬p, etc. The third clause is also recursive and thus\\nalso provides for an inﬁnite number of WFFs. Given two WFFs p and q, it\\nprovides for (p ∧ q), (p ∨ q), (p → q), and (p ↔ q). The fourth clause is the\\nlimiting case. The second and third clauses can be combined with each other\\nto produce inﬁnitely large expressions. For example:\\n(4.1)\\n(p → ¬¬(¬p ∨ (p ∨ p)))\\n((p → p) ↔ (p ∧ q))\\n(p ∧ (q ∨ (r ∧ (s ∨ t))))\\n. . .\\nCHAPTER 4. SENTENTIAL LOGIC\\n48\\nBe careful with this notation. The point of it is precision, so we must\\nbe precise in how it is used. For example, parentheses are required for the\\nelements of the third clause and disallowed for the elements of the second\\nclause. Thus, the following are not WFFs: ¬(p), (¬p), p ∨ q, etc.1\\nThe\\nupshot is that the deﬁnition of a WFF is a primitive ‘syntax’ for our logical\\nlanguage.\\nThis syntax is interesting from a number of perspectives.\\nFirst, it is\\nextremely simple.\\nSecond, it is unambiguous; there is one and only one\\n‘parse’ for any WFF. Third, it is, in some either really important or really\\ntrivial sense, “artiﬁcial”.\\nLet’s look at each of these points a little more\\nclosely.\\n4.2.1\\nSimplicity\\nIf we want to compare the structures we have developed with those of natural\\nlanguage, we can see that the structures proposed here are far more limited.\\nThe best analogy is that atomic statements are like simple clauses and WFFs\\nare combinations of clauses. Thus, we might see p as analogous to Ernie likes\\nlogic and q as analogous to Apples grow on trees. We might then take (p ∧ q)\\nas analogous to Ernie likes logic and apples grow on trees.\\nThis analogy is ﬁne, but it’s easy to see that the structures allowed by\\nsentential logic are quite primitive in comparison with human language.\\nFirst, natural language provides for many many mechanisms to build\\nsimple sentences. Sentential logic only allows the “prime”. Second, while\\nwe have ﬁve ways to build on atomic statements in logic, natural language\\nallows many more ways to combine sentences.\\n1There are some variations in symbology that you’ll ﬁnd when you look at other texts.\\nThe negation symbol in something like ¬a can also be written ∼ a. Likewise, the ‘and’\\nsymbol in something like (a ∧ b) can also be written (a&b).\\nThere are lots of other\\nnotational possibilities and parentheses are treated in diﬀerent ways.\\nCHAPTER 4. SENTENTIAL LOGIC\\n49\\n(4.2) Ernie likes logic\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nand\\nif\\nso\\nwhile\\nunless\\nbecause\\nthough\\nbut\\n. . .\\n\\uf8fc\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8fd\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8fe\\napples grow on trees.\\n4.2.2\\nLack of Ambiguity\\nOn the other hand, this impoverished syntax has a very nice property: it is\\nunambiguous. Recall that sentences in natural language can often be parsed\\nin multiple ways. For example, we saw that a sentence like Ernie saw the man\\nwith the binoculars has two diﬀerent meanings associated with two diﬀerent\\nstructures.\\nThis cannot happen with WFFs; structures will always have a single\\nparse.\\nConsider, for example, a WFF like (p ∧ (q ∧ r)).\\nHere, the fact\\nthat q and r are grouped together before p is included is apparent from the\\nparentheses. The other parse would be given by ((p ∧ q) ∧ r). An ambiguous\\nstructure like (p ∧ q ∧ r) is not legal.\\n4.2.3\\nArtiﬁciality\\nBoth of these properties limit the use of sentential logic in encoding expres-\\nsions of human language. We will see below, however, that the semantics\\nof sentential logic are impoverished as well. It turns out that the syntax is\\ndemonstrably just powerful enough to express a particular set of meanings.\\n4.3\\nBasic Semantics\\nThe set of truth values or meanings that our sentences can have is very im-\\npoverished: T or F. We often interpret these as ‘true’ and ‘false’ respectively,\\nCHAPTER 4. SENTENTIAL LOGIC\\n50\\nbut this does not have to be the case. For example, we might interpret them\\nas ‘blue’ and ‘red’, 0 and 1, ‘apples’ and ‘oranges’, etc. An atomic statement\\nlike p can exhibit either of these values. Likewise, any WFF can bear one of\\nthese values.\\nLet’s go through each of the connectives and see how they aﬀect the\\nmeaning of the larger WFF. Negation reverses the truth value of its WFF.\\nIf p is true, then ¬p is false; if p is false, then ¬p is true.\\n(4.3)\\np\\n¬p\\nT\\nF\\nF\\nT\\nConjunction—logical ‘and’—combines two WFFs. If both are true, the com-\\nbination is true. In all other cases, the combination is false.\\n(4.4)\\np\\nq\\n(p ∧ q)\\nT\\nT\\nT\\nT\\nF\\nF\\nF\\nT\\nF\\nF\\nF\\nF\\nDisjunction—logical ‘or’—combines two WFFs. If both are false, the combi-\\nnation is false. In all other cases, the combination is true.\\n(4.5)\\np\\nq\\n(p ∨ q)\\nT\\nT\\nT\\nT\\nF\\nT\\nF\\nT\\nT\\nF\\nF\\nF\\nThe conditional—or logical ‘if’—is false just in case the left side is true and\\nthe right side is false. In all other cases, it is true.\\nCHAPTER 4. SENTENTIAL LOGIC\\n51\\n(4.6)\\np\\nq\\n(p → q)\\nT\\nT\\nT\\nT\\nF\\nF\\nF\\nT\\nT\\nF\\nF\\nT\\nFinally, the biconditional, or ‘if and only if’, is true when both sides are true\\nor when both sides are false. If the values of the conjuncts do not agree, the\\nbiconditional is false.\\n(4.7)\\np\\nq\\n(p ↔ q)\\nT\\nT\\nT\\nT\\nF\\nF\\nF\\nT\\nF\\nF\\nF\\nT\\nThese truth values should be seen as a primitive semantics. However, as\\nwith the syntax, the semantics diﬀers from natural language semantics in a\\nnumber of ways. For example, the system is obviously a lot simpler than\\nnatural language. Second, it is deterministic. If you know the meanings—\\ntruth values—of the parts, then you know the meaning—truth value—of the\\nwhole.\\nLet’s do some examples.\\n• ((p → q) ∨ r)\\n• ¬(p ↔ p)\\n• ¬¬¬(p ∨ q)\\n• (p ∨ (q ∧ (r ∨ ¬s)))\\n• (p ∨ ¬p)\\nCHAPTER 4. SENTENTIAL LOGIC\\n52\\nLet’s consider the ﬁrst case above: ((p → q) ∨ r). How do we build a\\ntruth table for this? First, we collect the atomic statements: p, q, and r. To\\ncompute the possible truth values of the full WFF, we must consider every\\ncombination of truth values for the component atomic statements.\\nSince\\neach statement can take on one of two values, there are 2n combinations for\\nn statements. In the present case, there are three atomic statements, so there\\nmust be 23 = 2 × 2 × 2 = 8 combinations.\\nThere will then be eight rows in our truth table. The number of columns\\nis governed by the number of atomic statements plus the number of instances\\nof the connectives. In the case at hand, we have three atomic statements and\\ntwo connectives: ‘→’ and ‘∨’. Thus there will be ﬁve columns in our table.\\nWe begin with columns labeled for the three atomic statements. The\\nrows are populated by every possible combination of their truth values: 8.\\n(4.8)\\np\\nq\\nr\\n. . .\\nT\\nT\\nT\\nT\\nT\\nF\\nT\\nF\\nT\\nT\\nF\\nF\\nF\\nT\\nT\\nF\\nT\\nF\\nF\\nF\\nT\\nF\\nF\\nF\\nWe then construct the additional columns by building up from the atomic\\nstatements. This eﬀectively means starting from the most embedded WFFs\\nand working outward. In the case at hand, we next construct a column for\\n(p → q). We do this from the values in columns one and two, following the\\npattern outlined in (4.6).\\nCHAPTER 4. SENTENTIAL LOGIC\\n53\\n(4.9)\\np\\nq\\nr\\n(p → q)\\n. . .\\nT\\nT\\nT\\nT\\nT\\nT\\nF\\nT\\nT\\nF\\nT\\nF\\nT\\nF\\nF\\nF\\nF\\nT\\nT\\nT\\nF\\nT\\nF\\nT\\nF\\nF\\nT\\nT\\nF\\nF\\nF\\nT\\nFinally, we construct the last column from the values in columns three and\\nfour using the pattern outlined in (4.5).\\n(4.10)\\np\\nq\\nr\\n(p → q)\\n((p → q) ∨ r)\\nT\\nT\\nT\\nT\\nT\\nT\\nT\\nF\\nT\\nT\\nT\\nF\\nT\\nF\\nT\\nT\\nF\\nF\\nF\\nF\\nF\\nT\\nT\\nT\\nT\\nF\\nT\\nF\\nT\\nT\\nF\\nF\\nT\\nT\\nT\\nF\\nF\\nF\\nT\\nT\\nThe remaining WFFs are left as an exercise.\\n4.4\\nThe Meanings of the Connectives\\nThe names of the individual connectives and the corresponding methods for\\nconstructing truth tables suggest strong parallels with the meanings of vari-\\nous natural language connectives. There are indeed parallels, but it is essen-\\ntial to keep in mind that the connectives of sentential logic have very precise\\ninterpretations that can diﬀer wildly from our intuitive understandings of\\nCHAPTER 4. SENTENTIAL LOGIC\\n54\\nthe corresponding expressions in English. Let’s consider each one of the con-\\nnectives and see how each diﬀers in interpretation from the corresponding\\nnatural language expression.\\n4.4.1\\nNegation\\nThe negation connective switches the truth value of the WFF it attaches\\nto. Thus ¬p bears the opposite value from p, whatever that is. There are a\\nnumber of ways this diﬀers from natural language.\\nFirst, as we discussed in chapter 2, some languages use two negative\\nwords to express a single negative idea. Thus the Spanish Ernie no vio nada\\n‘Ernie saw nothing’ uses two negative words no and nada to express a single\\nnegative. In the version of sentential logic that we have deﬁned, adding a\\nsecond instance of ‘¬’ undoes the eﬀect of the ﬁrst. Thus ¬¬p bears the\\nsame truth value as p and the opposite truth value from ¬p.\\nAnother diﬀerence between natural language negation and formal senten-\\ntial logic negation can be exempliﬁed with the following pair of sentences:\\n(4.11)\\nErnie likes logic.\\nErnie doesn’t like logic.\\nIn natural language, these sentences do not exhaust the range of possibilities.\\nErnie could simply not care. That is, in natural language a sentence and its\\nnegation do not exhaust the range of possible meanings. In sentential logic,\\nthey do. Thus either p is true or ¬p is true. There is no other possibility.\\n4.4.2\\nConjunction\\nNatural language conjunction is also diﬀerent from sentential logic conjunc-\\ntion. Consider, for example, a sentence like the following:\\n(4.12)\\nErnie went to the library and Hortence read the book.\\nA sentence like this has several implications beyond whether Ernie went to\\nthe library and whether Hortence read some book. In particular, the sentence\\nimplies that these events are connected. For example, the book was probably\\nborrowed from the library. Another implication is that the events happened\\nin the order they are given: Ernie ﬁrst went to the library and then Hortence\\nread the book.\\nCHAPTER 4. SENTENTIAL LOGIC\\n55\\nThese sorts of connections and implications do not apply to a WFF like\\n(p ∧ q). All we know is the relationship between the truth value of the whole\\nand the truth values of the parts: (p ∧ q) is true just in case p is true and q\\nis true.\\n4.4.3\\nDisjunction\\nDisjunction in natural language is also interpreted diﬀerently from logical\\ndisjunction. Consider the following example.\\n(4.13)\\nErnie went to the library or Hortence read the book.\\nA sentence like this has the interpretation that one of the two events holds,\\nbut not both. Thus one might interpret this sentence as being true just in\\ncase the ﬁrst part is true and the second false or the second part is true and\\nthe ﬁrst is false, but not if both parts are true.\\nThis is in contrast to a WFF like (p ∨ q), which is true if either or both\\ndisjuncts are true.\\n4.4.4\\nConditional\\nA natural language conditional is also subject to a diﬀerent interpretation\\nfrom the sentential logic conditional. Consider a sentence like the following:\\n(4.14)\\nIf pigs can ﬂy, Ernie will go to the library.\\nA sentence like this would normally be interpreted as indicating that Ernie\\nwill not be going to the library. That is, if the antecedent is obviously false,\\nthe consequent—the second statement—must also be false.\\nThis is not true of the sentential logic conditional.\\nA sentential logic\\nconditional like (p → q) is false just in case p is true and q is false; in\\nall other cases, it is true. Thus, if p is false, the whole expression is true\\nregardless of the truth value of q.\\n4.4.5\\nBiconditional\\nThe biconditional is probably the least intuitive of the logical connectives.\\nThe expression if and only if does not appear to be used in colloquial English.\\nCHAPTER 4. SENTENTIAL LOGIC\\n56\\nRather, to communicate the content of a biconditional in spoken English, one\\nfrequently hears rather convoluted expressions like the following:\\n(4.15)\\nHortence will read the book if Ernie goes to the library and\\nwon’t if he doesn’t.\\n4.5\\nHow Many Connectives?\\nNotice that each of the connectives, except ¬, can be deﬁned in terms of\\nthe others. The expressions in the second column below are true just when\\nthe expressions in the third column below are true. Thus the connective in\\nthe second column can be deﬁned in terms of the connectives in the third\\ncolumn.\\n(4.16)\\nif\\n(p → q)\\n(¬p ∨ q)\\niﬀ\\n(p ↔ q)\\n((p → q) ∧ (q → p))\\nand\\n(p ∧ q)\\n¬(¬p ∨ ¬q)\\nor\\n(p ∨ q)\\n¬(¬p ∧ ¬q)\\nConsider, for example, the truth table below for the ﬁrst row in (4.16).\\n(4.17)\\np\\nq\\n(p → q)\\n¬p\\n(¬p ∨ q)\\nT\\nT\\nT\\nF\\nT\\nT\\nF\\nF\\nF\\nF\\nF\\nT\\nT\\nT\\nT\\nF\\nF\\nT\\nT\\nT\\nNotice that the values for the third and ﬁfth columns are identical, indicating\\nthat these expressions have the same truth values under the same conditions.\\nWhat this means is that any expression with a conditional can be replaced\\nwith a negation plus disjunction, and vice versa. The same is true for the\\nother rows in the table in (4.16).\\nWe can, therefore, have a logical system that has the same expressive\\npotential as the one we’ve deﬁned with ﬁve connectives, but that has only\\ntwo connectives: ¬ plus and one of →, ∧, or ∨. For example, let’s choose\\nCHAPTER 4. SENTENTIAL LOGIC\\n57\\n¬ and ∧. The following chart shows how all three other connectives can be\\nexpressed with only combinations of those two connectives.\\n(4.18)\\nif\\n(p → q)\\n¬(¬¬p ∧ ¬q)\\niﬀ\\n(p ↔ q)\\n(¬(¬¬p ∧ ¬q) ∧ ¬(¬¬q ∧ ¬p))\\nor\\n(p ∨ q)\\n¬(¬p ∧ ¬q)\\nWhat we’ve done here is take each equivalence in the chart in (4.16) through\\nsuccessive equivalences until all that remains are instances of negation and\\nconjunction. For example, starting with (p → q) (ﬁrst row of the preceding\\ntable), we ﬁrst replace the conditional with negation and disjunction, i.e.\\n(¬p ∨ q). We then use the equivalence of disjunction and conjunction in row\\nthree of (4.16) to replace the disjunction with an instance of conjunction,\\nadding more negations in the process. The following truth table for (p → q)\\nand ¬(¬¬p ∧ ¬q) shows that they are equivalent.\\n(4.19)\\np\\nq\\n(p → q)\\n¬p\\n¬¬p\\n¬q\\n(¬¬p ∧ ¬q)\\n¬(¬¬p ∧ ¬q)\\nT\\nT\\nT\\nF\\nT\\nF\\nF\\nT\\nT\\nF\\nF\\nF\\nT\\nT\\nT\\nF\\nF\\nT\\nT\\nT\\nF\\nF\\nF\\nT\\nF\\nF\\nT\\nT\\nF\\nT\\nF\\nT\\nThe third and eighth columns have identical truth values.\\nIn fact, it is possible to have the same expressive potential with only one\\nconnective: the Sheﬀer stroke, e.g. in (p | q). A formula built on the Sheﬀer\\nstroke is false just in case both components are true; else it is true.\\n(4.20)\\np\\nq\\n(p | q)\\nT\\nT\\nF\\nT\\nF\\nT\\nF\\nT\\nT\\nF\\nF\\nT\\nConsider, for example, how we might use the Sheﬀer stroke to get nega-\\ntion. The following truth table shows the equivalence of ¬p and (p | p).\\nCHAPTER 4. SENTENTIAL LOGIC\\n58\\n(4.21)\\np\\n¬p\\n(p | p)\\nT\\nF\\nF\\nF\\nT\\nT\\nThe following truth table shows how we can use the Sheﬀer stroke to get\\nconjunction.\\n(4.22)\\np\\nq\\n(p ∧ q)\\n(p | q)\\n((p | q) | (p | q))\\nT\\nT\\nT\\nF\\nT\\nT\\nF\\nF\\nT\\nF\\nF\\nT\\nF\\nT\\nF\\nF\\nF\\nF\\nT\\nF\\nWe leave the remaining equivalences as exercises.\\n4.6\\nTautology, Contradiction, Contingency\\nWe’ve seen that the truth value of a complex formula can be built up from the\\ntruth values of its component atomic statements. Sometimes, this process\\nproduces a formula that can be either true or false. Such a formula is referred\\nto as a contingency. All the formulas we have given so far have been of this\\nsort. Sometimes, however, the formula can only be true or can only be false.\\nThe former is a tautology and the latter a contradiction.\\nFor example, p → q is a contingency, since it can be true or false.\\n(4.23)\\np\\nq\\n(p → q)\\nT\\nT\\nT\\nT\\nF\\nF\\nF\\nT\\nT\\nF\\nF\\nT\\nThe rightmost column of this truth table contains instances of T and in-\\nstances of F. Notice that there are no “degrees” of contingency. If both\\nvalues are possible, the formula is contingent.\\nCHAPTER 4. SENTENTIAL LOGIC\\n59\\nWe also have formulas that can only be true: tautologies. For example,\\np → p is a tautology.\\n(4.24)\\np\\n(p → p)\\nT\\nT\\nF\\nT\\nEven though p can be either true or false, the combination can only bear the\\nvalue T. A similar tautology can be built using the biconditional.\\n(4.25)\\np\\n(p ↔ p)\\nT\\nT\\nF\\nT\\nLikewise, we have contradictions: formulas that can only be false, e.g.\\n(p ∧ ¬p).\\n(4.26)\\np\\n¬p\\n(p ∧ ¬p)\\nT\\nF\\nF\\nF\\nT\\nF\\nAgain, even though p can have either value, this combination of connectives\\ncan only be false.\\n4.7\\nProof\\nWe can now consider proof and argument. Our goal is to show how we can\\nreason from some set of WFFs to some speciﬁc conclusion, e.g. another WFF.\\nWe’ll start with the simplest cases: reasoning from one WFF to another or\\nreasoning that some WFF is a tautology.\\nNotice ﬁrst that tautologies built on the biconditional entail that the two\\nsides of the biconditional are logical equivalents. That is, each side is true\\njust in case the other side is true and each side is false just in case the other\\nside is false. For example, ((p ∧ q) ↔ ¬(¬p ∨ ¬q)) has this property. (p ∧ q)\\nCHAPTER 4. SENTENTIAL LOGIC\\n60\\nis logically equivalent to ¬(¬p ∨ ¬q). If we did truth tables for these two\\nWFFs, we would see that they bear the same truth values.\\nConditionals are similar, though not as powerful. For example: ((p∧q) →\\np). The atomic statement p is a logical consequence of (p ∧ q), and is true\\nwhenever the latter is true. However, the converse is not the case; (p ∧ q)\\ndoes not follow from p. We can see this in the following truth table.\\n(4.27)\\np\\nq\\n(p ∧ q)\\n((p ∧ q) → p)\\nT\\nT\\nT\\nT\\nT\\nF\\nF\\nT\\nF\\nT\\nF\\nT\\nF\\nF\\nF\\nT\\nThe rightmost column has only instances of T, so this WFF is a tautology.\\nHowever, comparison of the second and third columns shows that the two\\nsides of the formula do not bear the same value in all instances; they are not\\nlogical equivalents. In particular, row two of the table shows that the left\\nside can be false when the right side is true. This, of course, is allowed by\\na conditional, as opposed to a biconditional, but it exempliﬁes directly that\\nthe values of the two sides need not be identical for the whole WFF to be a\\ntautology.\\nWe can use these relationships—along with a general notion of substitu-\\ntion—to build arguments. The basic idea behind substitution is that if some\\nformula A contains B as a subformula, and B is logically equivalent to C,\\nthen C can be substituted for B in A. For example, we can show by truth\\ntable that p ↔ (p ∧ p).\\n(4.28)\\np\\n(p ∧ p)\\nT\\nT\\nF\\nF\\nIt therefore follows that p and (p ∧ p) are logical equivalents. Hence, any\\nformula containing p can have all instances of p replaced with (p∧p) without\\naltering its truth value. For example, (p → q) has the same truth value as\\n((p ∧ p) → q). This is shown in the following truth table.\\nCHAPTER 4. SENTENTIAL LOGIC\\n61\\n(4.29)\\np\\nq\\n(p ∧ p)\\n(p → q)\\n((p ∧ p) → q)\\nT\\nT\\nT\\nT\\nT\\nT\\nF\\nT\\nF\\nF\\nF\\nT\\nF\\nT\\nT\\nF\\nF\\nF\\nT\\nT\\nThe values for the fourth and ﬁfth columns are the same.\\nSubstitution for logical consequence is more restricted. If some formula\\nA contains B as a subformula, and C is a logical consequence of B, then\\nC cannot be substituted for B in A.\\nA logical consequence can only be\\nsubstituted for the entire formula. Thus if B appears by itself, C may be\\nsubstituted for it.\\nConsider this example: (p ∧ ¬p) → q). First, we can show that this is a\\ntautology by inspection of the truth table.\\n(4.30)\\np\\nq\\n¬p\\n(p ∧ ¬p)\\n((p ∧ ¬p) → q)\\nT\\nT\\nF\\nF\\nT\\nT\\nF\\nF\\nF\\nT\\nF\\nT\\nT\\nF\\nT\\nF\\nF\\nT\\nF\\nT\\nThe rightmost column contains only instances of T; hence the WFF is a\\ntautology. The last connective is a conditional and the values of the two\\nsides do not match in all cases—in particular rows one and three; hence the\\nsecond half of the formula q is a logical consequence of the ﬁrst (p ∧ ¬p).\\nNow, by the same reasoning ((p∧¬p) → r) is also a tautology. However, if\\nwe erroneously do a partial substitution on this second formula, substituting\\nq for the antecedent (p∧¬p) based on the ﬁrst formula, the resulting formula\\n(q → r) is certainly not a tautology.\\nCHAPTER 4. SENTENTIAL LOGIC\\n62\\n(4.31)\\np\\nq\\n(p → q)\\nT\\nT\\nT\\nT\\nF\\nF\\nF\\nT\\nT\\nF\\nF\\nT\\nHence, though partial substitution based on logical equivalence preserves\\ntruth values, partial substitution based on logical consequence does not.\\nThere are a number of logical equivalences and consequences that can be\\nused for substitution. We will refer to these as the Laws of Sentential Logic.\\nThe most interesting thing to note about them is that they are very much\\nlike the laws for set theory in section 3.4. In all of the following we use the\\nsymbol ‘⇐⇒’ to indicate explicitly that the subformulas to each side can be\\nsubstituted for each other in any WFF.\\nIdempotency says that the disjunction or conjunction of identical WFFs\\nis identical to the WFF itself.\\nThis can be compared with set-theoretic\\nidempotency in (3.9).\\n(4.32)\\nIdempotency\\n(p ∨ p) ⇐⇒ p\\n(p ∧ p) ⇐⇒ p\\nAssociativity says that the order in which successive conjunctions or suc-\\ncessive disjunctions are applied can be switched. This can be compared with\\n(3.11).\\n(4.33)\\nAssociativity\\n((p ∨ q) ∨ r) ⇐⇒ (p ∨ (q ∨ r))\\n((p ∧ q) ∧ r) ⇐⇒ (p ∧ (q ∧ r))\\nCommutativity says that the order of conjuncts and disjuncts is irrelevant.\\nCompare with (3.10).\\nCHAPTER 4. SENTENTIAL LOGIC\\n63\\n(4.34)\\nCommutativity\\n(p ∨ q) ⇐⇒ (q ∨ p)\\n(p ∧ q) ⇐⇒ (q ∧ p)\\nDistributivity can “lower” a conjunction into a disjunction or lower a\\ndisjunction into a conjunction. This an be compared with (3.12).\\n(4.35)\\nDistributivity\\n(p ∨ (q ∧ r)) ⇐⇒ ((p ∨ q) ∧ (p ∨ r))\\n(p ∧ (q ∨ r)) ⇐⇒ ((p ∧ q) ∨ (p ∧ r))\\nThe truth value T is the identity element for conjunction and the value\\nF is the identity element for disjunction. This is analogous to the roles of ∅\\nand U in union and intersection, respectively: (3.13).\\n(4.36)\\nIdentity\\n(p ∨ F) ⇐⇒ p\\n(p ∧ T) ⇐⇒ p\\nWe have an equivalent to set-theoretic Domination (3.14) as well.\\n(4.37)\\nDomination\\n(p ∨ T) ⇐⇒ T\\n(p ∧ F) ⇐⇒ F\\nThe Complement Laws govern the role of negation and can be compared\\nwith (3.15).\\n(4.38)\\nComplement Laws\\n(p ∨ ¬p) ⇐⇒ T\\n(p ∧ ¬p) ⇐⇒ F\\nCHAPTER 4. SENTENTIAL LOGIC\\n64\\nThere is also an equivalent to the set-theoretic Double Complement Law\\n(3.16) that governs multiple negations.\\n(4.39)\\nDouble Complement Law\\n¬¬p ⇐⇒ p\\nDeMorgan’s Laws allow us to use negation to relate conjunction and\\ndisjunction, just as set complement can be used to relate intersection and\\nunion: (3.18).\\n(4.40)\\nDeMorgan’s Laws\\n¬(p ∨ q) ⇐⇒ (¬p ∧ ¬q)\\n¬(p ∧ q) ⇐⇒ (¬p ∨ ¬q)\\nThe Conditional and Biconditional Laws deﬁne conditionals and bicon-\\nditionals in terms of the other connectives.\\n(4.41)\\nConditional Laws\\n(p → q) ⇐⇒ (¬p ∨ q)\\n(p → q) ⇐⇒ (¬q → ¬p)\\n(p → q) ⇐⇒ ¬(p ∧ ¬q)\\n(4.42)\\nBiconditional Laws\\n(p ↔ q) ⇐⇒ ((p → q) ∧ (q → p))\\n(p ↔ q) ⇐⇒ ((¬p ∧ ¬q) ∨ (p ∧ q)\\nWe can build arguments with these. For example, we can use these to\\nshow that this statement is a tautology: (((p → q)∧p) → q). First, we know\\nthis is true because of the truth table.\\n(4.43)\\np\\nq\\n(p → q)\\n((p → q) ∧ p)\\n(((p → q) ∧ p) → q)\\nT\\nT\\nT\\nT\\nT\\nT\\nF\\nF\\nF\\nT\\nF\\nT\\nT\\nF\\nT\\nF\\nF\\nT\\nF\\nT\\nCHAPTER 4. SENTENTIAL LOGIC\\n65\\nWe can also show that it’s a tautology using the Laws of Sentential Logic\\nabove. We start out by writing the WFF we are interested in. We then\\ninvoke the diﬀerent laws one by one until we reach a T.\\n(4.44)\\n1\\n(((p → q) ∧ p) → q)\\nGiven\\n2\\n(((¬p ∨ q) ∧ p) → q)\\nConditional\\n3\\n(¬((¬p ∨ q) ∧ p) ∨ q)\\nConditional\\n4\\n((¬(¬p ∨ q) ∨ ¬p) ∨ q)\\nDeMorgan\\n5\\n(¬(¬p ∨ q) ∨ (¬p ∨ q))\\nAssociativity\\n6\\n((¬p ∨ q) ∨ ¬(¬p ∨ q))\\nCommutativity\\n7\\nT\\nComplement\\nThe ﬁrst two steps eliminate the conditionals using the Conditional Laws.\\nWe then use DeMorgan’s Law to replace the conjunction with a disjunction.\\nWe rebracket the expression using Associativity and reorder the terms using\\nCommutativity. Finally, we use the Complement Laws to convert the WFF\\nto T.\\nHere’s a second example: (p → (¬q ∨ p)).\\nWe can show that this is\\ntautologous by truth table:\\n(4.45)\\np\\nq\\n¬q\\n(¬q ∨ p)\\n(p → (¬q ∨ p))\\nT\\nT\\nF\\nT\\nT\\nT\\nF\\nT\\nT\\nT\\nF\\nT\\nF\\nF\\nT\\nF\\nF\\nT\\nT\\nT\\nWe can also show this using substitution and the equivalences above.\\n(4.46)\\n1\\n(p → (¬q ∨ p))\\nGiven\\n2\\n(¬p ∨ (¬q ∨ p))\\nConditional\\n3\\n((¬q ∨ p) ∨ ¬p)\\nCommutativity\\n4\\n(¬q ∨ (p ∨ ¬p))\\nAssociativity\\n5\\n(¬q ∨ T)\\nComplement\\n6\\nT\\nDomination\\nCHAPTER 4. SENTENTIAL LOGIC\\n66\\nFirst, we remove the conditional with the Conditional Laws and reorder the\\ndisjuncts with Commutativity. We then rebracket with Associativity. We\\nuse the Complement Laws to replace (p ∨ ¬p) with T, and Domination to\\npare this down to T.\\nSo far, we have done proofs that some particular WFF is a tautology,\\nwhere we reason from some WFF to T. We can also do proofs of one formula\\nfrom another. In this case, the ﬁrst line of the proof is the ﬁrst formula and\\nthe last line of the proof must be the second formula. For example, we can\\nprove (p ∧ q) from ¬(¬q ∨ ¬p) as follows.\\n(4.47)\\n1\\n¬(¬q ∨ ¬p)\\nGiven\\n2\\n(¬¬q ∧ ¬¬p)\\nDeMorgan\\n3\\n(q ∧ ¬¬p)\\nDoub. Comp.\\n4\\n(q ∧ p)\\nDoub. Comp.\\n5\\n(p ∧ q)\\nCommutativity\\nFirst we use DeMorgan’s Law to eliminate the disjunction. We then use the\\nDouble Complement Law twice to eliminate the negations and Commutativ-\\nity to reverse the order of the terms.\\nWe interpret such a proof as indicating that the the ending WFF is true\\nif the starting WFF is true, in this case, that if ¬(¬q ∨ ¬p) is true, (p ∧ q) is\\ntrue. We will see in section 4.9 that it follows that (¬(¬q ∨ ¬p) → (p ∧ q))\\nmust be tautologous.\\nFinally, let’s consider an example where we need to use a logical conse-\\nquence, rather than a logical equivalence. First, note that ((p∧q) → p) has p\\nas a logical consequence of (p∧q). That is, if we establish that ((p∧q) → p),\\nwe will have that p can be substituted for (p ∧ q). We use ‘=⇒’ to denote\\nthat a term can be substituted by logical consequence: (p ∧ q) =⇒ p. We\\ncan show this by truth table:\\n(4.48)\\np\\nq\\n(p ∧ q)\\n((p ∧ q) → p)\\nT\\nT\\nT\\nT\\nT\\nF\\nF\\nT\\nF\\nT\\nF\\nT\\nF\\nF\\nF\\nT\\nCHAPTER 4. SENTENTIAL LOGIC\\n67\\nWhenever T appears in the third column, T appears in the fourth column.\\nIt now follows that (p ∧ q) =⇒ p.\\nIncidentally, note that the following truth table shows that the bicondi-\\ntional version of this WFF, ((p ∧ q) ↔ p), is not a tautology.\\n(4.49)\\np\\nq\\n(p ∧ q)\\n((p ∧ q) ↔ p)\\nT\\nT\\nT\\nT\\nT\\nF\\nF\\nF\\nF\\nT\\nF\\nT\\nF\\nF\\nF\\nT\\nThis is not a tautology because there is an instance of F in the rightmost\\ncolumn.\\nWe now have: (p ∧ q) =⇒ p. We can use this—in conjunction with the\\nLaws of Sentential Logic—to prove q from ((p → q) ∧ p).\\n(4.50)\\n1\\n((p → q) ∧ p)\\nGiven\\n2\\n((¬p ∨ q) ∧ p)\\nConditional\\n3\\n((¬p ∧ p) ∨ (q ∧ p))\\nDistributivity\\n4\\n((p ∧ ¬p) ∨ (q ∧ p))\\nCommutativity\\n5\\n(F ∨ (q ∧ p))\\nComplement\\n6\\n((q ∧ p) ∨ F)\\nCommutativity\\n7\\n(q ∧ p)\\nIdentity\\n8\\nq\\nEstablished just above\\nFirst, we remove the conditional with the Conditional Laws. We lower the\\nconjunction using Distributivity and reorder terms with Commutativity. We\\nuse the Complement Laws to replace (p ∧ ¬p) with F and reorder again\\nwith Commutativity. We now remove the F with Identity and use our newly\\nproven logical consequence to replace (q ∧ p) with q.\\nSummarizing thus far, we’ve outlined the basic syntax and semantics of\\npropositional logic. We’ve shown how WFFs can be categorized into con-\\ntingencies, tautologies, and contradictions. We’ve also shown how certain\\nWFFs are logical equivalents and others are logical consequences.\\nWe’ve\\nused the Laws of Sentential Logic to construct simple proofs.\\nCHAPTER 4. SENTENTIAL LOGIC\\n68\\n4.8\\nRules of Inference\\nThe Laws of Sentential Logic allow us to progress in a proof from one WFF\\nto another, resulting in a WFF or truth value at the end. For example, we\\nnow know that we can prove q from ((p → q) ∧ p). We can also show that\\n(((p → q) ∧ p) → q) is a tautology.\\nThe Laws do not allow us to reason over sets of WFFs, however. The\\nproblem is that they do not provide a mechanism to combine the eﬀect of\\nseparate WFFs. For example, we do not have a mechanism to reason from\\n(p → q) and p to q.\\nThere are Rules of Inference that allow us to do this. We now present\\nsome of the more commonly used ones.\\nModus Ponens allows us to deduce the consequent of a conditional from\\nthe conditional itself and its antecedent.\\n(4.51)\\nModus Ponens\\nP → Q\\nM.P.\\nP\\nQ\\nModus Tollens allows us to conclude the negation of the antecedent from\\na conditional and the negation of its consequent.\\n(4.52)\\nModus Tollens\\nP → Q\\nM.T.\\n¬Q\\n¬P\\nHypothetical Syllogism allows us to use conditionals transitively.\\n(4.53)\\nHypothetical Syllogism\\nP → Q\\nH.S.\\nQ → R\\nP → R\\nDisjunctive Syllogism allows us to conclude the second disjunct from a\\ndisjunction and the negation of its ﬁrst disjunct.\\nCHAPTER 4. SENTENTIAL LOGIC\\n69\\n(4.54)\\nDisjunctive Syllogism\\nP ∨ Q\\nD.S.\\n¬P\\nQ\\nSimpliﬁcation allows us to conclude that the ﬁrst conjunct of a conjunc-\\ntion must be true.2 Note that this is a logical consequence and not a logical\\nequivalence.\\n(4.55)\\nSimpliﬁcation\\nP ∧ Q\\nSimp.\\nP\\nConjunction allows us to assemble two independent WFFs into a con-\\njunction.3\\n(4.56)\\nConjunction\\nP\\nConj.\\nQ\\nP ∧ Q\\nFinally, Addition allows us to disjoin a WFF and anything.4\\n(4.57)\\nAddition\\nP\\nAdd.\\nP ∨ Q\\nNote that some of the Rules have funky Latin names and corresponding ab-\\nbreviations. These are not terribly useful, but we’ll keep them for conformity\\nwith other treatments.\\nLet’s consider a very simple example. Imagine we are trying to prove q\\nfrom (p → q) and p. We can reason from the two premises to the conclusion\\nwith Modus Ponens (M.P.). The proof is annotated as follows.\\n2This is also called Conjunction Elimination.\\n3This is also called Adjunction or Conjunction Introduction.\\n4This is also called Disjunction Introduction.\\nCHAPTER 4. SENTENTIAL LOGIC\\n70\\n(4.58)\\n1\\n(p → q)\\nGiven\\n2\\np\\nGiven\\n3\\nq\\n1,2 M.P.\\nThere are two formulas given at the beginning.\\nWe then use them and\\nModus Ponens to derive the third step. Notice that we explicitly give the\\nline numbers that we are using Modus Ponens on.\\nHere’s a more complex example. We want to prove p from (t → q), (t∨s),\\n(q → r), (s → p), and ¬r.\\n(4.59)\\n1\\n(t → q)\\nGiven\\n2\\n(t ∨ s)\\nGiven\\n3\\n(q → r)\\nGiven\\n4\\n(s → p)\\nGiven\\n5\\n¬r\\nGiven\\n6\\n¬q\\n3,5 M.T.\\n7\\n¬t\\n1,6 M.T.\\n8\\ns\\n2,7 D.S.\\n9\\np\\n4,8 M.P.\\nFirst, we give the ﬁve statements that we are starting with. We then use\\nModus Tollens on the third and ﬁfth to get ¬q. We use Modus Tollens again\\non the ﬁrst and sixth to get ¬t. We can now use Disjunctive Syllogism to\\nget s and Modus Ponens to get p, as desired.\\nNotice that the Laws and Rules are redundant.\\nFor example, we use\\nModus Tollens above to go from steps 3 and 5 to step 6. We could instead\\nuse one of the Conditional Laws to convert (q → r) to (¬r → ¬q), and then\\nuse Modus Ponens on that and ¬r to make the same step. That is, Modus\\nTollens is unnecessary if we already have Modus Ponens and the Conditional\\nLaws.\\nHere’s another example showing how the Laws of Sentential Logic can be\\nused to massage WFFs into a form that the Rules of Inference can be applied\\nto. Here we wish to prove (p → r) from (p → (q ∨ r)) and ¬q.\\nCHAPTER 4. SENTENTIAL LOGIC\\n71\\n(4.60)\\n1\\n(p → (q ∨ r))\\nGiven\\n2\\n¬q\\nGiven\\n3\\n(¬p ∨ (q ∨ r))\\n1 Cond.\\n4\\n((q ∨ r) ∨ ¬p)\\n3 Comm.\\n5\\n(q ∨ (r ∨ ¬p))\\n4 Assoc.\\n6\\n(r ∨ ¬p)\\n2,5 D.S.\\n7\\n(¬p ∨ r)\\n6 Comm.\\n8\\n(p → r)\\n7 Cond.\\nAs usual, we ﬁrst eliminate the conditional with the Conditional Laws. We\\nthen reorder with Commutativity and rebracket with Associativity. We use\\nDisjunctive Syllogism on lines 2 and 5, reorder, and then reinsert the condi-\\ntional.\\n4.9\\nConditional Proof\\nA special method of proof is available when you want to prove a conditional.\\nImagine you have a conditional of the form (A → B) that you want to prove.\\nWhat you do is assume A and attempt to prove B from it. If you can, then\\nyou can conclude (A → B).\\nHere’s an example showing how to prove (p → r) from (p → (q ∨ r)) and\\n¬q.5\\n(4.61)\\n1\\n(p → (q ∨ r))\\nGiven\\n2\\n¬q\\nGiven\\n3\\np\\nAuxiliary Premise\\n4\\n(q ∨ r)\\n1,3 M.P.\\n5\\nr\\n2,4 D.S.\\n6\\n(p → r)\\n3–5 Conditional Proof\\nWe indicate a Conditional Proof with a left bar. The bar extends from the\\nassumption of the antecedent of the conditional—the “Auxiliary Premise”—\\nto the point where we invoke Conditional Proof. The key thing to keep in\\n5We just proved this above directly on page 71.\\nCHAPTER 4. SENTENTIAL LOGIC\\n72\\nmind is that we cannot refer to anything to the right of that bar below the\\nend of the bar. In this case, we are only assuming p to see if we can get r to\\nfollow. We cannot conclude on that basis that p is true on its own.\\nWe can show that this is the case with a fallacious proof of p from no\\nassumptions. First, here’s an example of Conditional Proof used correctly:\\nwe prove (p → p) from no assumptions.\\n(4.62)\\n1\\np\\nAuxiliary Premise\\n2\\np\\n1 Repeated\\n3\\n(p → p)\\n1–2 Conditional Proof\\nNow we add an incorrect step, referring to material to the right of the bar\\nbelow the end of the bar.\\n(4.63)\\n1\\np\\nAuxiliary Premise\\n2\\np\\n1 Repeated\\n3\\n(p → p)\\n1–2 Conditional Proof\\n4\\np\\n1 Repeated Wrong!\\nIntuitively, this should be clear. The proof above does not establish the truth\\nof p. Here’s another very simple example of Conditional Proof. We prove\\n((p → q) → (¬p ∨ q)).\\n(4.64)\\n1\\n(p → q)\\nAuxiliary Premise\\n2\\n(¬p ∨ q)\\n1 Cond.\\n3\\n((p → q) → (¬p ∨ q))\\n1–2 Conditional Proof\\nThe important thing is to understand the intuition behind Conditional\\nProof. If you want to show that p follows from q, assume that q is true and\\nsee if you get p. Let’s use it now to prove (p → q) from q.\\n(4.65)\\n1\\nq\\nGiven\\n2\\np\\nAuxiliary Premise\\n3\\nq\\n1, Repeated\\n4\\n(p → q)\\n2–3 Conditional Proof\\nNotice in this last proof that when we are to the right of the bar we can refer\\nto something above and to the left of the bar.\\nCHAPTER 4. SENTENTIAL LOGIC\\n73\\n4.10\\nIndirect Proof\\nA proof technique with a similar structure to Conditional Proof that has\\nwide applicability is Indirect Proof or Reductio ad Absurdum. Imagine we\\nare attempting to prove q. We assume ¬q and try to prove a contradiction\\nfrom it. If we succeed, we have proven q. The reasoning is like this. If we\\ncan prove a contradiction from ¬q, then there’s no way ¬q can be true. If\\nit’s not true, it must be false.6 Here’s a simple example, where we attempt\\nto prove (p ∨ q) from (p ∧ q).\\n(4.66)\\n1\\n(p ∧ q)\\nGiven\\n2\\n¬(p ∨ q)\\nAuxiliary Premise\\n3\\n(¬p ∧ ¬q)\\n2 DeMorgan\\n4\\np\\n1 Simp.\\n5\\n¬p\\n3 Simp.\\n6\\n(p ∧ ¬p)\\n4,5 Conj.\\n7\\n(p ∨ q)\\n2–6 Indirect Proof\\nThe indirect portion of the proof is notated like Conditional Proof. Moreover,\\nas with Conditional Proof, the material to the right of the bar cannot be\\nreferred to below the bar.\\nNotice too that when we are in the ‘indirect’\\nportion of the proof, we can refer to material above the bar and to the left.\\nThe restriction on referring to material to the right of the bar is therefore\\n“one-way”, just as with Conditional Proof.\\nHere’s another example of Indirect Proof. We prove ¬r from p and (r →\\n¬p). First, here is one way to prove this via a direct proof.\\n(4.67)\\n1\\np\\nGiven\\n2\\n(r → ¬p)\\nGiven\\n3\\n(¬¬p → ¬r)\\n2 Cond.\\n4\\n(p → ¬r)\\n3 Doub. Comp.\\n5\\n¬r\\n1,4 M.P.\\nThe proof begins as usual with the two assumptions. We reverse the condi-\\ntional with the Conditional Laws and remove the double negation with the\\nDouble Complement Law. Finally, we use Modus Ponens to get ¬r.\\n6Note that this argument goes through only if there are only two values to our logic!\\nCHAPTER 4. SENTENTIAL LOGIC\\n74\\nFollowing, we prove the same thing using Indirect Proof.\\n(4.68)\\n1\\np\\nGiven\\n2\\n(r → ¬p)\\nGiven\\n3\\n¬¬r\\nAuxiliary Premise\\n4\\nr\\n3 Doub. Comp.\\n5\\n¬p\\n2,4 M.P.\\n6\\n(p ∧ ¬p)\\n1,5 Conj.\\n7\\n¬r\\n3–6 Indirect Proof\\nWe begin with the same two assumptions. Since we are interested in proving\\n¬r, we assume the negation of that, ¬¬r, and attempt to prove a contradic-\\ntion from it. First, we remove the double negation with the Double Comple-\\nment Laws and use Modus Ponens to extract ¬p. This directly contradicts\\nour ﬁrst assumption. We conjoin these and that contradiction completes the\\nIndirect Proof, allowing us to conclude ¬r.\\n4.11\\nLanguage\\nThere are three ways in which a system like this is relevant to language.\\nFirst, we can view sentence logic as a primitive language with a very\\nprecise syntax and semantics. The system diﬀers from human language in\\nseveral respects, but it can be seen as an interesting testbed for hypotheses\\nabout syntax, semantics, and the relation between them.\\nSecond, we can use the theory of proof as a model for grammatical de-\\nscription. In fact, this is arguably the basis for modern generative grammar.\\nWe’ve set up a system where we can use proofs to establish that some\\nstatement is or is not a tautology. For example, we can show that something\\nlike ¬(p ∧ ¬p) is tautologous as follows.\\n(4.69)\\n1\\n¬(p ∧ ¬p)\\nGiven\\n2\\n(¬p ∨ ¬¬p)\\n1 DeMorgan\\n3\\n(¬p ∨ p)\\n2 Doub. Compl.\\n4\\n(p ∨ ¬p)\\n3 Commut.\\n5\\nT\\n4 Compl.\\nCHAPTER 4. SENTENTIAL LOGIC\\n75\\nThe steps are familiar from preceding proofs. The key intuition, however,\\nis that these steps demonstrate that our Laws and Rules of Inference will\\ntransform the assumptions that the proof begins with into the conclusion at\\nthe end. We will see later that a similar series of steps can be used to show\\nthat some particular sentence is well-formed in a language with respect to a\\ngrammatical description.\\n4.12\\nSummary\\nThis chapter has introduced the basic syntax and semantics of sentential\\nlogic.\\nWe provided a simple syntax and semantics for atomic statements\\nand the logical connectives that can build on them. The system was very\\nsimple, but has the virtue of being unambiguous syntactically. In addition,\\nthe procedure for computing the truth value of a complex WFF is quite\\nstraightforward.\\nWe also introduced the notion of truth table. These can be used to com-\\npute the truth value of a complex WFF. We saw that there are three basic\\nkinds of WFFs: tautologies, contradictions, and contingencies. Tautologies\\ncan only be true; contradictions can only be false; and contingencies can be\\neither.\\nWe examined the logical connectives more closely. We saw that, while\\nthey are similar in meaning to some human language constructions, they are\\nnot quite the same. In addition, we saw that we don’t actually need as many\\nconnectives as we have. In fact, we could do with a single connective, e.g.\\nthe Sheﬀer stroke.\\nWe then turned to proof, showing how we could reason from one WFF\\nto another and that this provided a mechanism for establishing that some\\nparticular WFF is tautologous.\\nWe proposed sets of Laws and Rules of\\nInference to justify the steps in proofs.\\nLast, we considered two special proof techniques: Conditional Proof and\\nIndirect Proof. We use Conditional Proof to establish that some conditional\\nexpression is tautologous. We use Indirect Proof to establish that the nega-\\ntion of some WFF is tautologous.\\nCHAPTER 4. SENTENTIAL LOGIC\\n76\\n4.13\\nExercises\\n1. Construct truth tables for the remaining WFFs on page 51.\\n2. Express (p → q), (p ↔ q), and (p ∨ q) using only the Sheﬀer stroke.\\n3. Give a contradictory WFF, using diﬀerent techniques from those ex-\\nempliﬁed in the chapter.\\n4. Give a tautologous WFF, using diﬀerent techniques from those exem-\\npliﬁed in the chapter.\\n5. Prove ¬r from (r → q), (q → p) and ¬p.\\n6. Prove r from p, ¬q, and ((¬q ∧ p) → r).\\n7. Prove (p ∨ q) from (p ∧ q) using Conditional Proof.\\n8. Prove (p → s) from (p → ¬q), (r → q), and (¬r → s).\\n9. There are other ways than the Sheﬀer stroke to reduce the connectives\\nto a single connective. Can you work out another that would work and\\nshow that it does? (This is diﬃcult).\\n10. Prove r from (p ∧ ¬p).\\n11. Prove ¬r from (p ∧ ¬p).\\n12. Explain your answers to the two preceding questions.\\n13. Prove (p → q) from q using direct proof, indirect proof, and conditional\\nproof.\\n14. Find rules of grammar that can be expressed with each of the logical\\nconnectives.\\n15. Can you use the proof techniques we’ve developed in this chapter to\\nprove anything useful about the rules you found in the preceding ques-\\ntion?\\nChapter 5\\nPredicate Logic\\nIn this chapter, we consider predicate logic, with functions and quantiﬁers.1\\nThe discussion is broken up into syntax, semantics, and proofs.\\nPredicate logic is a richer system than sentential logic and allows us to\\nmove closer to the kind of system we need for natural language semantics. It\\nallows us to introduce model theory at an introductory level, a set-theoretic\\ncharacterization of predicate logic semantics.\\n5.1\\nSyntax\\nThe syntax of predicate logic can be broken up into a vocabulary and a set of\\nrules for constructing formulas out of that vocabulary. Basically, the primi-\\ntives are constants, variables, predicates, connectives, quantiﬁers, and delim-\\niters. Constants and variables correspond to objects in our world. Constants\\nare like names, e.g. Ernie or Hortence. Variables are more like pronouns, e.g.\\nthey or it. Predicates allow us to describe properties of objects and sets of\\nobjects. Quantiﬁers allow us to refer to sets of things. The connectives and\\ndelimiters are the same ones from sentential logic.\\nThese are the elements of the vocabulary of ﬁrst-order predicate logic:\\nConstants a, b, c, . . .; with or without primes. The primes allow us to con-\\nvert a ﬁnite set of letters into an inﬁnite set of constants.\\nVariables x, y, z, . . .; with or without primes. Again, the primes allow us to\\nconvert a ﬁnite set of letters into an inﬁnite set of variables.\\n1We do not consider identity or function terms.\\n77\\nCHAPTER 5. PREDICATE LOGIC\\n78\\nTerms Constants ∪ Variables = Terms. That is, all the constants and vari-\\nables grouped together constitute the terms of predicate logic.\\nPredicates F, G, H, . . .. Each takes a ﬁxed number of terms. These too can\\nbe marked with primes to get an inﬁnite set of predicates.\\nConnectives The usual suspects: ∧, ∨, ¬, →, ↔. We use the same connec-\\ntives as in sentential logic.\\nQuantiﬁers ∀ and ∃. The ﬁrst is the universal quantiﬁer and the second is\\nthe existential quantiﬁer.\\nDelimiters Parentheses.\\nWith primes, there are an inﬁnite number of constants, variables, and pred-\\nicates.\\nThe vocabulary is used to deﬁne the set of WFFs recursively.\\n1. If P is an n-nary predicate and t1, . . . , tn are terms, then P(t1, . . . , tn)\\nis a WFF. (Notice that the number of terms present must match the\\nnumber of terms required by the predicate.)\\n2. If ϕ and ψ are WFFs, then ¬ϕ, (ϕ∧ψ), (ϕ∨ψ), (ϕ → ψ), and (ϕ ↔ ψ)\\nare WFFs.\\n3. If ϕ is a WFF and x is a variable, then (∀x)ϕ and (∃x)ϕ are WFFs.\\n(The scope of the quantiﬁers in these WFFs is ϕ.)\\n4. That’s it.\\nThe rules here are straightforward, but there are nuances you need to be\\nalert to.\\nNotice that there is no syntactic requirement that the variable next to the\\nquantiﬁer be paired with the same variable in the formula in its scope. Thus\\n(∃x)F(x) is as well-formed a formula as (∃x)F(y). In fact, (∃x)(∀y)(∀y)F(z)\\nis just as well-formed.\\nNotice too that we’ve deﬁned a notion scope. For example, in a WFF\\nlike (∀x)(∀y)G(x, y), the scope of (∀y) is G(x, y) and the scope of (∀x) is\\n(∀y)G(x, y). Notice too that the scope of a quantiﬁer is not simply everything\\nto the right. In a WFF like ((∃x)G(y)∧F(a)), the scope of (∃x) is only G(y).\\nCHAPTER 5. PREDICATE LOGIC\\n79\\nOn the other hand, in a WFF like (∀x)(F(a) ∧ G(b)), the scope of (∀x) is\\n(F(a) ∧ G(b)). Notice how important parentheses and the syntax of a WFF\\nare to determining scope. We will make use of scope in the semantics of\\npredicate logic.\\n5.2\\nSemantics\\nThe semantics of predicate logic can be understood in terms of set theory.\\nFirst, we deﬁne the notion of model.\\nDeﬁnition 6 (Model) A model is a set D and a function f.\\n1. f assigns each constant to a member of D.\\n2. f assigns each one-place predicate to a subset of D.\\n3. f assigns each two-place predicate to a subset of D × D.\\n4. etc.\\nThe basic idea is that a set of constants and predicates are paired with\\nelements from the set of elements provided by the model. We can think of\\nthose elements as things in the world. Each constant can be paired directly\\nwith an element of the model. Thus if our model includes the individual\\nErnie, we might pair the constant a with that individual, e.g. f(a) = Ernie ∈\\nD.\\nPredicates are a little more complex. We can think of each predicate as\\nholding either for a set of individual elements of the model or a set of ordered\\ntuples deﬁned in the model. Each predicate thus deﬁnes a set of elements\\nor tuples of elements. Thus, if the predicate G is taken to deﬁne the set of\\nindividuals that might be in the kitchen at some particular time, we might\\ntake G to be deﬁned as follows: f(G) = {Ernie, Hortence} ⊆ D.\\nPredicates with more than one argument are mapped to a set of tuples.\\nFor example, if our model is the set of sounds of English and F is deﬁned as\\nthe set of consonants in English that are paired for voicing, we would have\\nf(F) = {⟨p, b⟩, ⟨t, d⟩, ⟨k, g⟩, . . .}.2\\n2Voicing refers to vibration of the vocal folds. The diﬀerence can be felt if you say the\\nsounds in isolation with your ﬁnger placed gently to the side of your adam’s apple: there\\nis vibration with the voiced sound during the consonant.\\nCHAPTER 5. PREDICATE LOGIC\\n80\\nNotice that there is no requirement that there be a single model.\\nA\\nlogical system can be paired with any number of models. Thus a predicate\\nG could be paired with a model of individuals and rooms and deﬁne a set of\\nindividuals that are in the kitchen or it could be paired instead with a model\\nof sounds in English and deﬁne the set of nasal consonants.\\nWe can use model theory to understand how predicate logic formulas are\\nevaluated. As with simple sentential logic, predicate logic formulas evaluate\\nto one of two values: T or F.\\nThe logical connectives have their usual\\nfunction, but we must now include a mechanism for understanding predicates\\nand quantiﬁers.\\nConsider predicates ﬁrst. An expression like G(a) is true just in case f(a)\\nis in the subset of D that f assigns G to. For example, if a is paired with\\nErnie and Ernie is in the set of D that G is paired with, F(a) = Ernie and\\nErnie ∈ f(G), then G(a) is true.\\nLikewise, H(a, b) is true just in case ⟨a, b⟩ is in the subset of D × D that\\nf assigns H to. For example, if we take D to be the set of words of English\\nand we take H to be the relation ‘has fewer letters than’, then H(a, b) is\\ntrue just in case the elements we pair a and b with are in the set of ordered\\npairs deﬁned by f(H). For example, if f(a) = hat and f(b) = chair, then\\n⟨hat, chair⟩ ∈ f(H) and H(a, b) is true.\\nQuantiﬁed variables are then understood as follows. Variables range in\\nvalue over the members of D. An expression quantiﬁed with ∀ is true just\\nin case the expression is true of all members of the model D; an expression\\nquantiﬁed with ∃ is true just in case the expression is true of at least one\\nmember of the model D. Let’s go through some examples of this. Assume\\nfor these examples, that D = {m, n, N}.\\nFor example, an expression like (∀x)G(x) is true just in case every member\\nof D is in the set f assigns to G. In the case at hand, we might think of G\\nas ‘is nasal’, in which case, f(G) = {m, n, N}. Since f(G) = D, (∀x)G(x) is\\ntrue.3 On the other hand, if we interpret G as ‘is coronal’, then f(G) = {n}\\nand (∀x)G(x) is false, since f(G) ⊂ D.4 Likewise, (∃x)G(x) is true just in\\ncase the subset of D that f assigns to G has at least one member. If we\\ninterpret G as ‘is coronal’, then this is true, since f maps G to the subset\\n{n}, which has one member, i.e. |f(G)| ≥ 1.\\n3Recall that nasal sounds are produced with air ﬂowing through the nose.\\n4Coronal sounds are produced with the tip of the tongue.\\nCHAPTER 5. PREDICATE LOGIC\\n81\\nMore complex expressions naturally get trickier. Combining quantiﬁed\\nvariables and constants is straightforward. For example, (∀x)H(a, x) is true\\njust in case every member of D can be the second member of each tuple in\\nthe set of ordered pairs that f assigns to H, when a is the ﬁrst member.\\nThus, if f(a) = m, then H(a, x) = {⟨m, m⟩, ⟨m, n⟩, ⟨m, N⟩}.\\nTo get this set of ordered pairs, we might interpret H(a, b) as saying that\\na is at least as anterior as b, where anteriority refers to how far forward\\nin the mouth the main constriction for the sound is. This basically means\\nwhere the sound is made in the mouth. For example, the sound [m] is made\\nat the lips, [t] is made with the tip of the tongue, and [N] is made with the\\nbody of the tongue. On this model, f(H) = {⟨m, m⟩, ⟨m, n⟩, ⟨m, N⟩, ⟨n, n⟩,\\n⟨n, N⟩, ⟨N, N⟩}.\\nWe can use this latter interpretation of H to treat another predicate logic\\nformula: (∀x)H(x, x). Here there is still only one quantiﬁer and no connec-\\ntives, but there is more than one quantiﬁed variable. The same variable letter\\nis used twice; the interpretation is that both arguments of H must be the\\nsame. This expression is true if H can pair all elements of D with themselves.\\nThis is true in the just preceding case since {⟨m, m⟩, ⟨n, n⟩, ⟨N, N⟩} ⊆ f(H).\\nThat is, every sound in the set D is at least as anterior as itself!\\nLet’s now consider how to interpret quantiﬁers and connectives in formu-\\nlas together. The simplest case is where some connective occurs ‘outside’ any\\nquantiﬁer, e.g. ((∀x)G(x) ∧ (∀y)H(y)). This is true just in case f(G) = D\\nand f(H) = D, that is, if G is true of all the members of D and H is true of\\nall the members of D, e.g. (f(G) = D ∧ f(H) = D).\\nIf the universal quantiﬁer ∀ is ‘outside’ the connective, (∀x)(G(x)∧H(x)),\\nthe formula ends up having the same truth value, but the interpretation is a\\nlittle diﬀerent. This latter formula is true on the model where G and H apply\\nto every member of D. Here, we interpret the conjunction within the scope\\nof the universal quantiﬁer in its set-theoretic form: intersection. We then\\nintersect f(G) and f(H). If this intersection is D, then the original expression\\nis true. Thus (∀x)(G(x) ∧ H(x)) is true just in case (f(G) ∩ f(H)) = D is\\ntrue.5\\nSince expressions involving conjunction and universal quantiﬁcation mean\\nthe same thing whether the quantiﬁer is outside or inside the connective,\\nit follws that a universal quantiﬁer and be raised from or lowered into a\\nconjunction changing the scope of the quantiﬁer with no diﬀerence in truth\\n5It is a theorem of set theory that if A ∩ B = U, then A = B = U.\\nCHAPTER 5. PREDICATE LOGIC\\n82\\nvalue.\\nWith the existential quantiﬁer, scope forces diﬀerent interpretations. The\\nexpression with the existential quantiﬁer inside conjunction:\\n((∃x)G(x) ∧ (∃y)H(y))\\ndoes not have the same value as the expression with the existential quantiﬁer\\noutside the conjunction:\\n(∃x)(G(x) ∧ H(x)).\\nThe ﬁrst is true just in case there is some element of D that G holds of\\nand there is some element of D that H holds of; the two elements need not\\nbe the same. In formal terms: |f(G)| ≥ 1 and |f(H)| ≥ 1, e.g. (|f(G)| ≥\\n1 ∧ |f(H)| ≥ 1). The two sets, f(G) and f(H), need not have any elements\\nin common. The second is true only if there is some element of D that both\\nG and H hold of, that is in f(G) and in f(H); that is, they must have at\\nleast one element in common: |(f(G) ∩ f(H))| ≥ 1.\\nThe point of these latter two examples is twofold. First, nesting connec-\\ntives within the scope of quantiﬁers requires that we convert logical connec-\\ntives to set-theoretic operations. Second, nesting quantiﬁers and connectives\\ncan result in diﬀerent interpretations depending on the quantiﬁer and de-\\npending on the connective.\\nThe relative scope of more than one quantiﬁer also matters. Consider the\\nfollowing four formulas.\\n(5.1)\\na.\\n(∀x)(∀y)G(x, y)\\nb.\\n(∃x)(∃y)G(x, y)\\nc.\\n(∀x)(∃y)G(x, y)\\nd.\\n(∃x)(∀y)G(x, y)\\nWhen the quantiﬁers are the same, their relative nesting is irrelevant. Thus\\n(5.1a) is true just in case every element can be paired with every element,\\ni.e. f(G) = D × D. Reversing the universal quantiﬁers gives exactly the\\nsame interpretation: (∀y)(∀x)G(x, y). Likewise, (5.1b) is true just in case\\nthere’s at least one pair of elements in D that G holds of, i.e. |f(G)| ≥ 1.\\nCHAPTER 5. PREDICATE LOGIC\\n83\\nReordering the existential quantiﬁers does not change this interpretation:\\n(∃y)(∃x)G(x, y).\\nOn the other hand, when the quantiﬁers are diﬀerent, the interpretation\\nchanges depending on which quantiﬁer comes ﬁrst. Example (5.1c) is true\\njust in case every member of D can occur as the ﬁrst member of at least one\\nof the ordered pairs of f(G). Reversing the quantiﬁers produces a diﬀerent\\ninterpretation. Thus (∃y)(∀x)G(x, y) means that there is at least one element\\ny that can occur as the second member of a pair with all elements.\\nThese diﬀerent interpretations can be depicted below. The required in-\\nterpretation for (5.1c) with respect to our model is this:\\n(5.2)\\nG(m, ?)\\nG(n, ?)\\nG(N, ?)\\nIt doesn’t matter what the second member of each of these pairs is, as long\\nas f(G) includes at least three with these ﬁrst elements.\\nWhen we reverse the quantiﬁers, we must have one of the following three\\nsituations: a, b, or c.\\n(5.3)\\na.\\nG(m, m)\\nG(n, m)\\nG(N, m)\\nb.\\nG(m, n)\\nG(n, n)\\nG(N, n)\\nc.\\nG(m, N)\\nG(n, N)\\nG(N, N)\\nAll members of D must be paired with some unique member of D.\\nThe interpretation of (5.1d) is similar to the interpretation of (5.1c) with\\nreversed quantiﬁers; it is true just in case there is some element that can be\\npaired with every element of D as the second member of the ordered pair.\\nWe must have m, n, and N as the second member of the ordered pairs, but\\nthe ﬁrst member must be the same across all three.\\n(5.4)\\na.\\nG(m, m)\\nG(m, n)\\nG(m, N)\\nb.\\nG(n, m)\\nG(n, n)\\nG(n, N)\\nc.\\nG(N, m)\\nG(N, n)\\nG(N, N)\\nSome unique member of D must be paired with all members of D.\\nReversing the quantiﬁers in (5.1d), (∀y)(∃x)G(x, y), is interpreted like\\nthis.\\nCHAPTER 5. PREDICATE LOGIC\\n84\\n(5.5)\\nG(?, m)\\nG(?, n)\\nG(?, N)\\nEvery member of D must occur as the second member of some pair in f(G).\\nThe interpretation of quantiﬁer scope can get quite tricky in more complex\\nWFFs.\\nFinally, notice that this system provides no interpretation for unquantiﬁed\\nor free variables. Thus a syntactically well-formed expression like G(x) has\\nno interpretation. This would seem to be as it should be.\\n5.3\\nLaws and Rules\\nWe can reason over formulas with quantiﬁers, but we need some additional\\nLaws and Rules of Inference.\\n5.3.1\\nLaws\\nAs we’ve already seen, quantiﬁers can distribute over logical connectives in\\nvarious ways. The Laws of Quantiﬁer Distribution capture the relationships\\nbetween the quantiﬁers and conjunction and disjunction. We can separate\\nthese into logical equivalences and logical consequences. Here are the equiv-\\nalences.\\n(5.6)\\nLaws of Quantiﬁer Distribution: Equivalences\\nLaw 1: QDE1\\n¬(∀x)ϕ(x) ⇐⇒ (∃x)¬ϕ(x)\\nLaw 2: QDE2\\n(∀x)(ϕ(x) ∧ ψ(x)) ⇐⇒ ((∀x)ϕ(x) ∧ (∀x)ψ(x))\\nLaw 3: QDE3\\n(∃x)(ϕ(x) ∨ ψ(x)) ⇐⇒ ((∃x)ϕ(x) ∨ (∃x)ψ(x))\\nConsider ﬁrst Law 1 (QDE1). This says that that if ϕ(x) is not universally\\ntrue, then there must be at least one element for which ϕ(x) is not true. This\\nactually follows directly from what we have said above and the fact that\\nthe set-theoretic equivalent of negation is complement.\\nThe set-theoretic\\ntranslation of ¬(∀x)ϕ(x) is ¬(f(ϕ) = D). If this is true, then it follows that\\nthe complement of f(ϕ) contains at least one element. The model-theoretic\\ninterpretation of the right side of the ﬁrst law says just this: f(ϕ)′ ≥ 1.\\nCHAPTER 5. PREDICATE LOGIC\\n85\\nLaw 2 (QDE2) allows us to move a universal quantiﬁer down into a con-\\njunction. The logic is that if something true for a whole set of predicates,\\nthen it is true for each individual predicate. Law 3 allows us to move an\\nexistential quantiﬁer down into a disjunction. The logic is that if something\\nis true for at least one of a set of predicates, then it is true for at least one of\\nthem, each considered independently. Both Law 1 and Law 2 should be ex-\\npected given the example we went through in the previous section to explain\\nthe relationship of quantiﬁers and connectives. In general, the relationships\\nabove can be made sense of if we think of the universal quantiﬁer as a con-\\njunction of all the elements of the model D and the existential quantiﬁer as\\na disjunction of all the elements of the model D. Thus:\\n(∀x)F(x) =\\n|D|\\n�\\ni=1\\nF(xi)\\nThe big wedge symbol is interpreted as indicating that every element x in\\nthe superscripted set D is conjoined together. In other words, (∀x)F(x) is\\ntrue just in case we apply F to every member of D and conjoin the values.\\nThe same is true for the existential quantiﬁer:\\n(∃x)F(x) =\\n|D|\\n�\\ni=1\\nF(xi)\\nThe big ‘v’ symbol is interpreted as indicating that every element x in the\\nsuperscripted set D is disjoined together. In other words, (∃x)F(x) is true\\njust in case we apply F to every member of D and disjoin the values.\\nGiven the associativity of disjunction and conjunction (4.33), it follows\\nthat the universal quantiﬁer can raise and lower into a conjunction and the\\nexistential quantiﬁer can raise and lower into a disjunction.\\nFor example, imagine that our universe D is composed of only two ele-\\nments a and b. If would then follow that an expression like\\n(∀x)(F(x) ∧ G(x))\\nis equivalent to\\nCHAPTER 5. PREDICATE LOGIC\\n86\\n((F(a) ∧ G(a)) ∧ (F(b) ∧ G(b)))\\nUsing Associativity, we can move terms around to produce\\n((F(a) ∧ F(b)) ∧ (G(a) ∧ G(b)))\\nTranslating each conjunct back, this is equivalent to\\n((∀x)F(x) ∧ (∀x)G(x))\\nLaw 1 can also be cast in these terms, given DeMorgan’s Law (4.40).\\nThus\\n¬(∀x)F(x) ⇐⇒ (∃x)¬F(x)\\nis really the same thing as:\\n¬\\n|D|\\n�\\ni=1\\nF(xi) ⇐⇒\\n|D|\\n�\\ni=1\\n¬F(xi)\\nThe only diﬀerence is that the quantiﬁers range over a whole set of values\\nfrom D, not just a pair of values. Here are the logical consequences that\\nrelate to quantiﬁer distribution.\\n(5.7)\\nLaws of Quantiﬁer Distribution: Consequences\\nLaw 4: QDC4\\n((∀x)ϕ(x) ∨ (∀x)ψ(x)) =⇒ (∀x)(ϕ(x) ∨ ψ(x))\\nLaw 5: QDC5\\n(∃x)(ϕ(x) ∧ ψ(x)) =⇒ ((∃x)ϕ(x) ∧ (∃x)ψ(x))\\nLaw 4 (QDC4) says that a universal quantiﬁer can be raised out of a\\ndisjunction. This is a logical consequence, not a logical equivalence. Thus, if\\nwe know that ((∀x)G(x) ∨ (∀x)H(x)), then we know (∀x)(G(x) ∨ H(x)), but\\nnot vice versa. For example, if we know that everybody in the room either all\\nlikes logic or all likes rock climbing, then we know that everybody in the room\\nCHAPTER 5. PREDICATE LOGIC\\n87\\neither likes logic or likes rock climbing. However, if we know the latter, we\\ncannot conclude the former. The latter is consistent with a situation where\\nsome people like logic, but other people like rock climbing. The former does\\nnot have this interpretation. Casting this in model-theoretic terms, if we\\nhave that (f(G) = D ∨ f(H) = D), then we we have (f(G) ∪ f(H)) = D.\\nLaw 5 (QDC5) says that an existential quantiﬁer can be lowered into\\na conjunction. As with Law 4, this is a logical consequence, not a logical\\nequivalence. Thus, if we know (∃x)(G(x)∧H(x)), then we know ((∃x)G(x)∧\\n(∃x)H(x)), but not vice versa. For example, if we know that there is at least\\none person in the room that either likes logic or likes rock climbing, then we\\nknow that at least one person in the room likes logic or at least one person in\\nthe room likes rock climbing. The converse implication does not hold. From\\nthe fact that somebody likes logic or somebody likes rock climbing, it does not\\nfollow that that is the same somebody. Casting this in model-theoretic terms,\\nif we have that (f(G) ∩ f(H)) ≥ 1, then we have (f(G) ≥ 1 ∧ f(H) ≥ 1).\\nNow consider the Laws of Quantiﬁer Scope. These govern when quantiﬁer\\nscope is and is not relevant. As above, there are logical equivalences and\\nlogical consequences. Here are the logical equivalences.\\n(5.8)\\nLaws of Quantiﬁer Scope: Equivalences\\nLaw 6: QSE6\\n(∀x)(∀y)ϕ(x, y) ⇐⇒ (∀y)(∀x)ϕ(x, y)\\nLaw 7: QSE7\\n(∃x)(∃y)ϕ(x, y) ⇐⇒ (∃y)(∃x)ϕ(x, y)\\nThese are straightforward. The ﬁrst (QSE6) says the relative scope of two\\nuniversal quantiﬁers is irrelevant. The second (QSE7) says the relative scope\\nof two existential quantiﬁers is irrelevant.\\nThere is one logical consequence that relates to quantiﬁer scope.\\n(5.9)\\nLaws of Quantiﬁer Scope: Consequences\\nLaw 8: QSC8\\n(∃x)(∀y)ϕ(x, y) =⇒ (∀y)(∃x)ϕ(x, y)\\nQSC8 reﬂects an implicational relationship between the antecedent and\\nthe consequent. The antecedent is true just in case there is some x that bears\\nϕ to every y. The consequent is true just in case for every y there is at least\\none x, not necessarily the same one, that it bears ϕ to.\\nCHAPTER 5. PREDICATE LOGIC\\n88\\n5.3.2\\nRules of Inference\\nThere are four Rules of Inference for adding and removing quantiﬁers. You\\ncan use these to convert quantiﬁed expressions into simpler expressions with\\nconstants that our existing Laws and Rules will apply to, and then convert\\nthem back. These are thus extremely useful.\\nUniversal Instantiation (U.I.) allows us to replace a universal quantiﬁer\\nwith any arbitrary constant.\\n(5.10)\\nUniversal Instantiation (U.I.)\\n(∀x)ϕ(x)\\n∴\\nϕ(c)\\nThe intuition is that if ϕ is true of everything, then it is true of any individual\\nthing we might cite.\\nUniversal Generalization (U.G.) allows us to assume an arbitrary individ-\\nual v and establish some fact about it. If something is true of v, then it must\\nbe true of anything. Hence, v can be replaced with a universal quantiﬁer.\\nThe intuition behind v is something like generic names like John/Jane Doe,\\nJoe Sixpack, or Anyman. We use these designations to refer to a person who\\ncould be anybody.\\n(5.11)\\nUniversal Generalization (U.G.)\\nϕ(v)\\n∴\\n(∀x)ϕ(x)\\nThe constant v is special; only it can be replaced with the universal quantiﬁer.\\nOther constants, a, b, c, . . ., cannot. The intuition here is that if we establish\\nsome property holds of an arbitrary individual, then it must hold of all\\nindividuals. Notice that Universal Generalization (5.11) can proceed only\\nfrom v, but Universal Instantiation (5.10) can instantiate to any constant,\\nincluding v.\\nExistential Generalization (E.G.) allows us to proceed from any constant\\nto an existential quantiﬁer.\\nCHAPTER 5. PREDICATE LOGIC\\n89\\n(5.12)\\nExistential Generalization (E.G.)\\nϕ(c)\\n∴\\n(∃x)ϕ(x)\\nThus if some property holds of some speciﬁc individual, we can conclude that\\nit holds of at least one individual.6\\nFinally, Existential Instantiation (E.I.) allows us to go from an existential\\nquantiﬁer to a constant, as long as the constant has not been used yet in the\\nproof. It must be a new constant.\\n(5.13)\\nExistential Instantiation (E.I.)\\n(∃x)ϕ(x)\\n∴\\nϕ(w)\\nwhere w is a new constant\\nThis one is a bit tricky to state in intuitive terms. The basic idea is that if\\nwe know that some property holds of at least one individual, we can name\\nthat individual (as long as we don’t use a name we already know).\\nThe intuition might be best understood with an example. Imagine we\\nknow that somebody among us has taken the last cookie and we’re trying\\nto ﬁgure out who it is. We might start our inquiry like this: “We know\\nthat somebody has taken the cookie. For discussion, let’s call the culprit\\nBob. . . ” That discussion could then proceed by discussing the clues we have,\\nnarrowing in on Bob’s identity. Notice that this kind of discussion could not\\nproceed like this at all if one of the suspects was actually named Bob.\\n5.4\\nProofs\\nLet’s now look at some simple proofs using this new machinery. First, we\\nconsider an example of Universal Instantiation. We prove H(a) from G(a)\\nand (∀x)(G(x) → H(x)).\\n6Here, we can proceed from any of the normal constants or from the arbitrary constant\\nv.\\nCHAPTER 5. PREDICATE LOGIC\\n90\\n(5.14)\\n1\\nG(a)\\nGiven\\n2\\n(∀x)(G(x) → H(x))\\nGiven\\n3\\nG(a) → H(a)\\n2, U.I.\\n4\\nH(a)\\n1,3, M.P.\\nFirst, we remove the universal quantiﬁer with Universal Instantiation and\\nthen use Modus Ponens to get the desired conclusion.\\nNext, we have an example of Universal Generalization. We try to prove\\n(∀x)(F(x) → H(x)) from (∀x)(F(x) → G(x)) and (∀x)(G(x) → H(x)).\\n(5.15)\\n1\\n(∀x)(F(x) → G(x))\\nGiven\\n2\\n(∀x)(G(x) → H(x))\\nGiven\\n3\\n(F(v) → G(v))\\n1, U.I.\\n4\\n(G(v) → H(v))\\n2, U.I.\\n5\\n(F(v) → H(v))\\n3,4 H.S.\\n6\\n(∀x)(F(x) → H(x))\\n5 U.G.\\nFirst, we use Universal Instantiation on the two initial assumptions, mas-\\nsaging them into a form appropriate for Hypothetical Syllogism. We then\\nuse Universal Generalization to convert that result back into a universally\\nquantiﬁed expression. Notice how we judiciously chose to instantiate to v,\\nanticipating that we would be using Universal Generalization later.\\nFinally, we consider a case of Existential Instantiation (E.I.). We prove\\n((∃x)S(x) ∧ (∃x)T(x)) from (∃x)(S(x) ∧ T(x)).\\n(5.16)\\n1\\n(∃x)(S(x) ∧ T(x))\\nGiven\\n2\\n(S(a) ∧ T(a))\\n1 E.I.\\n3\\nS(a)\\n2 Simp.\\n4\\n(∃x)S(x)\\n3 E.G.\\n5\\nT(a)\\n2 Simp.\\n6\\n(∃x)T(x)\\n5 E.G.\\n7\\n((∃x)S(x) ∧ (∃x)T(x))\\n4,6 Conj.\\nFirst, we use Existential Instantiation to strip the quantiﬁer and replace\\nthe variable with a new constant.\\nWe then split oﬀ the conjuncts with\\nCHAPTER 5. PREDICATE LOGIC\\n91\\nSimpliﬁcation and use Existential Generalization on each to add separate\\nnew existential quantiﬁers. We then conjoin the results with Conjunction.\\nNotice that the basic strategy in most of these proofs is fairly clear. Sim-\\nplify the initial formulas so that quantiﬁers can be removed. Manipulate the\\ninstantiated formulas using the Laws and Rules from the preceding chapter.\\nFinally, generalize to appropriate quantiﬁers.\\nThe Laws allow us to prove things without ﬁrst removing quantiﬁers.\\nFor example, in the following proof, we conclude ((∀x)F(x) ∧ G(a)) from\\n(∀x)(F(x) ∧ G(x)).\\n(5.17)\\n1\\n(∀x)(F(x) ∧ G(x))\\nGiven\\n2\\n((∀x)F(x) ∧ (∀x)G(x))\\n1 QDE2\\n3\\n(∀x)F(x)\\n2 Simp.\\n4\\n(∀x)G(x)\\n3 Simp.\\n5\\nG(a)\\n4 U.I.\\n6\\n((∀x)F(x) ∧ G(a))\\n4,5 Conj.\\nFirst, we use QDE2 to lower the universal quantiﬁer into the conjunction.\\nWe extract each conjunct and then use Universal Instantiation on the second\\nconjunct before re-conjoining.\\nHere’s a second example, where we prove ¬(∀x)F(x) from (¬F(c)∧G(a)).\\n(5.18)\\n1\\n(¬F(c) ∧ G(a))\\nGiven\\n2\\n¬F(c)\\n1 Simp.\\n3\\n(∃x)¬F(x)\\n2 E.G.\\n4\\n¬(∀x)F(x)\\n3 QDE1\\nHere we extract the ﬁrst conjunct and use Existential Generalization to add\\nan existential quantiﬁer. We can then use QDE1 to move the negation out-\\nward and replace the existential with a universal quantiﬁer.\\n5.4.1\\nIndirect Proof\\nWe can use our other proof techniques with predicate logic too. Here we\\nshow how to use indirect proof. We prove ((∀x)G(x) → (∃x)G(x)) from no\\nassumptions.\\nCHAPTER 5. PREDICATE LOGIC\\n92\\n(5.19)\\n1\\n¬((∀x)G(x) → (∃x)G(x))\\nAuxiliary Premise\\n2\\n¬¬((∀x)G(x) ∧ ¬(∃x)G(x))\\n1 Cond.\\n3\\n((∀x)G(x) ∧ ¬(∃x)G(x))\\n2 Doub. Compl.\\n4\\n(∀x)G(x)\\n3 Simp.\\n5\\n¬(∃x)G(x)\\n3 Simp.\\n6\\nG(a)\\n4 U.I.\\n7\\n(∃x)G(x)\\n6 E.G.\\n8\\n((∃x)G(x) ∧ ¬(∃x)G(x))\\n5,7 Add.\\n9\\n((∀x)G(x) → (∃x)G(x))\\n1–8 Indirect Proof\\nWe start oﬀ by negating our conclusion and then attempting to produce a\\ncontradiction. The basic idea is to convert the negated conditional into a\\nconjunction. We then extract the conjuncts. We use Universal Instantia-\\ntion and then Existential Generalization on the ﬁrst conjunct to produce a\\ncontradiction to the second conjunct. We conjoin the contradicting WFFs,\\ncompleting the indirect proof.\\n5.4.2\\nConditional Proof\\nWe can prove the same thing by Conditional Proof.\\nWe assume the an-\\ntecedent (∀x)G(x) and then attempt to prove the consequent (∃x)G(x) from\\nit.\\n(5.20)\\n1\\n(∀x)G(x)\\nAuxiliary Premise\\n2\\nG(a)\\n1 U.I.\\n3\\n(∃x)G(x)\\n2 E.G.\\n4\\n((∀x)G(x) → (∃x)G(x))\\n1–3 Conditional Proof\\nWe begin by assuming the antecedent. From that assumption, we can in-\\nstantiate to a constant and then generalize to an existential quantiﬁer. That\\ncompletes the proof.\\n5.5\\nSummary\\nIn this chapter, we have treated the basics of predicate logic, covering syntax,\\nsemantics, and proof mechanisms.\\nCHAPTER 5. PREDICATE LOGIC\\n93\\nWe began with an introduction of the basic syntax of the system. Well-\\nformed formulas of predicate logic (WFFs) are built on well-formed atomic\\nstatements. Atomic statements are built up form a ﬁnite alphabet of (lower-\\ncase) letters and a potentially inﬁnite number of primes, e.g. p, q, r, p′, q′, q′′,\\netc.\\nThese, in turn, are combined via a restricted set of connectives into well-\\nformed formulas. There are three important diﬀerences with respect to simple\\nsentential logic. First, we have predicate symbols, e.g. F(a) or G(b, c), etc. In\\naddition, we have the universal and existential quantiﬁers: ∀ and ∃. Finally,\\nwe have a notion of scope with respect to quantiﬁers which is important in\\nthe semantics of predicate logic.\\nThe semantics of predicate logic is more complex than that of sentential\\nlogic. Speciﬁcally, formulas are true or false with respect to a model, where\\na model is a set of individuals and a mapping from elements of the syntax to\\nsets of elements drawn from the set of elements in the model.\\nQuantiﬁers control how many individuals must be in the range of the\\nmapping. For example, (∃x)F(x) is true only if the predicate F is mapped\\nto at least one individual; (∀x)G(x) is true only if the predicate G is mapped\\nto all individuals. These restrictions hold for any predicate in the scope of\\nthe quantiﬁer with an as yet free variable.\\nAll the Laws and Rules of Inference of sentential logic apply to predicate\\nlogic as well. However, there are additional Laws and Rules that govern quan-\\ntiﬁers. The Laws of Quantiﬁer Distribution and Scope govern the relations\\nbetween quantiﬁers and between quantiﬁers and connectives. The Rules of\\nInstantiation and Generalization govern how quantiﬁers can be added to or\\nremoved from formulas.\\nThe chapter concluded with demonstrations of these various Laws and\\nRules in proofs. We also showed how Conditional Proof and Indirect Proof\\ntechniques are applicable in predicate logic.\\n5.6\\nExercises\\n1. Identify the errors in the following WFFs:\\n(a) (∀x)G(y) → H(x)\\n(b) (∀z)(F(z) ⇐⇒ G(z))\\n(c) (F(x) ∧ G(y) ∧ H(z))\\nCHAPTER 5. PREDICATE LOGIC\\n94\\n(d) F(X′)\\n(e) ¬(¬(∀z)(F(x) ∨ K(w)))\\n(f) (F(x) ← ¬F(x))\\n2. In the following WFFs, mark the scope of each quantiﬁer with labelled\\nunderlining.\\n(a) (∀x)(∀y)(∀z)F(a)\\n(b) ((∀x)F(y) ∧ (∃y)G(x))\\n(c) (∃x)((∀y)F(x) ↔ F(y))\\n(d) (∃z)(F(a) ∧ (F(b) ∧ (G(c) → F(z))))\\n(e) ¬(∃y)¬¬F(y)\\n3. For the following questions, assume this model:\\nD = {canto, cantas, canta, cantamos, cantan}\\nf(FIRST) = {canto, cantamos}\\nf(SECOND) = {cantas}\\nf(THIRD) = {canta, cantan}\\nf(SG) = {canto, cantas, canta}\\nf(PL) = {cantamos, cantan}\\nFor each of the following WFFs, indicate whether it is true or false with\\nrespect to this model.\\n(a) (∃x)FIRST(x)\\n(b) (∀x)FIRST(x)\\n(c) (∀x)(SECOND(x) ∨ PL(x))\\n(d) (∀y)(SG(x) ∨ PL(x))\\n(e) (∃z)(SG(z) ∧ SECOND(z))\\n4. Prove ¬(∀z)(F(z) ∧ G(z)) from (¬F(a) ∨ ¬G(a)).\\n5. Prove G(a) from (∃x)(G(x) ∧ F(x)).\\nCHAPTER 5. PREDICATE LOGIC\\n95\\n6. Prove (∀x)(∃y)G(x, y) from (∀z)(∀x)G(x, z).\\n7. Prove that (∀x)(G(x) ∨ ¬G(x)) is a tautology.\\n8. Prove that ((∀x)F(x) → (∃x)F(x)) is a tautology.\\n9. Prove F(a) from (∀x)F(x) using Indirect Proof.\\n10. Prove ¬(∀x)F(x) from ¬F(a) using Indirect Proof.\\n11. Use Conditional Proof to prove ((∃x)F(x) → (∃x)G(x)) from the\\nWFFs ((∃x)F(x) → (∀z)H(z)) and (H(a) → G(b)).\\n12. Construct a set of predicates and a model for a small area of language.\\nChapter 6\\nFormal Language Theory\\nIn this chapter, we introduce formal language theory, the computational the-\\nory of languages and grammars. Formal language theory is actually inspired\\nby formal logic, enriched with insights from the theory of computation.\\nWe begin with the deﬁnition of a language and then proceed to a rough\\ncharacterization of the basic Chomsky hierarchy. We then turn to a more de-\\ntailed consideration of the types of languages in the hierarchy and automata\\ntheory.\\n6.1\\nLanguages\\nWhat is a language? Formally, a language L is deﬁned as as set (possibly\\ninﬁnite) of strings over some ﬁnite alphabet.\\nDeﬁnition 7 (Language) A language L is a possibly inﬁnite set of strings\\nover a ﬁnite alphabet Σ.\\nWe deﬁne Σ∗ as the set of all possible strings over some alphabet Σ. Thus\\nL ⊆ Σ∗. The set of all possible languages over some alphabet Σ is the set of\\nall possible subsets of Σ∗, i.e. 2Σ∗ or ℘(Σ∗). This may seem rather simple,\\nbut is actually perfectly adequate for our purposes.\\nNotice that Σ∗ is inﬁnite, as there is no upper bound on the length of the\\nstrings that can be formed from Σ.\\n96\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n97\\n6.2\\nGrammars\\nA grammar is a way to characterize a language L, a way to list out which\\nstrings of Σ∗ are in L and which are not. If L is ﬁnite, we could simply list\\nthe strings, but languages by deﬁnition need not be ﬁnite. In fact, all of the\\nlanguages we are interested in are inﬁnite. This is, as we showed in chapter 2,\\nalso true of human language.\\nRelating the material of this chapter to that of the preceding two, we\\ncan view a grammar as a logical system by which we can prove things. For\\nexample, we can view the strings of a language as WFFs. If we can prove\\nsome string u with respect to some language L, then we would conclude that\\nu is in L, i.e. u ∈ L.\\nAnother way to view a grammar as a logical system is as a set of formal\\nstatements we can use to prove that some particular string u follows from\\nsome initial assumption. This, in fact, is precisely how we presented the\\nsyntax of sentential logic in chapter 4. For example, we can think of the\\nsymbol WFF as the initial assumption or symbol of any derivational tree of\\na well-formed formula of sentential logic. We then follow the rules for atomic\\nstatements (page 47) and WFFs (page 47).\\nOur notion of grammar will be more speciﬁc, of course. The grammar\\nincludes a set of rules from which we can derive strings. These rules are\\neﬀectively statements of logical equivalence of the form: ψ → ω, where ψ\\nand ω are strings.1\\nConsider again the WFFs of sentential logic. We know a formula like\\n(p∧q′) is well-formed because we can progress upward from atomic statements\\nto WFFs showing how each ﬁts the rules cited above.\\nFor example, we\\nknow that p is an atomic statement and q is an atomic statement. We also\\nknow that if q is an atomic statement, then so is q′. We also know that\\nany atomic statement is a WFF. Finally, we know that two WFFs can be\\nassembled together into a WFF with parentheses around the whole thing and\\na conjunction ∧ in the middle.\\nWe can also do this in the other direction, from WFF to atomic state-\\nments. We start with the assumption that we are dealing with a WFF. We\\n1These statements seem to “go” in only one direction, yet they are not bound by the\\nrestriction we saw in ﬁrst-order logic where a substitution based on logical consequence\\ncan only apply to an entire formula. It’s probably best to understand these statements\\nas more like biconditionals, rather than conditionals, even though the traditional symbol\\nhere is the same as for a logical conditional.\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n98\\nknow ﬁrst that a WFF can be composed of two separate WFFs surrounded\\nby parentheses with a ∧ in the middle. We also know that those individ-\\nual WFFs can be simple atomic statements and that one of those atomic\\nstatements can be p and the other q′.\\nThe direction of the proof is thus irrelevant here.2\\nWe can represent all these steps in the form ψ → ω if we add some\\nadditional symbols. Let’s adopt W for a WFF and A for an atomic statement.\\nIf we know that p and q can be atomic statements, then this is equivalent to\\nA → p and A → q. Likewise, we know that any atomic statement followed\\nby a prime is also an atomic statement: A → A′. We know that any atomic\\nstatement is a WFF: W → A. Last, we know that any two WFFs can be\\nconjoined: W → (W ∧ W).\\n(6.1)\\nA → p\\nA → q\\nA → A′\\nW → A\\nW → (W ∧ W)\\nEach of these rules is part of the grammar of the syntax of WFFs. If\\nevery part of a formula follows one of the rules of the grammar of the syntax\\nof WFFs, then we say that the formula is indeed a WFF.\\nReturning to the example (p ∧ q′), we can show that every part of the\\nformula follows one of these rules by constructing a tree.\\n(6.2)\\nW\\n(\\nW\\nA\\np\\n∧\\nW\\nA\\nA\\nq\\n′\\n)\\n2It is quite relevant, however, if we are concerned with ﬁnding proofs in a simple fashion.\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n99\\nEach branch corresponds to one of the rules we posited.\\nThe mother of\\neach branch corresponds to ψ and the daughters to ω. The elements at the\\nvery ends of branches—those without daughters—are referred to as terminal\\nelements, and the elements higher in the tree are all non-terminal elements.\\nIf all branches correspond to actual rules of the grammar and the top node\\nis a legal starting node, then the string given by the terminal elements is\\nsyntactically well-formed with respect to that grammar.\\nFormally, we deﬁne a grammar as {VT, VN, S, R}, where VT is the set of\\nterminal elements, VN is the set of non-terminals, S is a member of VN, and R\\nis a ﬁnite set of rules of the form above. The symbol S is deﬁned as the only\\nlegal ‘root’ non-terminal: the deisgnated ‘start’ symbol. As in the preceding\\nexample, we use capital letters for non-terminals and lowercase letters for\\nterminals.\\nDeﬁnition 8 (Grammar) {VT, VN, S, R}, where VT is the set of terminal\\nelements, VN is the set of non-terminals, S is a member of VN, and R is a\\nﬁnite set of rules.\\nLooking more closely at R, we will require that the left side of a rule\\ncontain at least one non-terminal element and any number of other elements.\\nWe ﬁrst deﬁne Σ as VT ∪VN, all of the terminals and non-terminals together.\\nR is then a ﬁnite set of ordered pairs from Σ∗VNΣ∗ × Σ∗. Thus ψ → ω is\\nequivalent to ⟨ψ, ω⟩.\\nDeﬁnition 9 (Rule) R is a ﬁnite set of ordered pairs from Σ∗VNΣ∗ × Σ∗,\\nwhere Σ = VT ∪ VN.\\nWe can now consider grammars of diﬀerent types.\\nThe simplest case\\nto consider ﬁrst, from this perspective, are context-free grammars, or Type\\n2 grammars. In such a grammar, all rules of R are of the form A → ψ,\\nwhere A is a single non-terminal element of VN and ψ is a string of terminals\\nfrom VT and non-terminals from VN.3 Such a rule says that a non-terminal\\nA can dominate the string ψ in a tree. These are the traditional phrase-\\nstructure taught in introductory linguistics courses. The set of languages\\nthat can be generated with such a system is fairly restricted and derivations\\nare straightforwardly represented with a syntactic tree. The partial grammar\\nwe exempliﬁed above for sentential logic was of this sort.\\n3Any ordering of terminals and non-terminals is valid on the right side of a context-free\\nrule.\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n100\\nA somewhat more powerful system can be had if we allow context-sensitive\\nrewrite rules, e.g. A → ψ/α\\nβ (where ψ cannot be ǫ). Such a rule says that\\nA can dominate ψ in a tree if ψ is preceded by α and followed by β. If\\nwe set trees aside, and just concentrate on string equivalences, then this is\\nequivalent to αAβ → αψβ. Context-sensitive grammars are also referred to\\nas Type 1 grammars.\\nIn the other direction from context-free grammars, that is toward less\\npowerful grammars, we have the regular or right-linear or Type 3 grammars.\\nSuch grammars only contain rules of the following form: A → xB or A → x.\\nThe non-terminal A can be rewritten as a single terminal element x or a\\nsingle non-terminal followed by a single terminal.\\n(6.3)\\n1\\ncontext-sensitive\\nA → ψ/α\\nβ\\n2\\ncontext-free\\nA → ψ\\n3\\nright-linear\\n�\\nA → x B\\nA → x\\n�\\nWe will see that these three types of grammars allow for successively\\nmore restrictive languages and can be paired with speciﬁc types of abstract\\nmodels of computers. We will also see that the formal properties of the most\\nrestrictive grammar types are quite well understood and that as we move up\\nthe hierarchy, the systems become less and less well understood, or, more\\nand more interesting.\\nLet’s look at a few examples. For all of these, assume the alphabet is\\nΣ = {a, b, c}.\\nHow might we deﬁne a grammar for the language that includes all strings\\ncomposed of one instance of b preceded by any number of instances of a:\\n{b, ab, aab, aaab, . . .}? We must ﬁrst decide what sort of grammar to write\\namong the three types we’ve discussed. In general, context-free grammars\\nare the easiest and most intuitive to write.\\nIn this case, we might have\\nsomething like this:\\n(6.4)\\nS → A b\\nA → ǫ\\nA → A a\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n101\\nThe ﬁrst rule takes the designated start symbol S and rewrites it as the\\nnon-terminal A followed by the terminal b. The next two rules provide two\\ndiﬀerent ways of rewriting A. The rule A → ǫ says that A can be null, i.e.\\nrewritten as ǫ. The last rule, A → A a says that A can be rewritten as itself\\nfollowed by an a. In conjunction with the preceding rule, this allows for zero\\nor more instances of a.\\nThis grammar is context-free because all rules have a single non-terminal\\non the left and a string of terminals and non-terminals on the right. This\\ngrammar cannot be right-linear because it includes rules where the right side\\nhas a non-terminal followed by a terminal, e.g. A → A b. This grammar\\ncannot be context-sensitive because it contains rules where the right side is\\nǫ, e.g. A → ǫ. For the strings b, ab, and aab, this produces the following\\ntrees.\\n(6.5)\\nS\\nA\\nǫ\\nb\\nS\\nA\\nA\\nǫ\\na\\nb\\nS\\nA\\nA\\nA\\nǫ\\na\\na\\nb\\nIn terms of our formal characterization of grammars in Deﬁnition 8 above,\\nwe have:\\n(6.6)\\nVT\\n=\\n{a, b}\\nVN\\n=\\n{S, A}\\nS\\n=\\nS\\nR\\n=\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\nS → A b\\nA → ǫ\\nA → A a\\n\\uf8fc\\n\\uf8f4\\n\\uf8fd\\n\\uf8f4\\n\\uf8fe\\nOther grammars are possible for this language too. For example:\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n102\\n(6.7)\\nS → b\\nS → A b\\nA → a\\nA → A a\\nThe ﬁrst two rules rewrite S as either b or A b. The third and fourth rules\\nprovide options for expanding the non-terminal A as either a single instance\\nof a or A a.\\nThis grammar is context-free, but also qualiﬁes as context-sensitive. We\\nno longer have ǫ on the right side of any rule and a single non-terminal on\\nthe left qualiﬁes as a string of terminals and non-terminals. This grammar\\nproduces the following trees for the same three strings.\\n(6.8)\\nS\\nb\\nS\\nA\\na\\nb\\nS\\nA\\nA\\na\\na\\nb\\nWe can also write a grammar that qualiﬁes as right-linear that will char-\\nacterize this language.\\n(6.9)\\nS → b\\nS → a S\\nThis produces trees as follows for our three examples.\\n(6.10)\\nS\\nb\\nS\\na\\nS\\nb\\nS\\na\\nS\\na\\nS\\nb\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n103\\nThere are, therefore, situations where the same language can be charac-\\nterized in terms of all three types of grammars. This will not always be the\\ncase.\\nLet’s consider a somewhat harder case: a language where strings begin\\nwith an a, end with a b, with any number of intervening instances of c, e.g.\\n{ab, acb, accb, . . .}. This can also be described using all three grammar types.\\nFirst, a context-free grammar:\\n(6.11)\\nS → a C b\\nC → C c\\nC → ǫ\\nThis grammar is neither right-linear nor context-sensitive. It produces trees\\nlike these:\\n(6.12)\\nS\\na\\nC\\nǫ\\nb\\nS\\na\\nC\\nC\\nǫ\\nc\\nb\\nS\\na\\nC\\nC\\nC\\nǫ\\nc\\nc\\nb\\nHere is a right-linear grammar that generates the same strings:\\n(6.13)\\nS → a C\\nC → c C\\nC → b\\nThis produces trees as follows for the same three examples:\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n104\\n(6.14)\\nS\\na\\nC\\nb\\nS\\na\\nC\\nc\\nC\\nb\\nS\\na\\nC\\nc\\nC\\nc\\nC\\nb\\nWe can also write a grammar that is context-sensitive (and context-free)\\nthat produces this language.\\n(6.15)\\nS → a b\\nS → a C b\\nC → C c\\nC → c\\nThis results in the following trees.\\n(6.16)\\nS\\na\\nb\\nS\\na\\nC\\nc\\nb\\nS\\na\\nC\\nC\\nc\\nc\\nb\\nWe will see that the set of languages that can be described by the three\\ntypes of grammar are not the same. Right-linear grammars can only accom-\\nmodate a subset of the languages that can be treated with context-free and\\ncontext-sensitive grammars. If we set aside the null string ǫ, context-free\\ngrammars can only handle a subset of the languages that context-sensitive\\ngrammars can treat.\\nIn the following sections, we more closely examine the properties of the\\nsets of languages each grammar formalism can accommodate and the set of\\nabstract machines that correspond to each type.\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n105\\n6.3\\nFinite State Automata\\nIn this section, we treat ﬁnite state automata. We consider two types of\\nﬁnite state automata: deterministic and non-deterministic. We deﬁne each\\nformally and then show their equivalence.\\nWhat is a ﬁnite automaton? In intuitive terms, it is a very simple model\\nof a computer. The machine reads an input tape which bears a string of\\nsymbols. The machine can be in any number of states and, as each symbol is\\nread, the machine switches from state to state based on what symbol is read\\nat each point. If the machine ends up in one of a set of particular states,\\nthen the string of symbols is said to be accepted. If it ends up in any other\\nstate, then the string is not accepted.\\nWhat is a ﬁnite automaton more formally? Let’s start with a determin-\\nistic ﬁnite automaton (DFA). A DFA is a machine composed of a ﬁnite set\\nof states linked by arcs labeled with symbols from a ﬁnite alphabet. Each\\ntime a symbol is read, the machine changes state, the new state uniquely\\ndetermined by the symbol read and the labeled arcs from the current state.\\nFor example, imagine we have an automaton with the structure in ﬁgure 6.17\\nbelow.\\n(6.17)\\nq0\\nq1\\nb\\nb\\na\\na\\nThere are two states q0 and q1. The ﬁrst state, q0, is the designated start\\nstate and the second state, q1, is a designated ﬁnal state. This is indicated\\nwith a dark circle for the start state and a double circle for any ﬁnal state.\\nThe alphabet Σ is deﬁned for this DFA as {a, b}.\\nThis automaton describes the language where all strings contain an odd\\nnumber of instances of the symbol b, for it is only with an input string that\\nsatisﬁes that restriction that the automaton will end up in state q1.\\nFor\\nexample, let’s go through what happens when the machine reads the string\\nbab. It starts in state q0 and reads the ﬁrst symbol b. It then follows the arc\\nlabeled b to state q1. It then reads the symbol a and follows the arc from q1\\nback to q1. Finally, it reads the last symbol b and follows the arc back to q0.\\nSince q0 is not a designated ﬁnal state, the string is not accepted.\\nConsider now a string abbb. The machine starts in state q0 and reads the\\nsymbol a. It then follows the arc back to q0. It reads the ﬁrst b and follows\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n106\\nthe arc to q1. It reads the second b and follows the arc labeled b back to q0.\\nFinally, it reads the last b and follows the arc from q0 back to q1. Since q1 is\\na designated ﬁnal state, the string is accepted.\\nWe can deﬁne a DFA more formally as follows:\\nDeﬁnition 10 (DFA) A deterministic ﬁnite automaton (DFA) is a quintu-\\nple ⟨K, Σ, q0, F, δ⟩, where K is a ﬁnite number of states, Σ is a ﬁnite alphabet,\\nq0 ∈ K is a single designated start state, and δ is a function from K × Σ to\\nK.\\nFor example, in the DFA in ﬁgure 6.17, K is {q0, q1}, Σ is {a, b}, q0 is the\\ndesignated start state and F = {q1}. The function δ has the following domain\\nand range:\\n(6.18)\\ndomain\\nrange\\nq0, a\\nq0\\nq0, b\\nq1\\nq1, a\\nq1\\nq1, b\\nq0\\nThus, the function δ can be represented either graphically as arcs, as in\\n(6.17), or textually as a table, as in (6.18).4\\nThe situation of a ﬁnite automaton is a triple: (x, q, y), where x is the\\nportion of the input string that the machine has already “consumed”, q is\\nthe current state, and y is the part of the string on the tape yet to be read.\\nWe can think of the progress of the tape as a sequence of situations licensed\\nby δ. Consider what happens when we feed abab to the DFA in ﬁgure 6.17.\\nWe start with (ǫ, q0, abab) and then go to (a, q0, bab), then to (ab, q1, ab), etc.\\nThe steps of the derivation are encoded with the turnstile symbol ⊢. The\\nentire derivation is given below:\\n(6.19) (ǫ, q0, abab) ⊢ (a, q0, bab) ⊢ (ab, q1, ab) ⊢ (aba, q1, b) ⊢ (abab, q0, ǫ)\\n4Some treatments distinguish deterministic automata from complete automata. A de-\\nterministic automaton has no more than one arc from any state labeled with any particular\\nsymbol. A complete automaton has at least one arc from every state for every symbol.\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n107\\nSince the DFA does not end up in a state of F (q0 ̸∈ F), this string is not\\naccepted.\\nLet’s deﬁne the turnstile more formally as follows:\\nDeﬁnition 11 (produces in one move) Assume a DFA M, where M =\\n⟨K, Σ, δ, q0, F⟩. A situation (x, q, y) produces situation (x′, q′, y′) in one move\\niﬀ: 1) there is a symbol σ ∈ Σ such that y = σy′ and x′ = xσ (i.e., the\\nmachine reads one symbol), and 2) δ(q, σ) = q′ (i.e., the appropriate state\\nchange occurs on reading σ).\\nThe basic idea is that x is converted to x′ by adding σ, y is converted to y′\\nby removing σ, and there is an arc from q to q′ labeled with σ.\\nWe can use this to deﬁne a more general notion “produces in zero or\\nmore steps”: ⊢∗. We say that S1 ⊢∗ Sn if there is a sequence of situations\\nS1 ⊢ S2 ⊢ . . . ⊢ Sn−1 ⊢ Sn. Thus the derivation in (6.19) is equivalent to the\\nfollowing.\\n(6.20) (ǫ, q0, abab) ⊢∗ (abab, q0, ǫ)\\nLet’s now consider non-deterministic ﬁnite automata (NFAs). These are\\njust like DFAs except i) arcs can be labeled with the null string ǫ, and ii)\\nthere can be multiple arcs with the same label from the same state; thus δ\\nis a relation, not a function, in a NFA.\\nDeﬁnition 12 (NFA) A non-deterministic ﬁnite automaton M is a quin-\\ntuple ⟨K, Σ, ∆, q0, F⟩, where K, Σ, q0, and F are as for a DFA, and ∆, the\\ntransition relation, is a ﬁnite subset of K × (Σ ∪ ǫ) × K.\\nLet’s look at an example.\\nThe NFA in (6.21) generates the language\\nwhere any instance of the symbol a must have at least one b on either side\\nof it; the string must also begin with at least one instance of b.\\n(6.21)\\nq0\\nq1\\nb\\na\\nb\\nb\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n108\\nHere, q0 is the designated start state and q1 is in F. We can see that there\\nare two arcs from q0 on b, but none on a; this automaton is thus necessarily\\nnon-deterministic.\\nThe transition relation ∆ can be represented in tabular form as well.\\nHere, we list all the mappings for every combination of K × Σ.\\n(6.22)\\ndomain\\nrange\\nq0, a\\n∅\\nq0, b\\n{q0, q1}\\nq1, a\\n{q0}\\nq1, b\\n{q0}\\nNotice how K × Σ maps to a (possibly empty) set of states.\\nGiven that there are multiple paths through an NFA for any particular\\nstring, how do we assess whether a string is accepted by the automaton? To\\nsee if some string is accepted by a NFA, we must determine if there is at\\nleast one path through the automaton that terminates in a state of F.\\nConsider the automaton above and the string bab. There are several paths\\nto consider.\\n(6.23)\\na.\\n(ǫ, q0, bab) ⊢ (b, q0, ab) ⊢?\\nb.\\n(ǫ, q0, bab) ⊢ (b, q1, ab) ⊢ (ba, q0, b) ⊢ (bab, q0, ǫ)\\nc.\\n(ǫ, q0, bab) ⊢ (b, q1, ab) ⊢ (ba, q0, b) ⊢ (bab, q1, ǫ)\\nThe ﬁrst, (6.23a), doesn’t terminate. The second terminates, but only\\nin a non-ﬁnal state. The third, (6.23c), terminates in a ﬁnal state. Hence,\\nsince there is at least one path that terminates in a ﬁnal state, the string is\\naccepted.\\nIt’s a little trickier when the NFA contains arcs labeled with ǫ.\\nFor\\nexample:\\n(6.24)\\nq0\\nq1\\na\\nǫ\\nb\\na\\na\\nb\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n109\\nHere we have the usual sort of non-determinism with two arcs labeled with\\na from q0. We also have an arc labeled ǫ from q1 to q0. This latter sort of\\narc can be followed at any time without consuming a symbol. Let’s consider\\nhow a string like aba might be parsed by this machine. The following chart\\nshows all possible paths.\\n(6.25)\\na.\\n(ǫ, q0, aba) ⊢ (a, q0, ba) ⊢ (ab, q0, a) ⊢ (aba, q1, ǫ)\\nb.\\n(ǫ, q0, aba) ⊢ (a, q1, ba) ⊢ (ab, q1, a) ⊢ (aba, q1, ǫ)\\nc.\\n(ǫ, q0, aba) ⊢ (a, q1, ba) ⊢ (a, q0, ba) ⊢ (ab, q0, a) ⊢ (aba, q0, ǫ)\\nd.\\n(ǫ, q0, aba) ⊢ (a, q1, ba) ⊢ (ab, q1, a) ⊢ (ab, q0, a) ⊢ (aba, q0, ǫ)\\ne.\\n(ǫ, q0, aba) ⊢ (a, q0, ba) ⊢ (ab, q0, a) ⊢ (aba, q1, ǫ) ⊢ (aba, q0, ǫ)\\nf.\\n(ǫ, q0, aba) ⊢ (a, q1, ba) ⊢ (ab, q1, a) ⊢ (aba, q1, ǫ) ⊢ (aba, q0, ǫ)\\ng.\\n(ǫ, q0, aba) ⊢ (a, q1, ba) ⊢ (a, q0, ba) ⊢ (ab, q0, a) ⊢ (aba, q1, ǫ)\\nh.\\n(ǫ, q0, aba) ⊢ (a, q1, ba) ⊢ (a, q0, ba) ⊢ (ab, q0, a) ⊢\\n(aba, q1, ǫ) ⊢ (aba, q0, ǫ)\\ni.\\n(ǫ, q0, aba) ⊢ (a, q1, ba) ⊢ (ab, q1, a) ⊢ (ab, q0, a) ⊢ (aba, q1, ǫ)\\nj.\\n(ǫ, q0, aba) ⊢ (a, q1, ba) ⊢ (ab, q1, a) ⊢ (ab, q0, a) ⊢\\n(aba, q1, ǫ) ⊢ (aba, q0, ǫ)\\nThe ǫ-arc can be followed whenever the machine is in state q1. It is indicated\\nin the chart above by a move from q1 to q0 without a symbol being read.\\nNote that it results in an explosion in the number of possible paths. In this\\ncase, since at least one path ends up in the designated ﬁnal state q1, the\\nstring is accepted.\\nNotice that it’s a potentially very scary proposition to determine if some\\nNFA generates some string x. Given that there are ǫ-arcs, which can be\\nfollowed at any time without reading a symbol, there can be an inﬁnite\\nnumber of paths for any ﬁnite string.5 Fortunately, this is not a problem,\\nbecause DFAs and NFAs generate the same class of languages.\\nTheorem 1 DFAs and NFAs produce the same languages.\\nLet’s show this. DFAs are obviously a subcase of NFAs; hence any language\\ngenerated by a DFA is trivially generated by an NFA.\\n5This can arise if we have cycles involving ǫ.\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n110\\nProving this in the other direction is a little trickier. What we will do is\\nshow how a DFA can be constructed from any NFA (Hopcroft and Ullman,\\n1979). Recall that the arcs of an NFA can be represented as a map from\\nK × (Σ ∪ ǫ) to all possible subsets of K. What we do to construct the DFA\\nis to use these sets of states as literal labels for new states.\\nFor example, in the NFA in (6.21), call it M, we have ∆ as in (6.22). The\\npossible sets of states are: ∅, {q0}, {q1}, and {q0, q1}.6 The new DFA M ′ will\\nthen have state labels: [∅], [q0], [q1], and [q0, q1], replacing the curly braces\\nthat denote sets with square braces which we will use to denote state labels.\\nFor the new DFA, we deﬁne δ′ as follows:\\nδ′([q1, q2, . . . , qn], a) = [p1, p2, . . . , pn]\\nif and only if, in the original NFA:\\n∆({q1, q2, . . . , qn}, a) = {p1, p2, . . . , pn}\\nThe latter means that we apply ∆ to every state in the ﬁrst list of states and\\nunion together the resulting states.\\nApplying this to the NFA in (6.21), we get this chart for the new DFA.\\n(6.26)\\nδ([∅], a)\\n=\\n[∅]\\nδ([∅], b)\\n=\\n[∅]\\nδ([q0], a)\\n=\\n[∅]\\nδ([q0], b)\\n=\\n[q0, q1]\\nδ([q1], a)\\n=\\n[q0]\\nδ([q1], b)\\n=\\n[q0]\\nδ([q0, q1], a)\\n=\\n[q0]\\nδ([q0, q1], b)\\n=\\n[q0, q1]\\nThe initial start state was q0, so the new start state is [q0]. Any set containing\\na possible ﬁnal state from the initial automaton is a ﬁnal state in the new\\nautomaton: [q1] and [q0, q1]. The new automaton is given below.\\n6Recall that there will be 2K of these if there are K states.\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n111\\n(6.27)\\n[∅]\\n[q1]\\n[q0]\\n[q0, q1]\\na\\nb\\na\\nb\\na\\nb\\na\\nb\\nThis automaton accepts exactly the same language as the previous one.\\nIf we can always construct a DFA from an NFA that accepts exactly the\\nsame language, it follows that there is no language accepted by an NFA that\\ncannot be accepted by a DFA. 2\\nNotice two things about the resulting DFA in (6.27). First, there is a\\nstate that cannot be reached: [q1]. Such states can safely be pruned. The\\nfollowing automaton is equivalent to (6.27).\\n(6.28)\\n[∅]\\n[q0]\\n[q0, q1]\\na\\nb\\na\\nb\\na\\nb\\nSecond, notice that the derived DFA can, in principle, be massively bigger\\nthan the original NFA. In the worst case, if the original NFA has n states,\\nthe new automaton can have as many as 2n states.7\\nIn the following, since NFAs and DFAs are equivalent, I will refer to the\\ngeneral class as Finite State Automata (FSAs).\\n7There are algorithms for minimizing the number of states in a DFA, but they are\\nbeyond the scope of this introduction. See Hopcroft and Ullman (1979). Even minimized,\\nit is generally true that an NFA will be smaller than its equivalent DFA.\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n112\\n6.4\\nRegular Languages\\nWe now consider the class of regular languages. We’ll show that these are\\nprecisely those that can be accepted by an FSA and which can be generated\\nby a right-linear grammar.\\nThe regular languages are deﬁned as follows.\\nDeﬁnition 13 (Regular Language) Given a ﬁnite alphabet Σ:\\n1. ∅ is a regular language.\\n2. For any string x ∈ Σ∗, {x} is a regular language.\\n3. If A and B are regular languages, then so is A ∪ B.\\n4. If A and B are regular languages, then so is AB.\\n5. If A is a regular language, then so is A∗.\\n6. Nothing else is a regular language.\\nConsider each of these operations in turn. First, we have that any string\\nof symbols from the alphabet can be a speciﬁcation of a language. Thus, if\\nthe alphabet is Σ = {a, b, c}, then the regular language L can be {a}.8\\nIf L1 = {a} and L2 = {b}, then we can deﬁne the regular language which\\nis the union of L1 and L2: L3 = L1 ∪ L2, i.e. L3 = {a, b}. In string terms,\\nthis is usually written L3 = (a|b).\\nWe can also concatenate two regular languages, e.g.\\nL3 = L1L2, e.g.\\nL3 = {ab}.\\nFinally, we have Kleene star, which allows us to repeat some regular\\nlanguage zero or more times. Thus, if L1 is a regular language, then L2 = L∗\\n1\\nis a regular language, e.g. L2 = {a, aa, aaa, . . .}. In string terms: L2 = a∗.\\nThese operations can, of course, be applied recursively in any order.9 For\\nexample, a(b∗|c)a∗ refers to the language where all strings are composed of\\na single instance of a followed by any number of instances of b or a single\\nc, followed in turn by any number of instances of a. We can construct this\\nlanguage stepwise from the deﬁnition above.\\n8Notice that it would work just as well to start form any symbol a ∈ Σ here, since we\\nhave recursive concatenation in #4.\\n9In complex examples, we can use parentheses to indicate scope.\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n113\\n(6.29)\\na\\nclause #2\\nb\\nclause #2\\nc\\nclause #2\\nb∗\\nclause #5\\n(b∗|c)\\nclause #3\\na(b∗|c)\\nclause #4\\na∗\\nclause #5\\na(b∗|c)a∗\\nclause #4\\nWe can go in the other direction as well. For example, how might we\\ndescribe the language where all strings contain an even number of instances\\nof a plus any number of the other symbols: ((b|c)∗a(b|c)∗a(b|c)∗)∗.\\n6.5\\nAutomata and Regular Languages\\nIt turns out that the set of languages that can be accepted by a ﬁnite state\\nautomaton is exactly the regular languages.\\nTheorem 2 A set of strings is a ﬁnite automaton language if and only if it\\nis a regular language.\\nWe won’t prove this rigorously, but we can see the logic of the proof fairly\\nstraightforwardly.\\nThere are really only four things that we can do with\\na ﬁnite automaton, and each of these four corresponds to one of the basic\\nclauses of the deﬁnition of a regular language.\\nFirst, we have that a single symbol is a legal regular language because we\\ncan have a ﬁnite automaton with a start state, a single arc, and a ﬁnal state.\\n(6.30)\\nq0\\nq1\\na\\nSecond, we have concatenation of two regular languages by taking two\\nautomata and connecting them with an arc labeled with ǫ.\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n114\\n(6.31)\\nFSA�1\\nFSA�2\\nǫ\\n. . .\\n. . .\\nWe connect all ﬁnal states of the ﬁrst automaton with the start state of the\\nsecond automaton with ǫ-arcs. The ﬁnal states of the ﬁrst automaton are\\nmade non-ﬁnal. The start state of the second automaton is made a non-start\\nstate.\\nUnion is straightforward as well.\\nWe simply create a new start state\\nand then create arcs from that state to the former start states of the two\\nautomata labeled with ǫ. We create a new ﬁnal state as well, with ǫ-arcs\\nfrom the former ﬁnal states of the two automata.\\n(6.32)\\nq0\\nFSA�2\\nq1\\nFSA�1\\nǫ\\nǫ\\nǫ\\nǫ\\nFinally, we can get Kleene star by creating a new start state (which is\\nalso a ﬁnal state), a new ﬁnal state, and an ǫ-loop between them.\\n(6.33)\\nq0\\nFSA�1\\nq1\\nǫ\\nǫ\\nǫ\\nIf we can construct an automaton for every step in the construction of a\\nregular language, it should follow that any regular language can be accepted\\nby some automaton.10\\n10A rigorous proof would require that we go through this in the other direction as well,\\nfrom automaton to regular language.\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n115\\n6.6\\nRight-linear Grammars and Automata\\nAnother equivalence that is of use is that between the regular languages and\\nright-linear grammars. Right-linear grammars generate precisely the set of\\nregular languages.\\nWe can show this by pairing the rules of a right-linear grammar with the\\narcs of an automaton. First, for every rewrite rule of the form A → x B, we\\nhave an arc from state A to state B labeled x. For every rewrite rule of the\\nform A → x, we have an arc from state A to the designated ﬁnal state, call\\nit F.\\nConsider this very simple example of a right-linear grammar.\\n(6.34)\\na.\\nS\\n→\\na A\\nb.\\nA\\n→\\na A\\nc.\\nA\\n→\\na B\\nd.\\nB\\n→\\nb\\nThis generates the language where all strings are composed of two or more\\ninstances of a, followed by exactly one b.\\nIf we follow the construction of the FSA above, we get this:\\n(6.35)\\nS\\nA\\nB\\nF\\na\\na\\nb\\na\\nThis FSA accepts the same language generated by the right-linear grammar\\nin (6.34).\\nNotice now that if FSAs and right-linear grammars generate the same\\nset of languages and FSAs generate regular languages, then it follows that\\nright-linear grammars generate regular languages. Thus we have a three-way\\nequivalence between regular languages, right-linear grammars, and FSAs.\\n6.7\\nClosure Properties of Regular Languages\\nLet’s now turn to closure properties of the regular languages.\\nA closure\\nproperty is one that when applied to an element of a set produces an element\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n116\\nof the same set. In the present context, a closure property for the regular\\nlanguages is one that, when applied to a regular language, produces a regular\\nlanguage.\\nBy the deﬁnition of regular language, it follows that they are closed under\\nthe properties that deﬁne them: concatenation, union, Kleene star. They are\\nalso closed under complement. The complement of some regular language L\\ndeﬁned over the alphabet Σ is L′ = Σ∗ − L, i.e. all strings over the same\\nalphabet not in L.\\nIt’s rather easy to show this using DFAs. In particular, to construct the\\ncomplement of some language L, we create the DFA that generates that\\nlanguage and then swap the ﬁnal and non-ﬁnal states.\\nLet’s consider the DFA in (6.17) on page 105 above. This generates the\\nlanguage a∗ba∗(ba∗ba∗)∗, where every legal string contains an odd number of\\ninstances of the symbol b, and any number of instances of the symbol a. We\\nnow reverse the ﬁnal and non-ﬁnal states so that q0 is both the start state\\nand the ﬁnal state.\\n(6.36)\\nq0\\nq1\\nb\\nb\\na\\na\\nThis now generates the complement language: a∗(ba∗ba∗)∗. Every legal string\\nhas an even number of instances of b (including zero), and any number of\\ninstances of a.\\nWith complement so deﬁned, and DeMorgan’s Law (the set-theoretic ver-\\nsion), it follows that the regular languages are closed under intersection as\\nwell. Recall the following equivalences from chapter 3.\\n(6.37)\\n(X ∪ Y )′ = X′ ∩ Y ′\\n(X ∩ Y )′ = X′ ∪ Y ′\\nTherefore, since the regular languages are closed under union and under\\ncomplement, it follows that they are closed under intersection. Thus if we\\nwant to intersect the languages L1 and L2, we union their complements, i.e.\\nL1 ∩ L2 = (L′\\n1 ∪ L′\\n2)′.\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n117\\n6.8\\nPumping Lemma for Regular Languages\\nThe Pumping Lemma for regular languages is quite daunting in its full formal\\nglory, but it is a powerful tool for classifying languages and actually quite\\nintuitive. In this section, we build up to, explain, and exemplify it.\\nRecall that the regular languages are deﬁned by three operations: con-\\ncatenation, union, and Kleene star. The last is actually rather diﬀerent from\\nthe ﬁrst two: it is only with Kleene star that a regular language can be\\ninﬁnite.\\nConcatenation cannot produce an inﬁnitely large language from ﬁnite\\nlanguages. For example, concatenating the language L1 = {a, b} with the\\nlanguage L2 = {b, c} produces a larger but still ﬁnite language where L3 =\\n{ab, ac, bb, bc}. Here the size of the concatenated language is equal to the\\nproduct of the sizes of the concatenated languages: |L3| = |L1| × |L2|. Thus,\\nif L1 and L2 are ﬁnite, L3 must be ﬁnite.\\nLikewise, union can only produce a ﬁnite language from ﬁnite languages.\\nFor example, the union of L1 = {a} and L2 = {b} gives: L3 = {a, b}, a larger\\nlanguage, but still ﬁnite. Here, the size of the unioned language is no bigger\\nthan the sum of the sizes of the individual languages: |L3| ≤ |L1| + |L2|.\\nThus, if L1 and L2 are ﬁnite, L3 must be ﬁnite.\\nKleene star, on the other hand, produces an inﬁnitely large language\\nfrom any language, whether it is already inﬁnite or not. For example, if\\nL1 = {a}, a ﬁnite language, then applying Kleene star to it produces the\\ninﬁnite language L2 = {a, aa, aaa, . . .}.\\nTherefore it follows that if a language is inﬁnite and regular, then its\\ncharacterization must include Kleene star. Since Kleene star allows a sub-\\nstring (or sublanguage) to recur an inﬁnite number of times, it follows that\\nin an inﬁnite regular language there will be at least one substring that can\\nbe repeated an inﬁnite number of times. This is what the Pumping Lemma\\nsays:\\nTheorem 3 If L is a regular language, then there is a constant n such that\\nif z is any word in L, and |L| > n, then we can write z = uvw such that\\n|uw| ≤ n, |v| ≥ 0, and for all i ≥ 0, uviw is in L. In addition, n is no\\ngreater than the number of states in the smallest FSA that accepts L.11\\n11The language of the text has it that this is a lemma, not a theorem, but the distinc-\\ntion is not important. We therefore give it as a theorem, but continue the traditional\\ncharacterization as a lemma.\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n118\\nThis says that if a language is regular and we look at words whose length\\nexceeds a speciﬁc length, then those words will always have a substring that\\ncan be repeated any number of times still producing a word of the language.\\nConsider for example, the language L = {ac, abc, abbc, abbbc, abbbbc, . . .},\\nor ab∗c. Here, once we consider strings of at least three letters, there is a\\nrepeatable substring. Thus, if n = 3 and z = abc, then u = a, v = b, and\\nw = c. Any number of repetitions of b results in a legal word.\\nNotice that if n were set to 2, we would not always ﬁnd a repeating\\nsubstring. Thus, in ac there is no v can that be repeated.\\nNotice too that the repetitions do not necessarily enumerate the entire\\nlanguage. For example, n = 3 for the language a(b|c) ∗ d. Thus, in the string\\nabd, u = a, v = b, and w = d. The substring b can be repeated any number\\nof times. However, the language also allows any number of repetitions of c\\nin the same position.\\nSince Kleene star can be used more than once in the deﬁnition of a lan-\\nguage, there may be more than one point in a string that can qualify for v.\\nConsider the language ab∗c∗d. Here, n = 3 as well, thus in a string abd, b\\ncan be repeated and in a string acd, c can be repeated. However, in a string\\nabcd, either b or c can be repeated.\\nFinally, the repeated string can of course be longer than a single letter.\\nIn the language a(bc)∗d, n = 4. In a string abcd, u = a, v = bc, and w = d.\\nWhat’s important about the Pumping Lemma is that it can be used to\\nshow that some language is not regular. Consider for example, the language\\nan, where n is prime, i.e. {a, aa, aaa, aaaaa, aaaaaaa, aaaaaaaaaaa, . . .}. For\\nany n we choose, there is no way to parse the string as uvw.\\nImagine n = 1 and we start with a. Then u = ǫ, v = a, and w = ǫ. This\\nwon’t work because uv4w = aaaa, which is not in L.\\nImagine n = 2 and we start with aa. Then u = a, v = a, and w = ǫ.\\nThis won’t work either because uv3w = aaaa, which is not in L.\\nIn fact, with suﬃcient mathematical expertise, we can show that there is\\nno ﬁnite n that will work. Thus an, where n is prime, is not regular.\\nHere’s another example: the language anbn, i.e. {ǫ, ab, aabb, aaabbb, . . .}.\\nOnce again, it is impossible to ﬁnd a workable value for n.\\nImagine n = 1 and we start with ab. The key question is what qualiﬁes\\nas v? There are only three possibilities:\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n119\\n(6.38)\\nu\\nv\\nw\\n1\\nǫ\\na\\nb\\n2\\na\\nb\\nǫ\\n3\\nǫ\\nab\\nǫ\\nEach of these parses leads to incorrect predictions. On the ﬁrst choice, we\\ncannot set i to 2, as uv2w = aab is not in the language. On the second\\nchoice, we also cannot set i to 2, as uv2w = abb is not in the language.\\nFinally, the third choice also does not allow i as 2, since uv2w = abab is not\\nin the language.\\nThe problem persists for larger values of n; there is no way to ﬁnd a ﬁnite\\nvalue of n that works, given the way the language is deﬁned. Hence anbn is\\nnot regular.\\nFinally, notice from the deﬁnition that the size of n is a function of the size\\nof the automaton that accepts the language. Speciﬁcally, n is no bigger than\\nthe smallest FSA that will accept the language. We leave the explanation of\\nthis as an exercise.\\n6.9\\nRelevance of Regular Languages\\nDo the regular languages and FSAs have any interest for linguists? Yes. It’s\\nbeen claimed that phonology can be treated as a ﬁnite-state system.12 It’s\\nalso been claimed that morphology is ﬁnite state.13 We brieﬂy discuss each\\nof these in this section.\\nIt’s been argued that phonological generalizations can be characterized\\nin terms of FSAs (Karttunen, 1983; Koskenniemi, 1984). There are actually\\ntwo main approaches: static phonological generalizations (Bird and Ellison,\\n1994; Bird, 1995) and phonological relationships (Kaplan and Kay, 1994).\\nWe discuss the former here and return to the latter in the next chapter.\\nConsider a very simple generalization with respect to Syllable Structure,\\ne.g. all syllables in Hawaiian are composed of a single consonant followed by\\n12Syllable structure can be expressed as a context-free grammar. Is syllable structure\\nnecessarily context-free?\\n13If morphology is ﬁnite state, then attempts to treat morphological structure using the\\nfull power of syntactic theory would appear to be bringing too much computational power\\nto the task.\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n120\\na single vowel. This is very easy to state as a regular expression. Let us say,\\nfor simplicity, that the consonants of Hawaiian are {p, t, k, r, n} and the\\nvowels are {a, i, u}.14 This means that Hawaiian has possible words of the\\nform: CVCVCV. . . . We can express this restriction as a regular expression.\\n(6.39) (p|t|k|r|n)(a|i|u)((p|t|k|r|n)(a|i|u))∗\\nAll words must be composed of a sequence of any consonant followed by any\\nvowel followed by any number of repetitions of the same sequence. Notice\\nhow the regular expression includes the stipulation that all words are at least\\none syllable long.\\nAnother fairly common kind of phonological generalization is Vowel Har-\\nmony, e.g. in Hungarian, Finnish, or Turkish. In vowel harmony systems all\\nthe vowels of a language are partitioned into some number of groups and the\\nwords of the language are restricted so that all the vowels of any one word\\nmust be drawn exclusively from only one group. For example, imagine the\\nvowels of Turkish are {i, a, e, u, ¨u, ¨o }.15 Let’s assume that the consonants\\nare the same as in Hawaiian and that there are two groups of vowels: {i,\\na, e} and {u, ¨u, ¨o}. We can then express the vowel harmony restriction as\\nfollows. Some words are of this form:\\n(6.40) “Type 1” = (p|t|k|r|n)∗(i|a|e)(p|t|k|r|n)∗((i|a|e)(p|t|k|r|n)∗)∗\\nOther words are of this form:\\n(6.41) “Type 2” = (p|t|k|r|n)∗(u|¨u|¨o)(p|t|k|r|n)∗((u|¨u|¨o)(p|t|k|r|n)∗)∗\\nAll words then are of this sort:\\n(6.42) (“Type 1”|“Type 2”)\\n14Hawaiian actually has more consonants and vowels than this, but we’re simplifying\\nthe inventory to make the general point.\\n15Again, Turkish has many more vowels than this and the vowel harmony system of\\nTurkish is more complex. The vowels marked with dieresis (two dots) are vowels produced\\nin the front of the mouth with lip rounding. The vowel [¨u] corresponds to [i] with rounding\\nand the vowel [¨o] corresponds to [e] with rounding.\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n121\\nThe basic idea is that each word type is deﬁned as being composed of\\ninstances of vowels from the relevant group (at least one vowel) with any\\nnumber of interspersed consonants. The words of the language follow one or\\nthe other pattern.\\nMorphological generalizations have also been proposed to be instances of\\nregular grammars. For example, the set of possible verbal inﬂectional aﬃxes\\nin English might be cast as a regular expression:\\n(6.43) VERB(-s|-ed|-ing|ǫ)\\nThe system rapidly gets more complex, however. For example, recall the\\npattern of forming words in English with -ize, -ation, and -al discussed on\\npage 25 in Chapter 2. This is not so obviously regular.\\n6.10\\nSummary\\nThis chapter began with a formal deﬁnition of a language as a set of strings\\nover a ﬁnite alphabet (or vocabulary). There are ﬁnite and inﬁnite languages\\nand we focus on inﬁnite languages.\\nSuch a language can be described with a grammar, where a grammar is\\ndeﬁned as a ﬁnite set of symbols and a ﬁnite set of rules mapping strings of\\nsymbols to other strings of symbols. We considered three general categories\\nof grammar: context-sensitive, context-free, and right-linear. These form a\\nhierarchy and are successively more restrictive, with right-linear being the\\nmost restrictive.\\nWe next turned to automata, abstract models of computers. A ﬁnite state\\nautomaton has a ﬁnite number of states and a ﬁnite number of arcs between\\nstates labeled with the symbols of an alphabet. FSAs come in two ﬂavors:\\ndeterministic and non-deterministic. We showed how these are equivalent in\\nterms of the kinds of languages they can describe.\\nA ﬁnite state automaton can describe a particular class of languages: the\\nregular languages. These are the set of languages that can be formalized\\nin terms of only union, concatenation, or Kleene star. We also showed how\\nregular languages can be described in terms of right-linear grammars.\\nLastly, we considered other closure properties of the regular languages:\\ncomplement and intersection and went over how the Pumping Lemma could\\nbe used to prove that some language is not regular.\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n122\\nThe chapter closed with a discussion of the relevance of regular languages\\nto morphology and phonology.\\n6.11\\nExercises\\n1. Write a right-linear grammar that generates the language where strings\\nmust have exactly one instance of a and any number of instances of the\\nother symbols to either side (Σ = {a, b, c}).\\n2. Write a right-linear grammar where strings must contain an a and a\\nb in just that order, plus any number of other symbols (Σ = {a, b, c})\\nbefore, after, and in between.\\n3. Write a context-free grammar where legal strings are composed of some\\nnumber of instances of a, followed by a c, followed by exactly the same\\nnumber of instances of b as there were of a, followed by another c.\\n4. Write a context-sensitive grammar where legal strings are composed of\\nsome number of instances of a, followed by exactly the same number of\\ninstances of b as there were of a, followed by exactly the same number\\nof instances of c.\\n5. Write a DFA that generates the language where strings must have\\nexactly one instance of a and any number of instances of the other\\nsymbols (Σ = {a, b, c}).\\n6. Write a DFA where strings must contain an a, a b, and a c in just that\\norder, plus any number of other symbols (Σ = {a, b, c}).\\n7. Write a DFA where anywhere a occurs, it must be immediately followed\\nby a b, and any number of instances of c my occur around those bits.\\n8. Describe this language in words: b(a∗|c∗)c\\n9. Describe this language in words: b(a|c)∗c\\n10. Describe this language in words: (a|b|c)∗a(a|b|c)∗a(a|b|c)∗\\n11. Formalize this language as a regular language: all strings contain pre-\\ncisely three symbols (Σ = {a, b, c}).\\nCHAPTER 6. FORMAL LANGUAGE THEORY\\n123\\n12. Explain why wwR cannot be regular.\\n13. Is the following a regular language? All strings contain more instances\\nof a than of b, in any order, with no instances of c. If it is, give a\\nregular expression or FSA; if it is not, explain why.\\n14. Why is the size of a unioned language: |L3| ≤ |L1| + |L2|? Under what\\ncircumstances does |L3| = |L1| + |L2|?\\n15. The size of n in the Pumping Lemma is given as less than the number\\nof states in the smallest FSA that can describe the language. Explain\\nwhy this restriction holds.\\nChapter 7\\nFormal Language Theory\\nContinued\\nIn this chapter, we treat the context-free languages, generated with rules of\\nthe form A → ψ, where A is a non-terminal and ψ is a string of terminals\\nand non-terminals. We also consider some higher-order systems and relations\\nbetween languages.\\n7.1\\nContext-free Languages\\nFrom the deﬁnition of right-linear grammars and context-free grammars, it\\nfollows that any language that can be described in terms of a right-linear\\ngrammar can be described in terms of a context-free grammar. This is true\\ntrivially since any right-linear grammar is deﬁnitionally also a context-free\\ngrammar.\\nWhat about in the other direction though? There are languages that can\\nbe described in terms of context-free grammars that cannot be described in\\nterms of right-linear grammars. Consider, for example, the language anbn:\\n{ǫ, ab, aabb, aaabbb, . . .}. It can be generated by a context-free grammar, but\\nnot by a right-linear grammar. Here is a simple context-free grammar for\\nthis language:\\n(7.1)\\nS → a S b\\nS → ǫ\\n124\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n125\\nHere are some sample trees produced by this grammar.\\n(7.2)\\nS\\nǫ\\nS\\na\\nS\\nǫ\\nb\\nS\\na\\nS\\na\\nS\\nǫ\\nb\\nb\\nIn the previous chapter, we showed how we could use the Pumping Lemma\\nfor regular languages to show that anbn could not be regular, and therefore\\ncannot be described with a right-linear grammar.\\nAnother language type that cannot be treated with a right-linear gram-\\nmar is xxR where a string x is followed by its mirror-image xR, including\\nstrings like aa, bb, abba, baaaaaab, etc. This can be treated with a context-\\nfree grammar like this:\\n(7.3)\\nS → a S a\\nS → b S b\\nS → ǫ\\nThis produces trees like this one:\\n(7.4)\\nS\\nb\\nS\\na\\nS\\na\\nS\\na\\nS\\nǫ\\na\\na\\na\\nb\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n126\\nThe problem is that both sorts of language require that we keep track\\nof a potentially inﬁnite amount of information over the string. Context-free\\ngrammars do this by allowing the edges of the right side of rules to depend\\non each other (with other non-terminals in between). Thus, in a rule like\\nA → b C d, we say that b and d depend on each other with the distance\\nbetween them governed by how C can be rewritten. This sort of dependency\\nis, of course, not possible with a right-linear grammar.\\n7.2\\nPushdown Automata\\nContext-free grammars are equivalent to a particular simple computational\\nmodel, e.g. a non-deterministic pushdown automaton (PDA). A PDA is just\\nlike a FSA, except it includes a stack, a memory store that can be utilized\\nas each symbol is read from the tape.\\nThe stack is restricted, however. In particular, symbols can be added to\\nor read oﬀ of the top of the stack, but not to or from lower down in the stack.\\nFor example, If the symbols a, b, and c are put on the stack in that order,\\nthey can only be retrieved from the stack in the opposite order: c, b, and\\nthen a. This is the intended sense of the term pushdown.1\\nThus, at each step of the PDA, we need to know what state we are in,\\nwhat symbol is on the tape, and what symbol is on top of the stack. We can\\nthen move to a diﬀerent state, reading the next symbol on the tape, adding\\nor removing the topmost symbol of the stack, or leaving it intact. A string\\nis accepted by a PDA if the following hold:\\n1. the whole input has been read;\\n2. the stack is empty;\\n3. the PDA is in a ﬁnal state.\\nA non-deterministic pushdown automaton can be deﬁned more formally\\nas follows:\\nDeﬁnition 14 (Non-deterministic PDA) A non-deterministic PDA is a\\nsextuple ⟨K, Σ, Γ, s, F, ∆⟩, where K is a ﬁnite set of states, Σ is a ﬁnite\\nset (the input alphabet), Γ is a ﬁnite set (the stack alphabet), s ∈ K is the\\n1A stack is also referred to as “last in ﬁrst out” (LIFO) memory.\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n127\\ninitial state, F ⊆ K is the set of ﬁnal states, and ∆, the set of transitions is\\na ﬁnite subset of K × (Σ ∪ ǫ) × (Γ ∪ ǫ) × K × (Γ ∪ ǫ).\\nLet’s consider an example. Here is a PDA for anbn.\\n(7.5)\\nStates:\\nK\\n=\\n{q0, q1}\\nInput alphabet:\\nΣ\\n=\\n{a, b}\\nStack alphabet:\\nΓ\\n=\\n{c}\\nInitial state:\\ns\\n=\\nq0\\nFinal states:\\nF\\n=\\n{q0, q1}\\nTransitions:\\n∆\\n=\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\n(q0, a, ǫ) → (q0, c)\\n(q0, b, c) → (q1, ǫ)\\n(q1, b, c) → (q1, ǫ)\\n\\uf8fc\\n\\uf8f4\\n\\uf8fd\\n\\uf8f4\\n\\uf8fe\\nThe PDA puts the symbol c on the stack every time it reads the symbol a\\non the tape. As soon as it reads the symbol b, it removes the topmost c from\\nthe stack and moves to state q1, where it removes an c from the stack for\\nevery b that it reads on the tape. If the same number of instances of a and\\nb are read, then the stack will be empty when there are no more symbols on\\nthe tape.\\nTo see this more clearly, let us deﬁne a situation for a PDA as follows.\\nDeﬁnition 15 (Situation of a PDA) A situation of a PDA is a quadru-\\nple (x, q, y, z), where q ∈ K, x, y ∈ Σ∗, and z ∈ Γ∗.\\nThe term x refers to how much of the string has been read. q is the current\\nstate. y is the remainder of the string, and z is the current contents of the\\nstack. This is just like the situation of an FSA, except that it includes a\\nspeciﬁcation of the state of the stack in z.\\nConsider now the sequence of situations which shows the operation of the\\nprevious PDA on the string aaabbb.\\n(7.6)\\n(ǫ, q0, aaabbb, ǫ) ⊢ (a, q0, aabbb, c) ⊢ (aa, q0, abbb, cc) ⊢\\n(aaa, q0, bbb, ccc) ⊢ (aaab, q1, bb, cc) ⊢ (aaabb, q1, b, c) ⊢\\n(aaabbb, q1, ǫ, ǫ)\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n128\\nThe ﬁrst situation (ǫ, q0, aaabbb, ǫ) shows that no part of the string has been\\nread, that the PDA starts in q0, that the string to be read is aaabbb, and that\\nthe stack is empty. The ﬁrst three symbols are read oﬀ one by one giving rise\\nto (aaa, q0, bbb, ccc). Here, the ﬁrst three symbols have been read: aaa. The\\nPDA is still in state q0 and there are still three symbols to read: bbb. Three\\nsymbols have been pushed onto the stack: ccc. The next symbol is the key\\none. Once a b is read, the machine moves to q1 and starts pulling symbols\\noﬀ the stack. Once all symbols have been read, we reach (aaabbb, q1, ǫ, ǫ),\\nindicating that all of aaabbb has been read, the machine is in q1, a legal ﬁnal\\nstate, there is no more of the string to read, and the stack is empty. This is,\\ntherefore, a legal string in the language.\\nNotice that this PDA is deterministic in the sense that there is no more\\nthan one arc from any state on the same symbol.2 This PDA still qualiﬁes\\nas non-deterministic under Deﬁnition 14, since deterministic automata are a\\nsubset of non-deterministic automata.\\nThe context-free languages cannot all be treated with deterministic PDAs,\\nhowever. Consider the language xxR (with Σ = {a, b}), where a string is\\nfollowed by its mirror image, e.g. aa, abba, bbaabb, etc. We’ve already seen\\nthat this is trivially generated using context-free rewrite rules. Here is a\\nnon-deterministic PDA that generates the same language.\\n(7.7)\\nStates:\\nK\\n=\\n{q0, q1}\\nInput alphabet:\\nΣ\\n=\\n{a, b}\\nStack alphabet:\\nΓ\\n=\\n{A, B}\\nInitial state:\\ns\\n=\\nq0\\nFinal states:\\nF\\n=\\n{q0, q1}\\nTransitions:\\n∆\\n=\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n(q0, a, ǫ) → (q0, A)\\n(q0, b, ǫ) → (q0, B)\\n(q0, a, A) → (q1, ǫ)\\n(q0, b, B) → (q1, ǫ)\\n(q1, a, A) → (q1, ǫ)\\n(q1, b, B) → (q1, ǫ)\\n\\uf8fc\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8fd\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8fe\\nHere is the sequence of situations for abba that results in the string being\\n2Notice that the PDA is not complete, as there is no arc on a from state q1.\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n129\\naccepted.\\n(7.8)\\n(ǫ, q0, abba, ǫ) ⊢ (a, q0, bba, A) ⊢ (ab, q0, ba, BA) ⊢\\n(abb, q1, a, A) ⊢ (abba, q1, ǫ, ǫ)\\nWe begin in (ǫ, q0, abba, ǫ): none of the string has been read; we are in state\\nq0; we have the whole string to read: abba; and the stack is empty. We then\\nread the ﬁrst symbol moving to this situation: (a, q0, bba, A) Here, a single\\na has been read; we are still in q0; we have bba to read; and the stack has a\\nsingle A in it. We next read b, putting us in this situation: (ab, q0, ba, BA).\\nHere, ab has been read; we are still in q0; we have ba yet to read; and the\\nstack has two symbols in it now: BA. The third symbol provides a choice.\\nWe can either add a B to the stack or remove a B from the stack. If we elect\\nto remove a B from the stack, we are in this situation: (abb, q1, a, A). Three\\nsymbols have been read; we are now in q1, a single a is left to read; and the\\nstack has a single A on it. We read the ﬁnal a and move to (abba, q1, ǫ, ǫ).\\nHere all symbols have been read; we are in a designated ﬁnal state: q1; there\\nare no more symbols to be read; and the stack is empty. This is therefore a\\nlegal string in the language.\\nNotice that at any point where two identical symbols occur in a row, the\\nPDA can guess wrong and presume the reversal has occurred or that it has\\nnot. In the case of abba, the second underlined b does signal the beginning\\nof the reversal, but in abbaabba, the second underlined b does not signal the\\nbeginning of the reversal. With a string of all identical symbols, like aaaaaa,\\nthere are many ways to go wrong.\\nThis PDA is necessarily non-deterministic.\\nThere is no way to know,\\nlocally, when the reversal begins. It then follows that the set of languages\\nthat are accepted by a deterministic PDA are not equivalent to the set of\\nlanguages accepted by a non-deterministic PDA. For example, any kind of\\nPDA can accept anbn, but only a non-deterministic PDA will accept xxR.\\n7.3\\nEquivalence of Non-deterministic PDAs\\nand CFGs\\nWe’ve said that non-deterministic PDAs accept the set of languages gener-\\nated by context-free grammars (CFGs).\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n130\\nTheorem 4 Context-free grammars generate the same kinds of languages as\\nnon-deterministic pushdown automata.\\nThis is actually rather complex to show, but we will show how to construct\\na non-deterministic PDA from a context-free grammar and vice versa.\\n7.3.1\\nCFG to PDA\\nGiven a CFG G = ⟨VN, VT, S, R⟩, we construct a non-deterministic PDA as\\nfollows.\\n1. K = {q0, q1}\\n2. s = q0\\n3. F = {q1}\\n4. Σ = VT\\n5. Γ = {VN ∪ VT}\\nThere are only two states, one being the start state and the other the sole\\nﬁnal state. The input alphabet is identical to the set of terminal elements\\nallowed by the CFG and the stack alphabet is identical to the set of terminal\\nplus non-terminal elements.\\nThe transition relation ∆ is constructed as follows:\\n1. (q0, ǫ, ǫ) → (q1, S) is in ∆.\\n2. For each rule of the CFG of the form A → ψ, ∆ includes a transition\\n(q1, ǫ, A) → (q1, ψ).\\n3. For each symbol a ∈ VT, there is a transition (q1, a, a) → (q1, ǫ).\\nThe ﬁrst rule above reads no symbol, but puts an S on the stack. The\\nsecond rule is the real workhorse.\\nFor every rule in the grammar of the\\nform A → ψ, there is a transition that pulls A oﬀ the stack and puts the\\nsymbols comprising ψ on the stack. Finally, for every terminal element of\\nthe grammar, there is a rule that reads that terminal from the string and\\npulls the corresponding element from the stack.\\nLet’s consider how this works with a simple context-free grammar:\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n131\\n(7.9)\\nS\\n→\\nNP VP\\nVP\\n→\\nV NP\\nNP\\n→\\nN\\nN\\n→\\nJohn\\nN\\n→\\nMary\\nV\\n→\\nloves\\nWe have K, s, and F as above.\\n1. K = {q0, q1}\\n2. s = q0\\n3. F = {q1}\\nFor Σ and Γ, we have:\\n(7.10)\\nΣ\\n=\\n{John, loves, Mary}\\nΓ\\n=\\n{S, NP, VP, V, N, John, loves, Mary}\\nThe transitions of ∆ are as follows:\\n(7.11)\\n(q0, ǫ, ǫ)\\n→\\n(q1, S)\\n(q1, ǫ, S)\\n→\\n(q1, NP VP)\\n(q1, ǫ, NP)\\n→\\n(q1, N)\\n(q1, ǫ, VP)\\n→\\n(q1, V NP)\\n(q1, ǫ, N)\\n→\\n(q1, John)\\n(q1, ǫ, N)\\n→\\n(q1, Mary)\\n(q1, ǫ, V )\\n→\\n(q1, loves)\\n(q1, John, John)\\n→\\n(q1, ǫ)\\n(q1, Mary, Mary)\\n→\\n(q1, ǫ)\\n(q1, loves, loves)\\n→\\n(q1, ǫ)\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n132\\nThe ﬁrst one corresponds to the ﬁrst case in the construction. The next six\\ncorrespond to individual rewrite rules and the second case in the construction.\\nThe last three above correspond to each of the terminal elements of the\\ngrammar and the third case of the construction.\\nLet’s now look at how this PDA treats an input sentence like Mary loves\\nJohn. Following, we give the sequence of situations that result in the sentence\\nbeing accepted.\\n(7.12)\\n(ǫ, q0, Mary loves John, ǫ) ⊢\\n(ǫ, q1, Mary loves John, S) ⊢\\n(ǫ, q1, Mary loves John, NP VP) ⊢\\n(ǫ, q1, Mary loves John, N VP) ⊢\\n(ǫ, q1, Mary loves John, Mary VP) ⊢\\n(Mary, q1, loves John, VP) ⊢\\n(Mary, q1, loves John, V NP) ⊢\\n(Mary, q1, loves John, loves NP) ⊢\\n(Mary loves, q1, John, NP) ⊢\\n(Mary loves, q1, John, N) ⊢\\n(Mary loves, q1, John, John) ⊢\\n(Mary loves John, q1, ǫ, ǫ)\\nFirst, the non-terminal elements are put onto the stack. We then proceed\\nto expand each non-terminal down to terminals in a left-to-right fashion,\\nreplacing symbols on the stack as we expand and popping symbols oﬀ the\\nstack as we reach terminal elements. Finally, the string is completely read\\nand the stack is empty.\\nThis is not a proof that an equivalent non-deterministic PDA can be\\nconstructed from any CFG, but it shows the basic logic of that proof.\\n7.3.2\\nPDA to CFG\\nIn this section, we show how to construct a context-free grammar from a\\npushdown automaton. The construction is a little tricky as we construct\\nnon-terminals from state/stack sequences of the form [p, X, q], where p and\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n133\\nq are states in the PDA and X is a symbol in the stack alphabet Γ.3\\nThere are two parts to the construction:\\nInitial Rule Add rewrite rules of the form S → [q0, ǫ, p] for all states p ∈ Q.\\nTransition Rule Add rules of the form\\n[q, A, qm+1] → a [q1, B1, q2] [q2, B2, q3] . . . [qm, Bm, qm+1]\\nfor each q, q1, q2, . . . , qm+1 in Q, each a in Σ∪{ǫ}, and A, B1, B2, . . . , Bm\\nin Γ, such that δ(q, a, A) contains (q1, B1B2 . . . Bm). (If m = 0, then\\nthe production is [q, A, q1] → a.)\\nIt’s best to understand the Transition Rule as applying in three cases:\\nremoving a symbol from the stack, changing a symbol on the stack, and\\nadding a symbol to the stack. Let’s take these in order of complexity. If a\\nsymbol A is removed from the stack when reading symbol a and we move\\nfrom state q to state q1, then we add a rule of the form [q, A, q1] → a.\\nIf we replace the topmost symbol A on the stack with another (singleton)\\nsymbol B, reading the symbol a, and moving from state q to state q1, then\\nwe add rules of this form to the grammar: [q, A, p] → a [q1, B, p]. We add as\\nmany instances of this rule as there are states in the original PDA, with p\\nbeing replaced by each of those state names.\\nFinally, if we add a symbol to the stack, eﬀectively replacing A with BA,\\nwhile reading a and moving from state q to q1, then we add rules of the form:\\n[q, A, r] → a [q1, B, p] [p, A, r], for every possible combination of states p and\\nr in the original PDA. Thus, if the original PDA had three states, we would\\nadd nine rewrite rules.\\nNotice that this requires that we keep track of what symbols might be on\\nthe top of the stack at any time.\\nLet’s show how this works for the PDA for anbn in (7.5). The Initial Rule\\ngives us two rules:\\n(7.13)\\nS → [q0, ǫ, q0]\\nS → [q0, ǫ, q1]\\nAs usual, S is the designated start symbol for the grammar. The symbols\\non the right side are constructed from the start state for the PDA q0 and the\\nset of all possible states in the automaton: {q0, q1}.\\n3See Hopcroft and Ullman (1979) for details.\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n134\\nTo apply the Transition Rule, we must consider the transitions of the\\noriginal PDA, repeated below.\\n(7.14)\\n(q0, a, ǫ) → (q0, c)\\n(q0, b, c) → (q1, ǫ)\\n(q1, b, c) → (q1, ǫ)\\nThe latter two cases are straightforward.\\nThey involve popping oﬀ a\\nsymbol from the stack and thus entail adding one rewrite rule each.\\n(7.15)\\n[q0, c, q1] → b\\n[q1, c, q1] → b\\nThe ﬁrst case is more complex, as it really stands in for two diﬀerent\\nsituations: the stack might be empty or the stack might already have a c on\\nit. These two cases are given below:\\n(7.16)\\n(q0, a, ǫ) → (q0, c)\\n(q0, a, c) → (q0, cc)\\nFor the ﬁrst case, we add two rules:\\n(7.17)\\n[q0, ǫ, q0] → a [q0, c, q0]\\n[q0, ǫ, q1] → a [q0, c, q1]\\nFor the second case, since it involves the addition of a stack symbol, we\\nadd four rules:\\n(7.18)\\n[q0, c, q0] → a [q0, c, q0] [q0, c, q0]\\n[q0, c, q0] → a [q0, c, q1] [q1, c, q0]\\n[q0, c, q1] → a [q0, c, q0] [q0, c, q1]\\n[q0, c, q1] → a [q0, c, q1] [q1, c, q1]\\nWe collect all the rules of the grammar below:\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n135\\n(7.19)\\nS → [q0, ǫ, q0]\\nS → [q0, ǫ, q1]\\n[q0, c, q1] → b\\n[q1, c, q1] → b\\n[q0, ǫ, q0] → a [q0, c, q0]\\n[q0, ǫ, q1] → a [q0, c, q1]\\n[q0, c, q0] → a [q0, c, q0] [q0, c, q0]\\n[q0, c, q0] → a [q0, c, q1] [q1, c, q0]\\n[q0, c, q1] → a [q0, c, q0] [q0, c, q1]\\n[q0, c, q1] → a [q0, c, q1] [q1, c, q1]\\nThis produces a tree like the following for aabb.\\n(7.20)\\nS\\n[q0, ǫ, q1]\\na\\n[q0, c, q1]\\na\\n[q0, c, q1]\\nb\\n[q1, c, q1]\\nb\\nThis is not a rigorous proof that an equivalent CFG can be constructed\\nfrom any PDA, but it shows the general character of that proof.\\n7.4\\nClosure Properties of Context-free Lan-\\nguages\\nThe context-free languages are closed under a number of operations including\\nunion, concatenation, and Kleene star. They are not closed under comple-\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n136\\nmentation, nor are they closed under intersection.4\\nThe demonstration that they are not generally closed under intersection\\nis easy to see. One can show that anbncn is beyond the limits of context-free\\ngrammar. Now we know that anbn is context-free. We can complicate that\\njust a little and still stay within the limits of context-free grammar: anbncm,\\nwhere though the ﬁrst two symbols must be paired, there can be any number\\nof instances of the third. If we try to intersect anbncm and ambncn, which is\\nalso of course describable with a context-free grammar, we get anbncn, which\\nis not context-free.\\n7.5\\nPumping Lemma for Context-free Lan-\\nguages\\nThe Pumping Lemma also applies to context-free languages. It is similarly\\nopaque, but we can try to make some sense of it. The basic idea is to track\\ndown how context-free languages allow for inﬁnity.\\nIf we think of the context-free languages in terms of a PDA, the basic\\nmechanism for inﬁnity is the stack, allowing an inﬁnite number of elements\\nto be stored and then read back.\\nThis allows for two matched inﬁnitely\\nlong substrings in a word of the language. One substring is where the stack\\nsymbols are added and the other is where the stack symbols are read back.\\nAnother way to think of this is in terms of a context-free grammar. In-\\nﬁnity comes about because of recursion. Some rule either feeds itself, e.g.\\nA → a A a or there are several rules that create a similar cycle, e.g. A → a B\\nand B → A a. Either way, we get an inﬁnite number of words in the lan-\\nguage. Focusing for the moment on the simple case, the key is that when\\na rule is recursive, it allows material to each side of the repeated symbol to\\nrecur. Thus in A → a A a, there is a symbol to each side of the A on the\\nright side of the rule that can be repeated. Therefore, there are, once again,\\ntwo inﬁnitely long substrings in an inﬁnite context-free language.\\nHere is a formal statement of the Pumping Lemma.5\\nTheorem 5 If L is a context-free language, then there is a constant n, such\\n4Though they are, as you might expect, closed under intersection with a regular lan-\\nguage.\\n5As in the previous chapter, we give this as a theorem, but refer to it as a lemma to\\nkeep with the traditional nomenclature.\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n137\\nthat if z is in L and |z| ≥ n, then we may write z = uvwxy such that:\\n|vx| ≥ 1, |vwx| ≤ n, and for all i ≥ 0, uviwxiy is in L.\\nThe basic idea is very similar to the Pumping Lemma for Regular Languages.\\nOnce the strings of a language reach a certain point in length, then we are\\nalways able to identify two substrings that can be repeated any number of\\ntimes to produce inﬁnitely more strings in the language.\\nConsider, for example, the language anbn, which we know is context-free.\\nHere, n = 2.\\nThat is, once we have a string ab, we can identify all the\\nvariables of uvwxy, i.e. u = ǫ, v = a, w = ǫ, x = b, and y = ǫ. For example\\nif i = 3, then we have a legal string: uv3wx3y = aaabbb.\\nThe Pumping Lemma for Context-free Languages can be used to show\\nthat some languages are not context-free. For example, the language anbncn\\nexceeds context-free. This language allows strings like:\\n(7.21) {abc, aabbcc, aaabbbccc, . . .}\\nImagine that we set n to 3.\\nIf so, then we must be able to ﬁnd two\\nrepeatable substrings in any word of three or more symbols. Take the string\\nabc. If we require that |vx| ≥ 1 and |vwx| ≤ 3, then only certain assignments\\nare possible and all of them make incorrect predictions when i = 2. These\\nare all diagrammed in the following chart.6\\n6The chart omits cases where a symbol can be assigned to more than one of u, w, or y.\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n138\\n(7.22)\\nu\\nvi\\nw\\nxi\\ny\\ni = 2\\nab\\nc\\nǫ\\nǫ\\nǫ\\nabcc\\nab\\nǫ\\nǫ\\nc\\nǫ\\nabcc\\na\\nbc\\nǫ\\nǫ\\nǫ\\nabcbc\\na\\nb\\nc\\nǫ\\nǫ\\nabbc\\na\\nb\\nǫ\\nc\\nǫ\\nabbcc\\na\\nǫ\\nǫ\\nbc\\nǫ\\nabcbc\\na\\nǫ\\nǫ\\nb\\nc\\nabbc\\nǫ\\nabc\\nǫ\\nǫ\\nǫ\\nabcabc\\nǫ\\nab\\nc\\nǫ\\nǫ\\nababc\\nǫ\\nab\\nǫ\\nc\\nǫ\\nababcc\\nǫ\\na\\nbc\\nǫ\\nǫ\\naabc\\nǫ\\na\\nb\\nc\\nǫ\\naabcc\\nǫ\\na\\nǫ\\nbc\\nǫ\\naabcbc\\nǫ\\na\\nǫ\\nb\\nc\\naabbc\\nǫ\\nǫ\\nǫ\\nabc\\nǫ\\nabcabc\\nǫ\\nǫ\\nǫ\\nab\\nc\\nababc\\nǫ\\nǫ\\nǫ\\na\\ncb\\naabc\\nThe ﬁrst ﬁve columns show the assignments and the rightmost column shows\\nwhat happens when i = 2. The same result obtains for any value of n and\\nanbncn is not context-free.\\n7.6\\nNatural Language Syntax\\nThe syntax of English cannot be regular. Consider these examples:\\n(7.23)\\nThe cat died.\\nThe cat [the dog chased] died.\\nThe cat [the dog [the rat bit] chased] died.\\n...\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n139\\nThis is referred to as center embedding. Center-embedded sentences generally\\nrequire a match between the number of subjects and the number of verbs.\\nThese elements do not occur adjacent to each other (except for the most\\nembedded pair). This, then, is equivalent to anbn and beyond the range of\\nthe regular languages.7\\nMost speakers of English ﬁnd sentences like these rather marginal once\\nthey get above two or three clauses. It is thus a little disturbing that the\\nbest example of how natural language syntax cannot be regular is of this\\nsort. The problem can be made clear with a formal example. We know that\\nanbn is context-free. What if we bound n? For example, imagine we have a\\nlanguage L = {ab, aabb, aaabbb}. Even though the pattern is the same, here\\nthe language is ﬁnite and can be deﬁned by simply enumerating its members.\\nHence, such a language is certainly no more than regular.\\nIf we assume that center-embedded sentences are grammatical up to in-\\nﬁnity, is natural language syntax context-free or context-sensitive? Shieber\\n(1985) argues that natural language syntax must be context-sensitive based\\non data from Swiss German. Examples like the following are grammatical.\\n(Assume the sentence begins with the phrase Jan s¨ait das ‘John said that’.)\\n(7.24)\\nmer\\nd’chind\\nem Hans\\nes huus\\nl¨ond\\nh¨alfe\\naastriiche.\\nwe\\nchildren\\nHans\\nhouse\\nlet\\nhelp\\npaint\\n‘. . . we let the children help Hans paint the house.’\\nThis is equivalent to the language ww, e.g. aa, abab, abbabb, etc., which is\\nknown not to be context-free.8\\nIf the Swiss German pattern is correct, then it means that any formal\\naccount of natural language syntax requires more than a PDA and that a\\nformalism based on context-free grammar is inadequate. Notice, however,\\nthat, once again, it is essential that the embedding be unbounded. That is,\\nwhile ww is clearly beyond context-free, {aa, abab, abcabc} is ﬁnite.\\n7.7\\nDeterminism\\nSince at least some context-free languages require non-deterministic PDAs,\\nand non-deterministic PDAs cannot be reduced to deterministic ones, deter-\\n7This argument is due to Partee et al. (1990).\\n8Shieber actually goes further and shows that examples of this sort are not simply an\\naccidental subset of the set of Swiss German sentences, but I leave this aside here.\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n140\\nmining whether some particular string is accepted by such a PDA is a more\\ncomplex operation than for the regular languages.\\nRecall that, for a regular language, we can always construct a DFA. Thus,\\nfor the regular languages, the number of steps we need to consider to deter-\\nmine the acceptability of some string s is equal to the length of that string.\\nOn the other hand, if we are interested in whether some string s is ac-\\ncepted by a non-deterministic PDA, we must keep considering paths through\\nthe automaton until we ﬁnd one that terminates with the appropriate prop-\\nerties: end of string, empty stack, in a ﬁnal state.\\nThis may be quite a\\nfew paths to consider. Recall the context-free language xxR and the non-\\ndeterministic PDA we described to treat it. For any string of length n, we\\nmust entertain the hypothesis that the reversal begins at any point between\\n1 and n. This entails that we must consider lots of paths for a long string.9\\nWhat this means, in concrete terms, is that if some phenomenon can\\nbe treated in ﬁnite-state terms or in context-free terms, and eﬃciency is a\\nconcern, go with the ﬁnite-state treatment.\\n7.8\\nOther Machines\\nThere are other machines far more powerful than PDAs. For example, there\\nare Turing machines (TMs). These are like FSAs except i) the reading head\\ncan move in either direction, and ii) it can write to as well as read from the\\ntape. These properties allow TMs to use empty portions of the input tape\\nas an unbounded memory store without the access limitations of a stack.\\nFormally, a TM is deﬁned as follows.\\nDeﬁnition 16 (Turing Machine) A Turing machine (TM) is a quadruple\\n(K, Σ, q0, δ), where K is a ﬁnite set of states, Σ is a ﬁnite alphabet including\\n#, q0 ∈ K is the start state, and δ is a partial function from K × Σ to\\nK × (Σ ∪ {L, R}).\\nHere # is used to mark initially empty portions of the input tape. The logic\\nof δ is that it maps from particular state/symbol combinations to a new\\n9It might be thought that we must consider an inﬁnite number of paths, but this is\\nnot necessarily so. Any non-deterministic PDA with an inﬁnite number of paths for some\\nﬁnite string can be converted into a non-deterministic PDA with only a ﬁnite number of\\npaths for some ﬁnite string. See Hopcroft and Ullman (1979). The eﬀect is that for a\\nstring s of length n and a non-deterministic automaton with m states, we may have to\\nconsider as many as nm paths.\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n141\\nstate, simultaneously (potentially) writing a symbol to the tape, or moving\\nleft or right.\\nTMs can describe far more complex languages than are thought to be\\nappropriate for human language. For example anbn! can be treated with a\\nTM. Likewise, an, where n is prime, can be handled with a TM. Hence, while\\nthere is a lot of work in computer science on the properties of TMs, there\\nhas not been a lot of work in grammatical theory using them.\\nAnother machine type that we have not treated so far is the ﬁnite state\\ntransducer (FST). The basic idea behind a transducer is that we start with\\nan FSA, but label the arcs with pairs of symbols.\\nThe machine can be\\nthought of as reading two tapes in parallel. If the pairs of symbols on each\\nrespective arc match what is on the two tapes—and the machine ﬁnishes\\nin a designated ﬁnal state—then the pair of strings is accepted. Another\\nway to think of these, however, is that the machine reads one tape and spits\\nout some symbol every time it transits an arc (perhaps writing those latter\\nsymbols to a new blank tape). Thus, if an arc is labeled a : b, the machine\\nwould read an a and spit out a b.\\nFormally, we can deﬁne an FST as follows:\\nDeﬁnition 17 (FST) An FST is a sextuple (K, Σ, Γ, s, F, ∆) where K is\\na ﬁnite set of states, Σ is the ﬁnite input alphabet, Γ is the ﬁnite output\\nalphabet, s ∈ K is the start state, F ⊆ K is the set of ﬁnal states and ∆ is\\na relation from K × (Σ ∪ ǫ) to K × (Γ ∪ ǫ).\\nThe relation ∆ moves from state to state pairing symbols of Σ with symbols\\nof Γ. The instances of ǫ in ∆ allow it to insert or remove symbols, thus\\nmatching strings of diﬀerent lengths.\\nFor example, here is an FST that operates with the alphabet Σ = {a, b, c},\\nwhere anytime a b is confronted on one tape, the machine spits out c.\\n(7.25)\\nq0\\nb : c\\na : a\\nc : c\\nSuch an FST would convert abbcbcaa into acccccaa.\\nThe interest of such machines is twofold. First, like FSAs they are quite\\nrestricted in power and very well understood. Second, many domains of lan-\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n142\\nguage and linguistics are modeled with input–output pairings and a trans-\\nducer provides a tempting model for such a system. For example, in phonol-\\nogy, if we posit rules or constraints that nasalize vowels before nasal conso-\\nnants, we might model that with a transducer that pairs oral vowels with\\nnasal vowels just in case the following segment is a nasal consonant.\\nLet’s look a little more closely at the kinds of relationships multi-tape\\ntransducers allow (Kaplan and Kay, 1994). First we need a notion of n-\\nway concatenation. This generalizes the usual notion of concatenation to\\ntransducers with more than one tape.10\\nDeﬁnition 18 (n-way concatenation) If X is an ordered tuple of strings\\n⟨x1, x2, . . . , xn⟩ and Y is an ordered tuple of strings ⟨y1, y2, . . . , yn⟩ then the\\nn-way concatenation of X and Y , X · Y is deﬁned as ⟨x1y1, x2y2, . . . xnyn⟩\\nIt’s also convenient to have a way to talk about alphabets that include ǫ.\\nWe deﬁne Σǫ = {Σ ∪ ǫ}. With these in place, we can deﬁne the notion of a\\nregular relation (Kaplan and Kay, 1994).\\nDeﬁnition 19 (Regular Relation) We deﬁne the regular relations as fol-\\nlows:\\n1. The empty set and all a in Σǫ × . . . × Σǫ are n-way regular relations.\\n2. If R1, R2, and R are all regular relations, then so are:\\nR1 · R2 = {xy | x ∈ R1, y ∈ R2}\\n(n-way concatenation)\\nR1 ∪ R2\\n(union)\\nR∗ = �∞\\ni=0 Ri\\n(n-way Kleene closure)\\n3. There are no other regular relations.\\nFirst, we have the base case that any combination of symbols is a regular\\nrelation. For example, a : b is a regular relation as is b : c, or ǫ : a or b : b.\\nThere are then three recursive clauses. The ﬁrst allows us to concatenate,\\ne.g. (a : b) · (b : c) = (ab) : (bc). The second allows us to take the union of\\ntwo regular relations, e.g. (a : b) ∪ (b : c). Finally, the third recursive clause\\nallows us to repeat a relation any number of times, e.g. (a : b)∗. Notice how\\n10The formal deﬁnitions here are general and allow for two or more tapes. Most appli-\\ncations in language only involve two tapes.\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n143\\nthese parts of the deﬁnition are exactly parallel to the parts of the deﬁnition\\nof a regular language (Deﬁnition 13 on page 112).\\nWe can show that the regular relations are equivalent to ﬁnite state trans-\\nducers. There are two parts to such a demonstration. First, we would need\\nto show that any regular relation can be mapped to an FST. Then we would\\nneed to show that every FST can be mapped to a regular relation.\\nLet’s consider this from the perspective of creating a regular relation from\\nan FST, e.g. the one in (7.25). There are three simple relations indicated in\\nthe FST.\\n(7.26) {(a : a), (b : c), (c : c)}\\nEach loop back to the start state can be seen as a Kleene star applied to a\\nsimple regular relation. Thus, we have:\\n(7.27) {(a : a)∗, (b : c)∗, (c : c)∗}\\nFinally, multiple outgoing arcs from a single node express union, so this can\\nbe converted to:\\n(7.28) (a : a)∗ ∪ (b : c)∗ ∪ (c : c)∗\\nSince each step corresponds to a clause in the deﬁnition of the regular rela-\\ntions, it follows that we can construct a regular relation from an FST.11\\nIt should be apparent that the regular relations are closed under the\\nusual regular operations.12 The regular relations are closed under a number\\nof other operations too, e.g. the ones above, but also reversal, inverse, and\\ncomposition. Finally, it should be apparent that each half of a regular relation\\nis itself a regular language. For example, in the case above, (a : a)∗ ∪ (b :\\nc)∗ ∪(c : c)∗, we have (a∗|b∗|c∗) as the upper language and (a∗|c∗|c∗) = (a∗|c∗)\\nas the lower language, both of which are regular.\\nThey are not closed under intersection and complementation, however.\\nFor example, imagine we have\\n11We can do this in the other direction as well, but we leave that as an exercise.\\n12Note that the regular relations are equivalent to the “rational relations” of algebra for\\nthe mathematically inclined.\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n144\\nR1 = {(an) : (bnc∗) | n ≥ 0}\\nand\\nR2 = {(an) : (b∗cn) | n ≥ 0}\\nEach of these is itself a regular relation. For example, we can show that\\n(an) : (bnc∗) is built up according to the deﬁnition of regular relations.\\n(7.29)\\na : b\\nbase\\n(a : b)∗\\nclause 3\\nǫ : c\\nbase\\n(ǫ : c)∗\\nclause 3\\n(a : b)∗ · (ǫ : c)∗\\nclause 1\\nThe number of instances of a will match the number of instances of b, but\\nthe number of instances of c will be independent. A similar derivation can\\nbe given for R2.\\nThe intersection is clearly not regular. The intersection of R1 and R2\\nwould be (an) : (bncn). The lower language here would be anbn and we know\\nthat is regular.\\nHence, since the upper and lower languages of a regular\\nrelation are themselves regular and anbn is not regular, it follows that (an) :\\n(bncn) is not regular that that the regular relations are not closed under\\nintersection.\\nIt then follows, of course, that the regular relations are not closed under\\ncomplementation (by DeMorgan’s Law).13\\n7.9\\nSummary\\nThe chapter began with a discussion of the context-free languages showing\\nhow they can be accommodated by non-deterministic pushdown automata.\\nWe showed how deterministic pushdown automata were not suﬃcient to ac-\\ncommodate all the context-free languages.\\n13Same-length regular relations are closed under intersection.\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n145\\nWe went on to discuss the closure properties of this class of languages\\nand the Pumping Lemma for Context-free Languages.\\nWe considered the implications of formal language theory for natural\\nlanguage syntax, citing several arguments that natural language must be\\ncontext-free and may even be context-sensitive.\\nLast, we brieﬂy reviewed two additional abstract machine types: Turing\\nmachines and ﬁnite state transducers. We discussed the regular relations and\\ntheir equivalence to transducers.\\n7.10\\nExercises\\n1. In the PDA in (7.7), why must the string have two identical symbols\\nin a row for it to be possible to guess wrong?\\n2. Explain why ww cannot be context-free.\\n3. Assuming the alphabet {a, b, c}, write a transducer which replaces any\\ninstance of a that precedes a b with an c. Otherwise, strings are un-\\nchanged.\\n4. Start with the context-free grammar given for xxR in the chapter and\\nshow how a PDA can be constructed for it using the procedure in the\\nchapter.\\n5. Start with the pushdown automaton given for xxR in the chapter and\\nshow how a CFG can be constructed for it using the construction in\\nthe chapter.\\n6. Give a ﬁnite state transducer that models the phonology of nasalization\\nin English: vowels are nasalized before nasal consonants. Assume Σ =\\n{V, N, C} and Γ = {V, N, C, E}, where V is an oral vowel, N is a nasal\\nconsonant, C is an oral consonant, and E is a nasal vowel.\\n7. Construct a ﬁnite state transducer for this regular relation: (((a : a)·(a :\\na)) ∪ (b : c)∗) · (b : b).\\n8. Explain why the regular relations are closed under the reverse opera-\\ntion.\\n9. Are there any instances of ww in human language?\\nCHAPTER 7. FORMAL LANGUAGE THEORY CONTINUED\\n146\\n10. Find a language phenomenon that we have not discussed and show\\nwhat level of grammar is required to describe it.\\nChapter 8\\nProbability\\nIn this section, we go over some simple concepts from probability theory. We\\nintegrate these with ideas from formal language theory in the next chapter.\\n8.1\\nWhat is it and why should we care?\\nProbability theory is a conceptual and mathematical treatment of likelihood.\\nThe basic idea is that, in some domain, events may not occur with certainty\\nand probability allows us to treat such events in a formal and precise fashion.\\nWe might think that linguistic events are all certain. That is, theories\\nof language are not concerned with judgments or intuitions as events that\\noccur with something less than 100% certainty. For example, we treat the\\ngrammaticality or ungrammaticality of any particular sentence as being cer-\\ntain. Thus John loves Mary is completely grammatical and loves John Mary\\nis completely ungrammatical.\\nLess clear judgments are usually ascribed to performance factors. For\\nexample, the center-embedded constructions we considered in the previous\\nchapter arguably exhibit gradient acceptability, rather than grammaticality,\\nat least on orthodox assumptions. Acceptability diﬀerences are taken to be\\na consequence of performance, while grammaticality diﬀerences are taken to\\nbe a consequence of grammar.\\nThere are, however, a number of places where probability does play a role\\nin language. Consider ﬁrst typological distribution, the range of patterns that\\noccur in the languages we know about. There are presumably an inﬁnite\\nnumber of possible languages, yet the set of extant described languages is\\n147\\nCHAPTER 8. PROBABILITY\\n148\\nonly a ﬁnite subset of those. If a pattern occurs in that subset, do we have\\nreason to assume that it also holds of the entire inﬁnite set? For example, all\\nextant languages have something like the vowel [a]. For example, there is no\\nlanguage where the word for ‘apple’ is fpststsatststsf. Are these representative\\nof the set of possible languages? Can we conclude that any language must\\nhave the vowel [a] and that no language can call an apple fpststsatststsf ?\\nLikewise there are clear skewings among the extant set of languages, cases\\nthat appear exceptional given the available sample of languages. Swiss Ger-\\nman alone exhibits cross-serial dependencies. Dyirbal is syntactically erga-\\ntive, perhaps the only such system.1 Hixkaryana is VOS, again perhaps the\\nonly system of this sort.2 Are these really exceptional systems?\\nA similar problem arises due to electronic corpora and the generalizations\\none might conclude from them.\\nNew technology has made huge corpora\\navailable to everyone, but this leads to statistical questions. For example, if\\nyou ﬁnd no examples of some particular construction in a Google query, does\\nthat mean it doesn’t occur?\\nA third reason for know about this is that sometimes statistical eﬀects\\nhave direct consequences in the grammar. For example, Fidelholtz (1975)\\nshows that vowel reduction in English correlates with lexical frequency.\\nVowel reduction refers to the fact that certain vowels can be pronounced\\nas a shorter “laxer” schwa when unstressed. For example, the ﬁrst vowel of\\nthe word parent [p´ær@nt] is reduced when stress moves from the ﬁrst vowel\\nto the second in a form like parental [p@r´Ent@l].\\nThus, a relatively frequent word like astronomy undergoes initial reduc-\\ntion more readily than a less frequent word like gastronomy. In other words,\\na pronunciation [@str´an@mi] with a reduced ﬁrst vowel is far more likely than\\n[æstr´an@mi] with a full ﬁrst vowel. On the other hand, [g@str´an@mi] with\\na reduced ﬁrst vowel is less likely than [gæstr´an@mi] with a full ﬁrst vowel.\\nAccording to Fidelholtz, this diﬀerence in pronunciation follows from the dif-\\nferent frequencies or likelihoods of the words: more frequent or more likely\\nwords undergo this initial reduction more readily.\\nSimilarly, Hooper (1976) shows that more frequent words undergo medial\\nsyncope more readily. Medial syncope refers to a phenomenon whereby—\\nin certain conﬁgurations—a medial vowel is not pronounced. Compare the\\n1Syntactic ergativity refers to a situation where the subjects of verbs with no objects\\nexhibit similar behavior to the objects of verbs. This can be opposed to languages like\\nEnglish where objects behave diﬀerently from subjects generally.\\n2VOS refers to the unmarked word order in sentences: verb – object – subject.\\nCHAPTER 8. PROBABILITY\\n149\\npronunciations of opera [´apr@] and operatic [`ap@r´æRIk].\\nIn the ﬁrst word,\\nthere is no vowel pronounced between the [p] and the [r]. In the second word,\\nthere is. The conﬁgurations in which this occurs are rather complex and not\\nrelevant here (Hammond, 1997). What Hooper shows is that more frequent\\nor likely words undergo this process more readily.\\nFor example, we ﬁnd\\nrelatively frequent memory [m´Emri] with syncope, but relatively infrequent\\nmammary [m´æm@ri] without.\\nAll of these facts suggest that it might be reasonable to incorporate a\\nnotion of likelihood into the theory of language.\\n8.2\\nBasic Probability\\nIn this section we deﬁne and explain several basic mathematical notions of\\nprobability.\\nFirst, we can deﬁne the probability of an event e as:\\n(8.1) p(e) = n\\nN\\nwhere n is the number of outcomes that qualify as e and N is the total\\npossible number of outcomes. For example, the probability of throwing a\\nthree with a single die is 1\\n6. Here there is a single outcome consistent with\\ne, and six possible outcomes in total. We can consider more complex cases\\ntoo. The probability of throwing a number less than three is 2\\n6 = 1\\n3.\\nTo use an example involving language, we might consider the probability\\nthat the next word out of my mouth will be probability. If, for the sake of\\ndiscussion, we say that English has 19528 words and all words are equally\\nlikely, then the probability of this event is approximately\\n1\\n19528 = .00005.3 If,\\ninstead, we were interested in the probability that the next word out of my\\nmouth begins with the sound [p] and there are 1704 words with this property\\nand all words are equally likely, then the probability of this event would be\\n1704\\n19528 = .08726.\\nSeveral important properties follow from this simple deﬁnition.\\nFirst,\\nit follows that if an event is impossible, it will have a probability of zero.\\n3English, of course, has many more words than this, but this particular number is\\nchosen from a convenient database (available from the author’s website) of English words\\nused to generate all the language examples in this chapter.\\nCHAPTER 8. PROBABILITY\\n150\\nFor example, the probability of throwing a seven with a single die is 0\\n6 =\\n0.\\nThe probability that the next word out of my mouth is [karandaˇs] is\\n0\\n100000.4 Likewise, the probability of a certain or sure event is one. Thus the\\nprobability of throwing a number less than seven with a single die is 6\\n6 = 1.\\nAnalogously, the likelihood that the next word out of my mouth will be a\\nword of English is 19528\\n19528 = 1. Finally, it follows as well that probability values\\nrange between 0 and 1.\\nIt should also now be clear that the probability of a disjunction of inde-\\npendent events is the sum of their probabilities.\\n(8.2) p(e1 ∨ e2) = p(e1) + p(e2)\\nFor example, the probability of throwing a one or a two is the probability of\\na one plus the probability of a two.\\np(1 ∨ 2) = p(1) + p(2) = 1\\n6 + 1\\n6 = 2\\n6 = 1\\n3\\nSimilarly, the probability of saying the word probability (e1) or the word\\nlikelihood (e2) is the sum of their independent probabilities, presumably:\\n1\\n19528 +\\n1\\n19528 =\\n2\\n19528 = .0001\\nAnother way to look at this is to see e1 and e2 each as sets of outcomes. We\\nthen calculate the disjunction of those events by calculating the probability\\nover the union of their component events, i.e. p(e1 ∨ e2) = p(e1 ∪ e2). The\\nexamples we have given so far involve events that are constituted by only a\\nsingle outcome, but the principle applies as well to cases where an event is\\nconstituted by multiple outcomes.\\nFor example, what is the probability of throwing a three or throwing an\\neven number?\\nThe probability of throwing a three is one in six and the\\nevent is constituted by a single outcome:\\n1\\n6. The probability of throwing an\\neven number is three in six and there are three outcomes in the event:\\n3\\n6.\\nThe probability of the disjunction can be calculated by adding together the\\nindependent probabilities:\\n4The Russian word for ‘pencil’.\\nCHAPTER 8. PROBABILITY\\n151\\n1\\n6 + 3\\n6 = 4\\n6\\nor by unioning together the independent events and by taking the probability\\nof the new event:\\np({3} ∪ {2, 4, 6}) = p({2, 3, 4, 6}) = 4\\n6\\nThe same reasoning would apply to ﬁnding the probability of uttering a\\nword that begins with [p] or the word likelihood. There are 1704 outcomes\\nconsistent with the ﬁrst event and one separate outcome consistent with the\\nsecond. Hence, the probability of one or the other occurring is:\\n1705\\n19528 = .08731\\nThis generalizes to the disjunction of overlapping events. Imagine we are\\ninterested in the probability of throwing a number less than three e1 or an\\neven number e2. The ﬁrst event has these outcomes: {1, 2}, and the second\\nhas these: {2, 4, 6}; the set of outcomes overlaps. Their union is: {1, 2, 4, 6},\\nso the probability of their disjunction is p(e1 ∨ e2) = 4\\n6 = 2\\n3.\\nA rather trivial language example would be the probability of uttering a\\nword that begins with [p] or the probability of uttering the word probability.\\nWe have assumed that there are 1704 words that begin with [p], but proba-\\nbility is one of those. Hence the probability of this disjunction is the union\\nof their sets of outcomes:\\n1704\\n19528.\\nThe conjunction of overlapping events—that both events occur—is also\\nstraightforward. It is the simply the number of cases compatible with both\\nevents: the intersection of the outcomes that make up those events. For\\nexample, the probability of throwing a number greater than two and less\\nthan ﬁve, is the intersection of the cases compatible with each. Thus, the\\ncases compatible with a roll greater than two are {3, 4, 5, 6}.\\nThe cases\\ncompatible with a role less than ﬁve are {1, 2, 3, 4}. The intersection of these\\nis {3, 4}, so the probability of both occurring is 2\\n6 = 1\\n3.\\nThe probability of uttering a word that both begins with [p] and ends\\nwith [p] is found by ﬁnding all the words that begin with [p] and all the\\nCHAPTER 8. PROBABILITY\\n152\\nwords that end with [p] and then ﬁnding the intersection of these sets: the\\nset of words that satisfy both properties.5\\nIt also follows that the combined probabilities of all independent events\\nis one. For a single die, the set of all possible events is {1, 2, 3, 4, 5, 6}. The\\nprobability of each is 1\\n6, and their combined probability is:\\n1\\n6 + 1\\n6 + 1\\n6 + 1\\n6 + 1\\n6 + 1\\n6 = 1\\nIn a similar fashion, the probability of a word beginning with any of [a], [b],\\n[c], [d], etc. is 1. Whatever the likelihood of each letter is as the ﬁrst letter\\nof a word, if we combine all possible choices, it must sum to 1.\\nThis is also true of more complex events. For example, the probability\\nof throwing a number less than three is 2\\n6 and the probability of throwing a\\nnumber three or greater is 4\\n6. (These two events exhaust the event space.)\\nTheir combined probability is then 2\\n6 + 4\\n6 = 1.\\nA language example would be the probability of a word beginning with\\na vowel plus the probability of a word beginning with a consonant. Let’s\\nassume that all letters are equally likely and that there are only ﬁve vowel\\nletters. The probability that a word begins with a vowel is then\\n5\\n26 = .192\\nand the probability that a word begins with a consonant is 21\\n26 = .808. These\\nsum as expected: .192 + .808 = 1.6\\nIt now also follows that the probability of some event e not occurring is\\n1 − p(e). For example, if the probability of throwing a number less than two\\nis 1\\n6, and the combined probability of all possible outcomes is one, then the\\nprobability of not throwing a number less than two is 1− 1\\n6 = 5\\n6. Returning to\\nour initial letter example, it follows that the probability of a word beginning\\nwith a consonant is 1 minus the probability that a word begins with a vowel:\\n1 − .192 = .808.\\n5In the sample used to generate the examples of this chapter, those ﬁgures are as\\nfollows.\\nwords that begin with [p]\\n1704\\nwords that end with [p]\\n338\\nwords that begin and end with [p]\\n25\\n6The actual facts are more complex. There are more than ﬁve vowels and twenty-one\\nconsonants; these segments are not equally likely; and spelling only indirectly reﬂects the\\nactual pronunciation.\\nCHAPTER 8. PROBABILITY\\n153\\n8.3\\nCombinatorics\\nLet’s now consider how to “count cases”. Let’s consider ﬁrst permutations,\\nthe possible ways of ordering some set of elements. If we have n elements,\\nthen there are n! ways of ordering those elements.7 For example, if we have\\nthree elements {a, b, c}, then there are 3! = 3 × 2 × 1 = 6 ways of ordering\\nthose elements:\\n(8.3)\\na > b > c\\na > c > b\\nb > a > c\\nb > c > a\\nc > a > b\\nc > b > a\\nWe can make intuitive sense of this easily. If we are attempting to order three\\nobjects in all possible ways, we have three ways to choose a ﬁrst object. Once\\nwe’ve done that, we then have two ways to choose a second object. Once\\nwe’ve done that, we only have a single way to choose a third object. This\\ngives us: 3 × 2 × 1.\\nLet’s move back to the dice example. How many options are there for\\ndrawing a card oﬀ the top of a full deck of cards? 52. How many ways are\\nthere to draw three cards? There are 52 ways to draw a ﬁrst card and then\\n51 ways to draw a second card and 50 ways to draw a third. Each choice\\nis independent, once the previous card has been selected, but the number of\\navailable cards is reduced by on each time, so we have:\\n52 × 51 × 50\\nA similar example in a language domain might be how many ways are there to\\nconstruct a three-letter word with three diﬀerent letters? Given 26 possible\\nletters, there are 26 ways to choose a ﬁrst letter, 25 ways to choose a second\\nletter, and 24 ways to choose a third letter.\\n26 × 25 × 24\\nChoosing r objects—some number of cards—from n objects—a full deck\\nof cards or the alphabet in this case—is a truncated factorial. The easiest\\nway to express this is like this:\\n7As a reminder, n! is the notation for the factorial of n. Factorials are computed by\\nmultiplying together all the integers up to n. For example, 4! is calculated as follows:\\n4 × 3 × 2 × 1 = 24.\\nCHAPTER 8. PROBABILITY\\n154\\n(8.4)\\nn!\\n(n − r)!\\nThe logic of this equation is that we do the full factorial in the numerator,\\nand then divide out the part we are not interested in. In the ﬁrst example\\nat hand, we put 52! in the numerator, and then divide out 49! because we’re\\nreally only interested in 52 × 51 × 50. In the second example, we put 26! in\\nthe numerator and 23! in the denoniminator. This may seem like a whole lot\\nof extra math, but it allows a general expression for these cases.\\nNotice that this distinguishes the order that we might draw the cards or\\nselect the letters. Thus the math above counts the same cards or letters in\\ndiﬀerent orders as diﬀerent possibilities. If we want to consider the possibility\\nthat we draw some number of cards or letters in any order, then we have\\nto divide out the number of orders too. This means we have to divide out\\nthe number of permutations for r elements, which we’ve already seen is r!.\\nThere is a special notation for this:\\n(8.5)\\n�\\nn\\nr\\n�\\n=\\nn!\\nr!(n − r)!\\nIn the case of the three-card example, we have:\\n�\\n52\\n3\\n�\\n=\\n52!\\n3!(52 − 3)! = 52 × 51 × 50\\n3 × 2 × 1\\n= 132600\\n6\\n= 22100\\nIn the case of the three-letter example, we have:\\n�\\n26\\n3\\n�\\n=\\n26!\\n3!(26 − 3)! = 26 × 25 × 24\\n3 × 2 × 1\\n= 15600\\n6\\n= 2600\\n8.4\\nLaws\\nWe’ve already seen some of these, but let’s summarize the laws of probability\\nthat we’ve dealt with. First, if some event e exhibits probability p(e), then\\ne not occurring, written p(e), has probability 1 − p(e).\\nCHAPTER 8. PROBABILITY\\n155\\n(8.6) p(e) = 1 − p(e)\\nThus, if the probability of throwing a six is 1\\n6, then the probability of not\\nthrowing a six is 1 − 1\\n6 = 5\\n6. If, for the sake of discussion, the probability\\nof uttering a verb is .3, then the probability of uttering something that isn’t\\na verb is 1 − .3 = .7. If there are 6273 verbs out of 19528 words, then the\\nprobability of a verb is:\\n6273\\n19528 = .3212\\nIt follows that the probability of uttering something that is not a verb is:\\n1 − .3212 = .6787\\nIf two events A and B are mutually exclusive (their outcomes do not\\noverlap), then the probability that A or B occurs is:\\n(8.7) p(A ∨ B) = p(A) + p(B)\\nThus, if the probability of throwing a six is 1\\n6 and the probability of throwing\\na ﬁve is 1\\n6, then the probability of throwing a ﬁve or a six is 1\\n6 + 1\\n6 = 2\\n6 = 1\\n3.\\nLikewise, if the probability of a word beginning with [p] is\\n1704\\n19528 = .0872\\nand the probability of a word beginning with [t] is\\n878\\n19528 = .0449\\nThen the probability of a word beginning with either [p] or [t] is .0872 +\\n.0449 = .1321.\\nIf two events A and B are independent, then the probability that A and\\nB occurs is:\\nCHAPTER 8. PROBABILITY\\n156\\n(8.8) p(A ∧ B) = p(A) × p(B)\\nThis is also written p(A, B) and is referred to as the joint probability of A and\\nB. For example, if the probability of throwing a six is 1\\n6, then the probability\\nof doing it twice in a row is 1\\n6 × 1\\n6 =\\n1\\n36. If the probability that the ﬁrst of\\ntwo words begins with a [p] is .0872 and the probability that the second word\\nbegins with a [p] is the same, then the probability that both begin with a [p]\\nis:\\n.0872 × .0872 = .0076\\n8.5\\nA Tricky Example\\nHere’s a tricky problem. Given n people in a room, what is the probability\\nthat at least two of those people will have the same birthday?\\nLet’s assume that there are four people in the room. The chances that\\nthey will have diﬀerent birthdays are:\\n365\\n365 × 364\\n365 × 363\\n365 × 362\\n365\\nTherefore the chances that at least two will have the same birthday (the\\nchances of any other conﬁguration) are:\\n1 − 365\\n365 × 364\\n365 × 363\\n365 × 362\\n365 = 1 −\\n365!\\n(365 − 4)! × 3654 = 1 − .9834 = .0165\\nThe general form of this is that the probability of at least two people\\nhaving the same birthday out of n people is:\\n(8.9) 1 −\\n365!\\n(365 − n)! × 365n\\nCHAPTER 8. PROBABILITY\\n157\\n8.6\\nConditional Probability\\nLet’s now consider the notion of conditional probability. The basic idea is\\nthat the probability of some event may be contingent on the probability of\\nsome other event. Consider, for example, the probability that some child\\nshould be a boy or girl. We can assume that the genders are equally likely.\\nGiven a family of two, where we know one child is a boy, what is the proba-\\nbility that the other child is a girl?\\nConsider the sample space; there are four equally probable situations.\\n(8.10)\\n1\\ngirl, girl\\n2\\ngirl, boy\\n3\\nboy, girl\\n4\\nboy, boy\\nGiven that one child is a boy, we know we are in one of the last three situa-\\ntions. Among those, the other child is a girl in two out of three cases. Thus\\nthe probability that the other child is a girl is 2\\n3 = .66.\\nThe conditional probability of A given B is written p(A|B). The deﬁnition\\nof conditional probability is the joint probability p(A, B) divided by the prior\\nprobability p(B).\\n(8.11) p(A|B) = p(A, B)\\np(B)\\nWe’ve already noted that the joint probability p(A, B) can also be written\\np(A ∧ B).\\nIn the example at hand, we take A to be the event where one child is a\\ngirl and B as the event where one child is a boy. We have p(A, B) = 2\\n4, the\\nprobability of the two kids being a boy and a girl. For the probability of a\\nboy, we have p(B) = 3\\n4. (This may seem odd, but follows from observing\\nthat in three out of the four possible situations, at least one child is a boy.)\\nThus:\\n(8.12) p(A|B) =\\n2\\n4\\n3\\n4\\n= 2\\n4 × 4\\n3 = 8\\n12 = 2\\n3 = .66\\nAs a language example, let’s consider the conditional probability that\\nthe second letter of a word is a vowel letter, given that the ﬁrst letter is a\\nCHAPTER 8. PROBABILITY\\n158\\nconsonant letter. Let’s represent this as p(V2|C1). We use the text of the\\npreceding paragraph (minus equations) for this. To do this, we must calculate\\np(C1) and p(C1, V2).\\nThere are 74 words in the paragraph, 45 of which begin with consonant\\nletters. Hence the probability of an initial consonant letter is:\\np(C1) = 45\\n74 = .608\\nThere are 26 words that begin with a consonant letter followed by a vowel let-\\nter. Hence the joint probability of an initial consonant letter and a following\\nvowel letter is:\\np(C1, V2) = 26\\n74 = .351\\nThe conditional probability of a vowel letter in second position based on a\\nconsonant letter in ﬁrst position is therefore:\\np(V2|C1) = p(C1, V2)\\np(C1)\\n= .351\\n.608 = .577\\nLet’s now consider some interesting properties that follow from this deﬁ-\\nnition. First, if events A and B are independent, then P(A|B) = p(A). This\\nfollows from the combination of independent events, given in (8.8) above.\\nThat is, if A and B are independent, then p(A, B) = p(A) × p(B).\\n(8.13) p(A|B) = p(A, B)\\np(B)\\n= p(A) × p(B)\\np(B)\\n= p(A)\\nThe ﬁrst step above is just the deﬁnition of conditional probability. The sec-\\nond step replaces p(A, B) with p(A) × p(B), since A and B are independent.\\nIn the last step, p(B) is cancelled from the numerator and denominator.\\nIt also follows that the conditional probability of an event A given all\\npossible events it could be conditional on is equivalent to p(A) directly. For\\nexample:\\n(8.14) p(A|B)p(B) + p(A|B)p(B) = p(A)\\nCHAPTER 8. PROBABILITY\\n159\\nHere we treat A in terms of some event B occurring and the same event not\\noccurring. This is, of course, equivalent to:\\n(8.15)\\np(A|B)p(B) + p(A|B)p(B)\\n=\\np(A,B)\\np(B) p(B) + p(A,B)\\np(B) p(B)\\n=\\np(A, B) + p(A, B)\\n=\\np(A)\\nThe ﬁrst step follows from the deﬁnition of conditional probability.\\nThe\\nsecond step cancels identical terms in the numerators and denominators.\\nFinally the last step follows from the fact that B and B exhaust the sample\\nspace.\\nThomas Bayes (1702–1761) proposed this equivalence, which follows from\\nwhat we have shown above:\\n(8.16) p(A|B) =\\np(B|A)p(A)\\np(B|A)p(A) + p(B|A)p(A)\\nThis is referred to as Bayes’ Law.\\nRecall that the denominator above is\\nequivalent to p(B). Hence Bayes’ Law is equivalent to:\\np(A|B) = p(B|A)p(A)\\np(B)\\nWe then multiply both sides by p(B) and get:\\np(A|B)p(B) = p(B|A)p(A)\\nFinally, from the deﬁnition of conditional probability, we know:\\np(A|B)p(B) = p(A, B) = p(B, A) = p(B|A)p(A)\\nBayes’ Law is a very simple algebraic manipulation. The key conceptual\\npoint is that we can use it to compute p(A|B) without know what p(B) is.\\nHere is a simple example showing the utility of Bayes’ Law (Isaac, 1995).\\nCHAPTER 8. PROBABILITY\\n160\\nImagine we have a test for some disease. The test has a false positive rate\\nof 5% (.05). This means that 5% of the people who get a positive response on\\nthe test actually do not have the disease. The distribution of the disease in\\nthe population is estimated at .3% (.003); 3 out of a thousand people in the\\ngeneral population actually have the disease. Assume as well that the true\\npositive rate is .95: if we give the test to 100 people who have the disease,\\n95 of them will get a positive response on the test. If you test positive for\\nthe disease, what are the chances you actually have it?\\nAssume the following assignments:\\nA\\n=\\n{tested person has disease}\\nB\\n=\\n{test result is positive}\\np(A)\\n=\\n.003\\np(A)\\n=\\n1 − p(A) = .997\\np(B|A)\\n=\\n.05\\np(B|A)\\n=\\n.95\\nThis gives us:\\np(A|B) =\\n(.95)(.003)\\n(.95)(.003) + (.05)(.997) ≈ .05\\nThe chance of a true positive, p(A|B) is only .05; if 100 people in the general\\npopulation test positive for the disease, only 5 of them will actually have\\nthe disease. This is a rather surprising result. Given the low occurrence of\\nthe disease and the relatively high false positive rate, the chances of a true\\npositive are fairly small.\\n8.7\\nDistributions\\nIn this section, we deﬁne the notion of a probability distribution, the range of\\nvalues that might occur in some space of outcomes. To do this, we need the\\nnotion of a random variable. Let’s deﬁne a random variable as follows.\\nDeﬁnition 20 (Random variable) A random variable is a function that\\nassigns to each outcome in a sample space a unique number. Those numbers\\nexhibit a probability distribution.\\nCHAPTER 8. PROBABILITY\\n161\\nConsider an example. The sample space is the number of heads we might\\nthrow in three consecutive throws of a coin.\\n(8.17)\\nthrow\\nnumber of heads\\n(H, H, H)\\n3\\n(H, T, H)\\n2\\n(H, H, T)\\n2\\n(H, T, T)\\n1\\n(T, H, H)\\n2\\n(T, T, H)\\n1\\n(T, H, T)\\n1\\n(T, T, T)\\n0\\nThere is one way to get three heads, three ways to get two heads, three\\nways to get one heads, and one way to get no heads. The probability of\\nthrowing a head on one throw is p(H) = .5. The probability of throwing\\na tails is then p(T) = 1 − p(H) = .5. Call the random variable X. The\\nprobability of throwing some number of heads can be computed as follows\\nfor each combination.\\n(8.18)\\np(X = 3) = 1 · p(H)3 · p(T)0 = 1 · (.5)3 · (.5)0 = 1 · .125 · 1 = .125\\np(X = 2) = 3 · p(H)2 · p(T)1 = 3 · (.5)2 · (.5)1 = 3 · .25 · .5 = .375\\np(X = 1) = 3 · p(H)1 · p(T)2 = 3 · (.5)1 · (.5)2 = 3 · .5 · .25 = .375\\np(X = 0) = 1 · p(H)0 · p(T)3 = 1 · (.5)0 · (.5)3 = 1 · 1 · .125 = .125\\nThe logic here is that you multiply together the number of possible combi-\\nnations, the chances of success, raised to the same number, and the chances\\nof failure, raised to the diﬀerence.\\nThis is a binomial random variable. It describes the distribution of “suc-\\ncesses” across some number of trials. If we have n trials and are interested in\\nthe likelihood of r successes, where the chance of success is p and the chance\\nof failure is q = 1 − p, then we have:\\n(8.19) p(X = r) =\\nn!\\nr!(n − r)! · prqn−r =\\n�\\nn\\nr\\n�\\n· prqn−r\\nCHAPTER 8. PROBABILITY\\n162\\nLet’s try this with a linguistic application. Imagine we have a vocabulary\\nof 100 words, 10 of which are verbs. All words are equally likely. All else\\nbeing equal, in a sentence of four words, what are the chances that two of\\nthose words are verbs?\\nWe assume n = 4, r = 2, p = .1, and q = .9. This gives us:\\n(8.20)\\np(X = 2)\\n=\\n4!\\n2!(4−2)! · .12 · .94−2\\n=\\n24\\n2·2 · .01 · .81\\n=\\n6 · .01 · .81\\n=\\n.0486\\nWe plot the whole distribution for four trials below.\\n(8.21)\\n0\\n1\\n2\\n3\\n4\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nnumber of successes\\nprobability\\nLet’s look at binomial distributions in graphical form. Here is the distri-\\nbution for heads and tails for three throws.\\nCHAPTER 8. PROBABILITY\\n163\\n(8.22)\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n0.15\\n0.20\\n0.25\\n0.30\\n0.35\\nnumber of successes\\nprobability\\nHere it is for 100 throws.\\n(8.23)\\n0\\n20\\n40\\n60\\n80\\n100\\n0.00\\n0.02\\n0.04\\n0.06\\n0.08\\nnumber of successes\\nprobability\\nAnd here it is for 1000 throws.\\nCHAPTER 8. PROBABILITY\\n164\\n(8.24)\\n0\\n200\\n400\\n600\\n800\\n1000\\n0.000\\n0.005\\n0.010\\n0.015\\n0.020\\n0.025\\nnumber of successes\\nprobability\\nThe binomial distribution is a discrete distribution. There are also con-\\ntinuous distributions, where there are an inﬁnite number of points along the\\ncurve. The most useful one of these is the normal distribution.8\\n(8.25) f(x) =\\n1\\n√\\n2πσ exp(−(x − µ)2\\n2σ2\\n)\\nThe variables to ﬁll in here are µ, the mean of the sample, and σ the standard\\ndeviation of the sample.9 Here is a picture of how this looks for a mean of 0\\nwith points evenly distributed from −5 to 5. (µ = 0 and σ = 2.93.)\\n8The expression exp(n) is equivalent to en.\\n9The standard deviation is deﬁned as:\\n��(x − µ)2, where x is each value in the\\ndistribution.\\nCHAPTER 8. PROBABILITY\\n165\\n(8.26)\\n−4\\n−2\\n0\\n2\\n4\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\ndata\\ndensity\\nThe normal distribution shows up in all sorts of contexts and is very widely\\nused in various statistical tests.\\n8.8\\nSummary\\nThis chapter has introduced the basics of probability theory. We began with\\na general characterization of the notion and proceeded to a mathematical\\none.\\nWe then considered some basic ideas of combinatorics, how to calculate\\nthe number of ways elements can be ordered or chosen.\\nWe presented some basic laws of probability theory, including Bayes’ Law,\\nand we deﬁned the notions of joint probability and conditional probability.\\nFinally, we very brieﬂy discussed the binomial and normal distributions.\\n8.9\\nExercises\\n1. What is the probability of throwing a 3 with one throw of one die?\\n2. What is the probability of throwing a 3 and then another 3 with two\\nthrows of a single die?\\nCHAPTER 8. PROBABILITY\\n166\\n3. What is the probability of throwing a 3 and then a 6 with two throws\\nof a single die?\\n4. What is the probability of throwing anything but a 3 and another 3\\nwith two throws of a single die?\\n5. What is the probability of not throwing a 3 at all in two throws of a\\nsingle die?\\n6. What is the probability of throwing at least one 3 with two throws of\\na single die?\\n7. What is the probability of drawing a jack in one draw from a full deck\\nof playing cards?\\n8. What is the probability of drawing four aces in four draws from a full\\ndeck of playing cards?\\n9. What is the probability of not drawing any aces in ﬁve draws from a\\nfull deck of playing cards?\\n10. How many people have to be in a room before the odds of at least two\\nof them having the same birthday is greater than 60%?\\n11. For the following questions, assume this inventory:\\na\\ne\\ni\\no\\nu\\np\\nt\\nk\\nb\\nd\\ng\\nm\\nn\\nN\\n(a) In a one-segment word, what are the odds of a word being [a]?\\n(b) In a three-segment word, what are the odds of a word being [tap]\\nvs. being [ppp]?\\n(c) On this story, are segments more like dice or more like cards?\\n12. Identify some phenomenon in language, other than those exempliﬁed in\\nthe chapter, that can be represented in terms of conditional probability.\\n13. Identify some phenomenon in language that can be seen as a random\\nvariable and give its empirical probability distribution (the distribution\\nbased on a sample of data).\\nChapter 9\\nProbabilistic Language Models\\nIn this chapter, we consider probability models that are speciﬁcally linguistic:\\nHidden Markov Models (HMMs) and Probabilistic Context-free Grammars\\n(PCFGs).\\nThese models can be used to directly encode probability values in linguis-\\ntic formalism. While such models have well-understood formal properties and\\nare widely used in computational research, they are extremely controversial\\nin the theory of language. It is a hotly debated question whether grammar\\nitself should incorporate probabilistic information.\\n9.1\\nThe Chain Rule\\nTo understand HMMs, we must ﬁrst understand the Chain Rule. The Chain\\nRule is one simple consequence of the deﬁnition of conditional probability:\\nthe joint probability of some set of events a1, a2, a3, a4 can also be expressed\\nas a ‘chain’ of conditional probabilities, e.g.:\\n(9.1) p(a1, a2, a3, a4) = p(a1)p(a2|a1)p(a3|a1, a2)p(a4|a1, a2, a3)\\nThis follows algebraically from the deﬁnition of conditional probability. If\\nwe substitute by the deﬁnition of conditional probability for each of the\\nconditional probabilities in the preceding equation and then cancel terms,\\nwe get the original joint probability.\\n167\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n168\\n(9.2)\\np(a1, a2, a3, a4)\\n=\\np(a1) × p(a2|a1) × p(a3|a1, a2) × p(a4|a1, a2, a3)\\n=\\np(a1) × p(a1,a2)\\np(a1) × p(a1,a2,a3)\\np(a1,a2) × p(a1,a2,a3,a4)\\np(a1,a2,a3)\\n=\\np(a1, a2, a3, a4)\\nNotice also that the chain rule can be used to express any dependency among\\nthe terms of the original joint probability. For example:\\n(9.3) p(a1, a2, a3, a4) = p(a4)p(a3|a4)p(a2|a3, a4)p(a1|a2, a3, a4)\\nHere we express the ﬁrst events as conditional on the later events, rather\\nthan vice versa. This array also follows algebraically:\\n(9.4)\\np(a1, a2, a3, a4)\\n=\\np(a4) × p(a3|a4) × p(a2|a3, a4) × p(a1|a2, a3, a4)\\n=\\np(a4) × p(a3,a4)\\np(a4) × p(a2,a3,a4)\\np(a3,a4) × p(a1,a2,a3,a4)\\np(a2,a3,a4)\\n=\\np(a1, a2, a3, a4)\\n9.2\\nN-gram models\\nAnother preliminary to both HMMs and PCGFs are N-gram models. These\\nare the very simplest kind of statistical language model. The basic idea is\\nto consider the structure of a text, corpus, or language as the probability\\nof diﬀerent words occurring alone or in sequence. The simplest model, the\\nunigram model, treats words in isolation.\\n9.2.1\\nUnigrams\\nTake a very simple text like the following:\\nPeter Piper picked a peck of pickled pepper.\\nWhere’s the pickled pepper that Peter Piper picked?\\nThere are sixteen words in this text.1 The word “Peter” occurs twice and\\nthus has a probability of\\n2\\n16 = .125. On the other hand, “peck” occurs only\\nonce and has the probability of\\n1\\n16 = .0625.\\n1We leave aside issues of text normalization, i.e. capitalization and punctuation.\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n169\\nThis kind of information can be used to judge the well-formedness of\\ntexts. The way this works is that we calculate the overall probability of the\\nnew text as a function of the individual probabilities of the words that occur\\nin it.\\nOn this view, the likelihood of a text like “Peter pickled pepper” would\\nbe a function of the probabilities of its parts: .125, .125, and .125. If we\\nassume the choice of each word is independent, then the probability of the\\nwhole string is the product of the independent words, in this case: .125 ×\\n.125 × .125 = .00195.\\nWe can make this more intuitive by considering a restricted hypothetical\\ncase. Imagine we have a language with only three words: {a, b, c}. Each\\nword has an equal likelihood of occurring: .33 each. There are nine possible\\ntwo-word texts, each having a probability of .1089. The range of possible\\ntexts thus exhibits a probability distribution; the probabilities of the set of\\npossible outcomes sums to 1 (9 × .1089).\\n(9.5)\\nxy\\np(x) × p(y)\\naa\\n.1089\\nab\\n.1089\\nac\\n.1089\\nba\\n.1089\\nbb\\n.1089\\nbc\\n.1089\\nca\\n.1089\\ncb\\n.1089\\ncc\\n.1089\\ntotal\\n1\\nThis is also the case if the individual words exhibit an asymmetric distri-\\nbution. Assume a vocabulary with the same words, but where the individual\\nword probabilities are diﬀerent, i.e. p(a) = .5, p(b) = .25, and p(c) = .25.\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n170\\n(9.6)\\nxy\\np(x) × p(y)\\naa\\n.25\\nab\\n.125\\nac\\n.125\\nba\\n.125\\nbb\\n.0625\\nbc\\n.0625\\nca\\n.125\\ncb\\n.0625\\ncc\\n.0625\\ntotal\\n1\\nThe way this model works is that the well-formedness of a text fragment is\\ncorrelated with its overall probability. Higher-probability text fragments are\\nmore well-formed than lower-probability texts, e.g. aa is a better exemplar\\nof L than cb.2\\nA major shortcoming of this model is that it makes no distinction among\\ntexts in terms of ordering. Thus this model cannot distinguish ab from ba.\\nThis, of course, is something that we as linguists think is essential, but we can\\nask the question whether it’s really necessary for computational applications.\\nA second major shortcoming is that the model makes the same predictions\\nat any point in the text. For example, in the second example above, the most\\nfrequent word is a. Imagine we want to predict the ﬁrst word of some text.\\nThe model above would tell us it should be a. Imagine we want to predict\\nthe nth word. Once again, the model predicts a. The upshot is that the\\ncurrent model predicts that a text should simply be composed of the most\\nfrequent item in the lexicon: aaa . . ..\\n9.2.2\\nBigrams\\nLet’s go on to consider a more complex model that captures some of the\\nordering restrictions that may occur in some language or text: bigrams. The\\nbasic idea behind higher-order N-gram models is to consider the probability\\nof a word occurring as a function of its immediate context. In a bigram\\nmodel, this context is the immediately preceding word:\\n2Note that we have made an interesting leap here.\\nWe are characterizing “well-\\nformedness” in terms of frequency. Is this fair?\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n171\\n(9.7) p(w1w2 . . . wi) = p(w1) × p(w2|w1) × . . . × p(wi|wi−1)\\nWe calculate conditional probability in the usual fashion. (We use absolute\\nvalue notation to denote the number of instances of some element.)\\n(9.8) p(wi|wi−1) = p(wi−1, wi)\\np(wi−1)\\n= |wi−1wi|\\n|wi−1|\\nIt’s important to notice that this is not the Chain Rule; here the context for\\nthe conditional probabilities is the immediate context, not the whole context.\\nAs a consequence we cannot return to the joint probability algebraically, as\\nwe did at the end of the preceding section (equation 9.2 on page 168). We\\nwill see that this limit on the context for the conditional probabilities in a\\nhigher-order N-gram model has important consequences for how we might\\nactually manipulate such models computationally.\\nLet’s work out the bigrams in our tongue twister (repeated here).\\nPeter Piper picked a peck of pickled pepper.\\nWhere’s the pickled pepper that Peter Piper picked?\\nThe frequencies of individual words are given in the table below:\\n(9.9)\\nWord\\nFrequency\\nPeter\\n2\\nPiper\\n2\\npicked\\n2\\na\\n1\\npeck\\n1\\nof\\n1\\npickled\\n2\\npepper\\n2\\nWhere’s\\n1\\nthe\\n1\\nthat\\n1\\nThe bigram frequencies are:\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n172\\n(9.10)\\nBigrams\\nBigram frequencies\\npicked a\\n1\\npepper that\\n1\\npeck of\\n1\\na peck\\n1\\npickled pepper\\n2\\nWhere s\\n1\\nPiper picked\\n2\\nthe pickled\\n1\\nPeter Piper\\n2\\nof pickled\\n1\\npepper Where\\n1\\nthat Peter\\n1\\ns the\\n1\\nCalculating conditional probabilities is then a straightforward matter of\\ndivision. For example, the conditional probability of “Piper” given “Peter”:\\n(9.11) p(Piper|Peter) = |Peter Piper|\\n|Peter|\\n= 2\\n2 = 1\\nHowever, the conditional probability of “Piper” given “a”:\\n(9.12) p(Piper|a) = |a Piper|\\n|a|\\n= 0\\n1 = 0\\nUsing conditional probabilities thus captures the fact that the likelihood of\\n“Piper” varies by preceeding context: it is more likely after “Peter” than\\nafter “a”.\\nThe bigram model addresses both of the problems we identiﬁed with the\\nunigram model above. First, recall that the unigram model could not distin-\\nguish among diﬀerent orderings. Diﬀerent orderings are distinguished in the\\nbigram model. Consider, for example, the diﬀerence between “Peter Piper”\\nand “Piper Peter”. We’ve already seen that the former has a conditional\\nprobability of 1. The latter, on the other hand:\\n(9.13) p(Peter|Piper) = |Piper Peter|\\n|Piper|\\n= 0\\n2 = 0\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n173\\nThe unigram model also made the prediction that the most well-formed\\ntext should be composed of repetitions of the highest-probability items. The\\nbigram model excludes this possibility as well.\\nContrast the conditional\\nprobability of “Peter Piper” with “Peter Peter”:\\n(9.14) p(Peter|Peter) = |Peter Peter|\\n|Peter|\\n= 0\\n2 = 0\\nThe bigram model presented doesn’t actually give a probability distri-\\nbution for a string or sentence without adding something for the edges of\\nsentences. To get a correct probability distribution for the set of possible\\nsentences generated from some text, we must factor in the probability that\\nsome word begins the sentence, and that some word ends the sentence. To\\ndo this, we deﬁne two markers that delimit all sentences: <s> and </s>.\\nThis transforms our text as follows.\\n<s> Peter Piper picked a peck of pickled pepper. </s>\\n<s> Where’s the pickled pepper that Peter Piper picked? </s>\\nThus the probability of some sentence w1w2 . . . wn is given as:\\n(9.15) p(w1|<s>) × p(w2|w1) × . . . × p(</s>|wn)\\nGiven the very restricted size of this text, and the conditional probabilities\\ndependent on the newly added edge markers, there are only several speciﬁc\\nways to add acceptable strings without reducing the probabilities to zero.\\nGiven the training text, these have the probabilities given below:\\n(9.16)\\np(Peter Piper picked)\\n=\\np(Peter|<s>) × p(Piper|Peter) × p(picked|Piper)×\\np(</s>|picked)\\n=\\n.5 × 1 × 1 × .5\\n=\\n.25\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n174\\n(9.17)\\np(Where’s the pickled pepper)\\n=\\np(Where’s|<s>) × p(the|Where’s) × p(pickled|the)×\\np(pepper|pickled) × p(</s>|pepper)\\n=\\n.5 × 1 × 1 × 1 × .5\\n=\\n.25\\n(9.18)\\np\\n�\\nWhere’s the pickled pepper that Peter\\nPiper picked a peck of pickled pepper\\n�\\n=\\np(Where’s|<s>) × p(the|Where’s) × p(pickled|the)×\\np(pepper|pickled) × p(that|pepper) × p(Peter|that)×\\np(Piper|Peter) × p(picked|Piper) × p(a|picked)×\\np(peck|a) × p(of|peck) × p(pickled|of)×\\np(peper|pickled) × p(</s>|pepper)\\n=\\n.5 × 1 × 1 × 1 × .5 × 1 × 1 × 1 × .5 × 1 × 1 × 1 × 1 × .5\\n=\\n.0625\\n(9.19)\\np\\n�\\nPeter Piper picked a peck of pickled pepper\\nthat Peter Piper picked\\n�\\n=\\np(Peter|<s>) × p(Piper|Peter) × p(picked|Piper)×\\np(a|picked) × p(peck|a) × p(of|peck)×\\np(pickled|of) × p(pepper|pickled) × p(that|pepper)×\\np(Peter|that) × p(Piper|Peter) × p(picked|Piper)×\\np(</s>|picked)\\n=\\n.5 × 1 × 1 × .5 × 1 × 1 × 1 × 1 × .5 × 1 × 1 × .5\\n=\\n.0625\\nNotice how the text that our bigram frequencies were calculated on only\\nleaves very restricted “choice” points. There are really only four:\\n1. What is the ﬁrst word of the sentence: “Peter” or “Where’s”?\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n175\\n2. What is the last word of the sentence: “pepper” or “picked”?\\n3. What follows the word “picked”: “a” or </s>”?\\n4. What follows the word “pepper”: “that” or </s>?\\nThe only sequences of words allowed are those sequences that occur in the\\ntraining text. However, even with these restrictions, this allows for sentences\\nof unbounded length; an example like 9.19 can be extended inﬁnitely.\\nNotice, however, that these restrictions do not guarantee that all sen-\\ntences produced in conformity with this language model will be grammatical\\n(by normal standards); example (9.18) is ungrammatical.\\nSince the only\\ndependencies captured in a bigram model are local/immediate ones, such\\nsentences will emerge as well-formed.3\\n9.2.3\\nHigher-order N-grams\\nN-gram models are not restricted to unigrams and bigrams; higher-order N-\\ngram models are also used. These higher-order models are characterized as we\\nwould expect. For example, a trigram model would view a text w1w2 . . . wn\\nas the product of a series of conditional probabilities:\\n(9.20) p(w1w2 . . . wn) = p(w1) × p(w2|w1) ×\\n�\\np(wn|wn−2wn−1)\\nThe advantage of higher-order N-gram models is that relationships span-\\nning longer sequences of words can be captured. For example, a trigram\\nmodel allows us to capture relationships between spans of three words long.\\nThe disadvantage of higher-order models is that much much larger train-\\ning texts are required and it is virtually impossible to avoid accidental gaps.\\n9.2.4\\nN-gram approximation\\nOne way to try to appreciate the success of N-gram language models is to use\\nthem to approximate text in a generative fashion. That is, we can compute\\nall the occurring N-grams over some text, and then use those N-grams to\\ngenerate new text.4\\n3Notice too, of course, that very simple sentences compatible with the vocabulary of\\nthis text would receive a null probability, e.g. “Where’s the pepper?” or “Peter picked a\\npeck”, etc.\\n4See Shannon (1951).\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n176\\nHere is an example. The following ten “sentences” were constructed in\\nthe following way. The frequencies of all words in the short story “White\\nFang” by Jack London were calculated. Then each sentence was constructed\\nby taking a random sample of those words whose frequency was above .002.\\n1. so her they dog no but there with in so\\n2. as not him they so he a that away then\\n3. be when dogs then up there he fang by a\\n4. on dogs out his and out he the away out\\n5. they then that on his into upon been their she\\n6. fang him this up dogs were he dogs no\\n7. by fang to into when him their when upon\\n8. up them at the was a been with there down\\n9. then down be him and on time one as into\\n10. as them be to for were that his at when\\nIt’s easy to see that, though the words used are relatively common, these are\\nhardly compelling as plausible sentences of English.\\nThe following ten examples were constructed by a similar technique using\\nbigrams extracted from the same text. Here the bigram frequencies exceed\\n.0002.\\n1. half feet on everywhere upon itself as strongest dog\\n2. far outweighed a hostile movement beside scott you know\\n3. judge unknown was because it toward personal life\\n4. everybody gave himself to cheapen himself oﬀ with\\n5. it bristled ﬁercely belligerent and save once and death\\n6. because they spoke matt should be used his tail\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n177\\n7. turn ’m time i counted the horse up live\\n8. beast that cautiously it discovered an act of plenty\\n9. fatty’s gone before had thought in matt argued stubbornly\\n10. what evil that night was ﬂying brands from weakness\\nNotice that this latter set of sentences is far more natural sounding.\\nFinally, here are ten examples constructed from trigrams taken from the\\nsame text. Here, trigram frequencies exceed .00002.\\n1. you was a thing to be the sign of\\n2. yet white fang with a wolf that knows enough\\n3. year when the master white fang he was not\\n4. void in his teeth to the ground white fang\\n5. upon the pack but he was no way of\\n6. two of them was the will of the bush\\n7. suspicious of them and the cub could not understand\\n8. rushed upon him with the dog with which to\\n9. put him into the world it was diﬀerent from\\n10. overture to white fang and the dogs and to\\nThese are even more natural-sounding.\\n9.3\\nHidden Markov Models\\nIn this section, we treat Hidden Markov Models (HMMs). These are inti-\\nmately associated with N-gram models and widely used as a computational\\nmodel for language processing.\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n178\\n9.3.1\\nMarkov Chains\\nTo understand Hidden Markov Models, we must ﬁrst understand Markov\\nchains. These are basically DFAs with associated probabilities. Each arc is\\nassociated with a probability value and all arcs leaving any particular node\\nmust exhibit a probability distribution, i.e. their values must range between 0\\nand 1, and must total 1. In addition, one node is designated as the “starting”\\nnode. There is no set of designated ﬁnal states. A simple example is given\\nbelow.\\n(9.21)\\ns1\\ns2\\na.3\\nb.7\\na.2\\nb.8\\nAs with an FSA, the machine moves from state to state following the arcs\\ngiven. The sequence of arc symbols denotes the string generated—accepted—\\nby the machine. The diﬀerence between a Markov chain and an FSA is that\\nin the former case there are probabilities associated with each arc. These\\nprobabilities are multiplied together to produce the probability that the ma-\\nchine might follow any particular sequence of arcs/states, generating the\\nappropriate string. For example, the probability of producing a single a and\\nreturning to s1 is .3; the probability of going from s1 to s2 and emitting a b is\\n.7. Hence the probability of ab is .3 × .7 = .21. The probability of producing\\nthe sequence ba, however, is .7 × .2 = .14. In the ﬁrst case we go from s1 to\\ns1 to s2; in the second case from s1 to s2 to s1.\\nThere are several key facts to note about a Markov chain.\\nFirst, as\\nwe stated above, the probabilities associated with the arcs from any state\\nexhibit a probability distribution. For example, in the Markov chain above,\\nthe arcs from s1 are .3 + .7 = 1. Second, Markov chains are analogous to\\na deterministic ﬁnite state automaton: there are no choices at any point\\neither in terms of start state or in terms of what arcs to follow. Thus there is\\nprecisely one and only one arc labeled with each symbol in the alphabet from\\neach state in the chain. Third, it follows that any symbol string uniquely\\ndetermines a state sequence. That is, for any particular string, there is one\\nand only one corresponding sequence of states through the chain.\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n179\\n9.3.2\\nHidden Markov Models\\nIt’s also possible to imagine a non-deterministic Markov chain; these are\\nreferred to as Hidden Markov Models (HMMs). Once we’ve introduced inde-\\nterminacy anywhere in the model, we can’t uniquely identify a state sequence\\nfor all strings. A legal string may be compatible with several paths; hence\\nthe state sequence is “hidden”. Given the model above, we can introduce\\nindeterminacy in several ways. First, we can allow for multiple start states.\\nThe example below is of this sort. Here each state is associated with a\\n“start” probability. (Those must, of course, exhibit a probability distribution\\nand sum to 1.) This means, that for any particular string, one must factor\\nin all possible start probabilities. For example, a string b could be gener-\\nated/accepted by starting in s1 and then following the arc to s2 (.4×.7 = .28).\\nWe could also start in s2 and then follow the arc back to s1 (.6 × .8 = .48).\\n(9.22)\\ns1.4\\ns2.6\\na.3\\nb.7\\na.2\\nb.8\\nThe overall probability of the string b is the sum of the probabilities of\\nall possible paths through the HMM: .28 + .48 = .76. Notice then that we\\ncannot really be sure which path may have been taken to get to b, though if\\nthe paths have diﬀerent probabilities then we can calculate the most likely\\npath. In the case at hand, this is s2 ⊢ s1.\\nIndeterminacy can also be introduced by adding multiple arcs from the\\nsame state for the same symbol. For example, the HMM below is a minimal\\nmodiﬁcation of the Markov chain above.\\n(9.23)\\ns1\\ns2\\na.3\\na.2\\na.2\\nb.8\\nb.5\\nConsider how this HMM deals with a string ab. Here only s1 is a legal start\\nstate. We can generate/accept a by either following the arc back to s1 (.3) or\\nby following the arc to s2 (.2). In the former case, we can get b by following\\nthe arc from s1 to s2. In the latter case, we can get b by following the arc\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n180\\nfrom s2 back to s1. This gives the following total probabilities for the two\\nstate sequences given.\\n(9.24)\\ns1 ⊢ s1 ⊢ s2 = .3 × .5 = .15\\ns1 ⊢ s2 ⊢ s1 = .2 × .8 = .16\\nThis results in an overall probability of .31 for ab. The second state sequence\\nis of course the more likely one since it exhibits a (slightly) higher overall\\nprobability.\\nA HMM can naturally include both extra arcs and multiple start states.\\nThe HMM below exempliﬁes.\\n(9.25)\\ns1.1\\ns2.9\\na.3\\na.2\\na.2\\nb.8\\nb.5\\nThis generally results in even more choices for any particular string. For\\nexample, the string ab can be produced with all the following sequences:\\n(9.26)\\ns1 ⊢ s1 ⊢ s2 = .1 × .3 × .5 = .015\\ns1 ⊢ s2 ⊢ s1 = .1 × .2 × .8 = .016\\ns2 ⊢ s1 ⊢ s2 = .9 × .2 × .5 = .09\\nThe overall probability is then .121.\\n9.3.3\\nFormal HMM properties\\nThere are a number of formal properties of Markov chains and HMMs that\\nare useful. One extremely important property is Limited Horizon:\\n(9.27) p(Xt+1 = sk|X1, . . . , Xt) = p(Xt+1 = sk|Xt)\\nThis says that the probability of some state sk given the set of states that\\nhave occurred before it is the same as the probability of that state given the\\nsingle state that occurs just before it.\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n181\\nAs a consequence of the structure of a HMM, there is a probability dis-\\ntribution over strings of any particular length:\\n(9.28) ∀n\\n�\\nw1n\\np(w1n) = 1\\nWhat this means is that when we sum the probabilities of all possible strings\\nof any length n, their total is 1.\\nFor example, consider the set of strings two characters long with respect\\nto the HMM in (9.25).\\n(9.29)\\nstring\\npath\\nprobability\\naa\\ns1 ⊢ s1 ⊢ s1\\n.1 × .3 × .3\\n=\\n.009\\ns1 ⊢ s1 ⊢ s2\\n.1 × .3 × .2\\n=\\n.006\\ns1 ⊢ s2 ⊢ s1\\n.1 × .2 × .2\\n=\\n.004\\ns2 ⊢ s1 ⊢ s1\\n.9 × .2 × .3\\n=\\n.054\\ns2 ⊢ s1 ⊢ s2\\n.9 × .2 × .2\\n=\\n.036\\nab\\ns1 ⊢ s1 ⊢ s2\\n.1 × .3 × .5\\n=\\n.015\\ns1 ⊢ s2 ⊢ s1\\n.1 × .2 × .8\\n=\\n.016\\ns2 ⊢ s1 ⊢ ss\\n.9 × .2 × .5\\n=\\n.09\\nba\\ns1 ⊢ s2 ⊢ s1\\n.1 × .5 × .2\\n=\\n.01\\ns2 ⊢ s1 ⊢ s1\\n.9 × .8 × .3\\n=\\n.216\\ns2 ⊢ s1 ⊢ s2\\n.9 × .8 × .2\\n=\\n.144\\nbb\\ns1 ⊢ s2 ⊢ s1\\n.1 × .5 × .8\\n=\\n.04\\ns1 ⊢ s2 ⊢ s1\\n.9 × .8 × .5\\n=\\n.36\\ntotal\\n=\\n1\\n9.3.4\\nBigrams and HMMs\\nIt is a straightforward matter to treat bigram models in terms of HMMs. In\\nfact, we can simplify our model considerably and still get the right eﬀect.\\nLet us assume that the name of each state corresponds to a symbol in the\\nalphabet.\\nAll arcs leading to some state s would thus be labeled s; for\\nconvenience, we leave this label oﬀ.\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n182\\nImagine now that we have a vocabulary of three words {a, b, c}.\\nWe\\nsimply create a HMM with a state for each item in the vocabulary and then\\narcs indicate the conditional probability of each bigram. Thus an arc from\\nstate si to state sj indicates the conditional probability: p(sj|si). An example\\nis given below. Here, for example, the conditional probability p(b|a) = .5. A\\ncomplete text given this model would get an overall probability in the usual\\nfashion.\\n(9.30)\\na\\n.1\\nb\\n.5\\nc\\n.4\\n.3\\n.3\\n.6\\n.2\\n.4\\n.4\\n9.3.5\\nHigher-order N-grams\\nHow would such a model be extended to higher-order N-grams?\\nAt ﬁrst\\nblush, we might think there’s a problem. After all, the limited horizon prop-\\nerty says that the history an HMM is sensitive to can be restricted to the\\nimmediately preceding state. A trigram model would appear to require more.\\nThis, however, doesn’t reckon with the assumption of a ﬁnite vocabulary\\n(albeit a large ﬁnite vocabulary). In the previous example, we took each state\\nas equivalent to a vocabulary item. To treat a trigram model, we must allow\\nfor states to be equivalent to both single words in the vocabulary and every\\npossible combination of words in the vocabulary. For example, to construct a\\nHMM for a trigram model for the same vocabulary as the previous examples,\\nwe would augment the model to include nine additional states representing\\neach combination of words. This is shown below. (Probabilities have been\\nleft oﬀ to enhance legibility.)\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n183\\n(9.31)\\na\\naa\\nab\\nac\\nba\\nbb\\nbc\\nca\\ncb\\ncc\\nb\\nc\\n9.4\\nProbabilistic Context-free Grammars\\nContext-free grammars can also be converted into statistical models: prob-\\nabilistic context-free grammars (PCFGs). In this section we consider the\\nstructure of these models.\\nA PCFG is a context-free grammar where each rule has an associated\\nprobability. In addition, the rules that expand any particular non-terminal\\nA must exhibit a probability distribution, i.e. their probabilities must sum\\nto one (Suppes, 1970).\\nLet’s exemplify this with a very simple grammar of a subset of English.\\nThis grammar produces transitive and intransitive sentences with two verbs\\nand two proper nouns.\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n184\\n(9.32)\\nS\\n→\\nNP VP\\nVP\\n→\\nV\\nVP\\n→\\nV NP\\nV\\n→\\nsees\\nV\\n→\\nhelps\\nNP\\n→\\nMindy\\nNP\\n→\\nMary\\nThis produces parse trees as follows for sentences like Mindy sees Mary.\\n(9.33)\\nS\\nNP\\nMindy\\nVP\\nV\\nsees\\nNP\\nMary\\nTo convert this into a probabilistic context-free grammar, we simply asso-\\nciate each production rule with a probability, such that—as noted above—the\\nprobabilities for all rules expanding any particular non-terminal sum to one.\\nA sample PCFG that satisﬁes these properties is given below. Notice how\\nthe single rule expanding S has a probability of 1, since there is only one such\\nrule. In all the other cases, there are two rules expanding each non-terminal\\nand the probabilities associated with each pair sum to 1.\\n(9.34)\\nS\\n→\\nNP VP\\n1\\nVP\\n→\\nV\\n.3\\nVP\\n→\\nV NP\\n.7\\nV\\n→\\nsees\\n.4\\nV\\n→\\nhelps\\n.6\\nNP\\n→\\nMindy\\n.2\\nNP\\n→\\nMary\\n.8\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n185\\nThe probability of some parse of a sentence is then the product of the\\nprobabilities of all the rules used.5 In the example at hand, Mindy sees Mary,\\nwe get: 1 × .2 × .7 × .4 × .8 = .0448. The probability of a sentence s, e.g.\\nany particular string of words, is the sum of the probabilities of all its parses\\nt1, t2, . . . , tn.\\n(9.35) p(s) =\\n�\\nj\\np(tj)p(s|tj)\\nIn the case at hand, there are no structural ambiguities; there is only one\\npossible structure for any acceptable sentence. Let’s consider another ex-\\nample, but one where there are structural ambiguities. The very simpliﬁed\\ncontext-free grammar for noun conjunction below has these properties.\\n(9.36)\\nNP\\n→\\nNP C NP\\n.4\\nNP\\n→\\nMary\\n.3\\nNP\\n→\\nMindy\\n.2\\nNP\\n→\\nMark\\n.1\\nC\\n→\\nand\\n1\\nThis grammar results in multiple trees for conjoined nouns like Mary\\nand Mindy and Mark as below. The ambiguity surrounds whether the ﬁrst\\ntwo conjuncts are grouped together or the last two.\\nThe same rules are\\nused in each parse, so the probability of either one of them is: .3 × .2 ×\\n.1 × 1 × 1 × .4 × .4 = .00096. The overall probability of the string is then\\n.00096 + .00096 = .00192.\\n5Note that if any rule is used more than once then it’s probability is factored in as\\nmany times as it is used.\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n186\\n(9.37)\\nNP\\nNP\\nMary\\nC\\nand\\nNP\\nNP\\nMindy\\nC\\nand\\nNP\\nMark\\n(9.38)\\nNP\\nNP\\nNP\\nMary\\nC\\nand\\nNP\\nMindy\\nC\\nand\\nNP\\nMark\\nNotice that the probability values get problematic when the PCFG is\\nrecursive, that is, when the grammar generates an inﬁnite number of sen-\\ntences. It then follows that at least some parses have inﬁnitely small values.\\nLet’s consider a toy grammar that allows recursive clausal embedding. This\\ngrammar allows optional recursion on S, but a very restricted vocabulary.\\n(9.39)\\np(S → NP VP)\\n=\\n1\\np(NP → N)\\n=\\n1\\np(N → John)\\n=\\n1\\np(V → knows)\\n=\\n1\\np(VP → V )\\n=\\n.6\\np(VP → V S)\\n=\\n.4\\nDo we get a probability distribution? Let’s look at a few examples:\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n187\\n(9.40)\\nS\\nNP\\nN\\nJohn\\nVP\\nV\\nknows\\nWe have p(John knows) = 1 × 1 × 1 × .6 × 1 = .6\\n(9.41)\\nS\\nNP\\nN\\nJohn\\nVP\\nV\\nknows\\nS\\nNP\\nN\\nJohn\\nVP\\nV\\nknows\\nWe have p(John knows John knows) = 1×1×1×.4×1×1×1×1×.6×1 = .24.\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n188\\n(9.42)\\nS\\nNP\\nN\\nJohn\\nVP\\nV\\nknows\\nS\\nNP\\nN\\nJohn\\nVP\\nV\\nknows\\nS\\nNP\\nN\\nJohn\\nVP\\nV\\nknows\\nWe have p(John knows John knows John knows) = 1 × 1 × 1 × .4 × 1 × 1 ×\\n1 × 1 × .4 × 1 × 1 × 1 × 1 × .6 = .096.\\nYou can see that every time we add a new clause, the probability value\\nof the new sentence is the previous sentence multiplied by .4. This gives us\\nthis chart showing the decrease in probabilities as a function of the number\\nof clauses.\\n(9.43)\\nclauses\\nprobability\\n1\\n.6\\n2\\n.24\\n3\\n.096\\n4\\n.0384\\n5\\n.0153\\n6\\n.0061\\nn\\n?\\nWe need some calculus to prove whether, in the limit, this totals to 1.\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n189\\nChi (1999) shows formally that PCFGs don’t exhibit a probability dis-\\ntribution. He argues as follows. First, assume a grammar with only two\\nrules:\\n(9.44)\\nS → S S\\nS → a\\nIf p(S → S S) = n then p(S → a) = 1−n. Chi argues as follows: “Let xh be\\nthe total probability of all parses with height no larger than h. Clearly, xh\\nis increasing. It is not hard to see that xh+l = (1 − n) + nx2\\nh. Therefore, the\\nlimit of xh, which is the total probability of all parses, is a solution for the\\nequation x = (1 − n) + nx2. The equation has two solutions: 1 and 1\\nn − 1. It\\ncan be shown that x is the smaller of the two: x = min(1, 1\\nn − 1). Therefore,\\nif n > 1\\n2, x < 1—an improper probability.”\\n9.5\\nSummary\\nThis chapter links the ideas about probability from the preceding chapter\\nwith the notions of grammar and automaton developed earlier.\\nWe began with the notion of N-gram modeling. The basic idea is that we\\ncan view a strings of words or symbols in terms of the likelihood that each\\nword might follow the preceding one or two words. While this is a stagger-\\ningly simple idea, it actually can go quite some distance toward describing\\nnatural language data.\\nWe next turned to Markov Chains and Hidden Markov Models (HMMs).\\nThese are probabilistically weighted ﬁnite state devices and can be used to\\nassociate probabilities with FSAs.\\nWe showed in particular how a simpliﬁed Markov Model could be used to\\nimplement N-grams, showing that those models could be treated in restrictive\\nterms.\\nFinally, we very brieﬂy considered probabilistic context-free grammars.\\n9.6\\nExercises\\n1. Give a hypothetical example showing how the limited horizon property\\nviolates the chain rule, how it does not give a probability distribution.\\nCHAPTER 9. PROBABILISTIC LANGUAGE MODELS\\n190\\n2. Construct a unigram model for a very limited text.\\n3. The N-gram approximation examples above did not treat the edges of\\nsentences diﬀerently from the interior of sentences. What eﬀect do you\\nexpect from this?\\n4. N-gram models fail to capture certain kinds of generalizations about\\nlanguage. Can you cite an example?\\n5. Theories of sentence structure are sometimes complicated to include\\nmovement, the possibility that words or phrases can change location in\\na parse tree. If you are familiar with such theories, how might they be\\nexpressed probabilistically?\\nBibliography\\nBerkley, Deborah Milam. 1994. The OCP and gradient data. Studies in the\\nLinguistic Sciences 24:59–72.\\nBird, Steven. 1995. Computational phonology. Cambridge: Cambridge Uni-\\nversity Press.\\nBird, Steven, and T. Mark Ellison. 1994. One-level phonology: autosegmental\\nrepresentations and rules as ﬁnite automata. Computational Linguistics\\n20:55–90.\\nChi, Zhiyi. 1999. Statistical properties of probabilistic context-free gram-\\nmars. Computational Linguistics 25:131–160.\\nColeman, John, and Janet Pierrehumbert. 1997.\\nStochastic phonological\\ngrammars and acceptability. In Computational phonology: Third meet-\\ning of the ACL special interest group in computational phonology, 49–56.\\nSomerset: Association for Computational Linguistics.\\nDavis, Stuart. 1989. Cross-vowel phonotactic constraints. Computational\\nLinguistics 15:109–111.\\nFidelholtz, James. 1975. Word frequency and vowel reduction in English. In\\nChicago Linguistic Society, volume 11, 200–213.\\nGreenberg, J. H., and J. J. Jenkins. 1964. Studies in the psychological cor-\\nrelates of the sound system of American English. Word 20:157–177.\\nHammond, Michael. 1997.\\nOT and prosody.\\nIn Optimality Theory, ed.\\nD. Archangeli and T. Langendoen, 33–58. Blackwell.\\n191\\nBIBLIOGRAPHY\\n192\\nHooper, Joan. 1976. Word frequency in lexical diﬀusion and the source of\\nmorphophonological change. In Current progress in historical linguistics,\\ned. W. Christie, 96–105. Amsterdam: North Holland.\\nHopcroft, J.E., and J.D. Ullman. 1979.\\nIntroduction to automata theory,\\nlanguages, and computation. Reading: Addison-Wesley.\\nIsaac, Richard. 1995. The pleasures of probability. New York: Springer.\\nKaplan, R., and M. Kay. 1994. Regular model of phonological rule systems.\\nComputational Linguistics 20:331–378.\\nKarttunen, L. 1983.\\nKIMMO: a general morphological processor.\\nTexas\\nLinguistic Forum 22:163–186.\\nKoskenniemi, Kimmo. 1984. A general computational model for word-form\\nrecognition and production. In COLING 84, 178–181.\\nLawson, Mark V. 2004.\\nFinite automata.\\nBoca Raton:\\nChapman &\\nHall/CRC.\\nOhala, John, and Manjari Ohala. 1986. Testing hypotheses regarding the\\npsychological manifestation of morpheme structure constraints. In Exper-\\nimental phonology, ed. John Ohala and Jeri Jaeger, 239–252. Orlando:\\nAcademic Press.\\nPartee, Barbara H., Alice ter Meulen, and Robert E. Wall. 1990. Mathemat-\\nical methods in linguistics. Dordrecht: Kluwer.\\nPullum, Geoﬀrey K. 1991. The great eskimo vocabulary hoax and other ir-\\nreverent essays on the study of language. Chicago: University of Chicago\\nPress.\\nSapir, Edward. 1921. Language. New York: Harcourt, Brace.\\nSch¨utze, Carson. 1996. The empirical base of linguistics: Grammaticality\\njudgments and linguistic methodology.\\nChicago: University of Chicago\\nPress.\\nShannon, C.E. 1951. Prediction and entropy of printed Ennglish. Bell System\\nTechnical Journal 30:50–64.\\nBIBLIOGRAPHY\\n193\\nShieber, S. M. 1985. Evidence against the context-freeness of natural lan-\\nguage. Linguistics and Philosophy 8:333–343.\\nSuppes, P. 1970. Probabilistic grammars for natural languages. Synth`ese\\n22:95–116.\\nWeaver, Warren. 1963. Lady luck. New York: Dover.\\nZamuner, Tania, LouAnn Gerken, and Michael Hammond. 2004. Phonotac-\\ntic probabilities in young children’s speech production. Journal of Child\\nLanguage 31:515–536.\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "how to get sentences only."
      ],
      "metadata": {
        "id": "jUtEsNoGREHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = page.getText('blocks')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djjmFGw0OqVW",
        "outputId": "a06a12b8-9874-44a1-f6cd-e8ea9474c8cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Deprecation: 'getText' removed from class 'Page' after v1.19 - use 'get_text'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "NTDhTQcPSLGY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(data,columns = ['a','b','c','d','e','f','g'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "VdVs-DKVSSFz",
        "outputId": "22001f3c-7324-4e2d-fac1-25c86d661b19"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-dd25b9ab-8984-4673-bc4c-60ada52664c3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "      <th>d</th>\n",
              "      <th>e</th>\n",
              "      <th>f</th>\n",
              "      <th>g</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>110.852997</td>\n",
              "      <td>92.264465</td>\n",
              "      <td>499.388062</td>\n",
              "      <td>106.878723</td>\n",
              "      <td>BIBLIOGRAPHY\\n193\\n</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>110.852997</td>\n",
              "      <td>126.672592</td>\n",
              "      <td>499.392090</td>\n",
              "      <td>153.084534</td>\n",
              "      <td>Shieber, S. M. 1985. Evidence against the cont...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>110.853226</td>\n",
              "      <td>165.525253</td>\n",
              "      <td>499.407379</td>\n",
              "      <td>191.937622</td>\n",
              "      <td>Suppes, P. 1970. Probabilistic grammars for na...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>110.853348</td>\n",
              "      <td>204.378342</td>\n",
              "      <td>382.905243</td>\n",
              "      <td>216.345398</td>\n",
              "      <td>Weaver, Warren. 1963. Lady luck. New York: Dov...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>110.853470</td>\n",
              "      <td>228.786545</td>\n",
              "      <td>499.402496</td>\n",
              "      <td>269.643372</td>\n",
              "      <td>Zamuner, Tania, LouAnn Gerken, and Michael Ham...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd25b9ab-8984-4673-bc4c-60ada52664c3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dd25b9ab-8984-4673-bc4c-60ada52664c3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dd25b9ab-8984-4673-bc4c-60ada52664c3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            a           b  ...  f  g\n",
              "0  110.852997   92.264465  ...  0  0\n",
              "1  110.852997  126.672592  ...  1  0\n",
              "2  110.853226  165.525253  ...  2  0\n",
              "3  110.853348  204.378342  ...  3  0\n",
              "4  110.853470  228.786545  ...  4  0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " sent= []\n",
        " for sentence in page.getText('blocks'):\n",
        "   sent.append(sentence[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHEcN2QPSimq",
        "outputId": "eb56d8fa-e602-444f-8879-7c3260e144e5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Deprecation: 'getText' removed from class 'Page' after v1.19 - use 'get_text'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "8DgXUK2hTNhX",
        "outputId": "80396420-bbbb-487f-83c3-cb338895b5ce"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Weaver, Warren. 1963. Lady luck. New York: Dover.\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2CkwCx5Tcy-",
        "outputId": "801a7ef1-215a-40b9-a30c-78758c73544b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(110.85299682617188,\n",
              "  92.26446533203125,\n",
              "  499.3880615234375,\n",
              "  106.87872314453125,\n",
              "  'BIBLIOGRAPHY\\n193\\n',\n",
              "  0,\n",
              "  0),\n",
              " (110.85299682617188,\n",
              "  126.67259216308594,\n",
              "  499.39208984375,\n",
              "  153.08453369140625,\n",
              "  'Shieber, S. M. 1985. Evidence against the context-freeness of natural lan-\\nguage. Linguistics and Philosophy 8:333–343.\\n',\n",
              "  1,\n",
              "  0),\n",
              " (110.85322570800781,\n",
              "  165.52525329589844,\n",
              "  499.4073791503906,\n",
              "  191.9376220703125,\n",
              "  'Suppes, P. 1970. Probabilistic grammars for natural languages. Synth`ese\\n22:95–116.\\n',\n",
              "  2,\n",
              "  0),\n",
              " (110.85334777832031,\n",
              "  204.3783416748047,\n",
              "  382.9052429199219,\n",
              "  216.34539794921875,\n",
              "  'Weaver, Warren. 1963. Lady luck. New York: Dover.\\n',\n",
              "  3,\n",
              "  0),\n",
              " (110.85346984863281,\n",
              "  228.7865447998047,\n",
              "  499.4024963378906,\n",
              "  269.64337158203125,\n",
              "  'Zamuner, Tania, LouAnn Gerken, and Michael Hammond. 2004. Phonotac-\\ntic probabilities in young children’s speech production. Journal of Child\\nLanguage 31:515–536.\\n',\n",
              "  4,\n",
              "  0)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "PDF reading ",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}