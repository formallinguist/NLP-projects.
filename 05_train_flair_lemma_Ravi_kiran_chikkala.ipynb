{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI1V9e9xpVlN"
      },
      "source": [
        "# Contextual based neural lemmatization with Flair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbtb9NusdN1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00470f62-870f-46c6-e295-57b9b1aa5fb7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQCUHxFt3GYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc2279ab-5df2-4d96-adfa-f79730e2dff6"
      },
      "source": [
        "!pip install flair"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flair\n",
            "  Downloading flair-0.11.3-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.9/401.9 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp38-cp38-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from flair) (1.0.2)\n",
            "Collecting conllu>=4.0\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting hyperopt>=0.2.7\n",
            "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from flair) (2022.6.2)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from flair) (1.13.1+cu116)\n",
            "Collecting pptree\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers>=4.0.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.8/dist-packages (from flair) (4.64.1)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 KB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.8/dist-packages (from flair) (2.8.2)\n",
            "Collecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.5/788.5 KB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from flair) (3.6.0)\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from flair) (9.0.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from flair) (4.9.2)\n",
            "Collecting janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from flair) (0.8.10)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.8/dist-packages (from flair) (3.2.2)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gdown==4.4.0 in /usr/local/lib/python3.8/dist-packages (from flair) (4.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair) (3.9.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair) (2.25.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from bpemb>=0.3.2->flair) (1.21.6)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from deprecated>=1.2.4->flair) (1.14.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.4.0->flair) (6.3.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.4.0->flair) (1.7.3)\n",
            "Collecting py4j\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 KB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair) (3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair) (0.16.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair) (2.2.1)\n",
            "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->flair) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->flair) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch!=1.8,>=1.5.0->flair) (4.5.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.0.0->flair) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.0.0->flair) (6.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->flair) (0.2.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.13.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.4.0->flair) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.4.0->flair) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.4.0->flair) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.4.0->flair) (2022.12.7)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.4.0->flair) (1.7.1)\n",
            "Building wheels for collected packages: mpld3, sqlitedict, langdetect, pptree, overrides\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=ffe02696347fb1741a76fed6fb17bbd6787037a5d5ebd7e87c2a8a617ad5c647\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/9f/9d/d806a20bd97bc7076d724fa3e69fa5be61836ba16b2ffa6126\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16869 sha256=4317204e4bf685ac8a41c184e4eb0766988e87b916259d84ac76250514d8050f\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/c6/16/46e174009277f9bccdaa7215a243939d2f70180804b249bf3a\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=4b23ccb3e1a7970379f2a72c9cdcced99d49e0f34d2750c1235b3fb4687101c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4629 sha256=43039f949347ba160a6bd4ab89977c8982400f17e929a339125a1e4679551203\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/8b/30/5b20240d3d13a9dfafb6a6dd49d1b541c86d39812cb3690edf\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=6d7a3bace65a4730476a20fcab6d0eeede578d372f3842b0ac83440c703f0895\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/4f/72/28857f75625b263e2e3f5ab2fc4416c0a85960ac6485007eaa\n",
            "Successfully built mpld3 sqlitedict langdetect pptree overrides\n",
            "Installing collected packages: tokenizers, sqlitedict, sentencepiece, py4j, pptree, overrides, mpld3, janome, segtok, langdetect, importlib-metadata, ftfy, deprecated, conllu, wikipedia-api, konoha, hyperopt, huggingface-hub, transformers, bpemb, flair\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.0.0\n",
            "    Uninstalling importlib-metadata-6.0.0:\n",
            "      Successfully uninstalled importlib-metadata-6.0.0\n",
            "  Attempting uninstall: hyperopt\n",
            "    Found existing installation: hyperopt 0.1.2\n",
            "    Uninstalling hyperopt-0.1.2:\n",
            "      Successfully uninstalled hyperopt-0.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.4.1 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "gym 0.25.2 requires importlib-metadata>=4.8.0; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bpemb-0.3.4 conllu-4.5.2 deprecated-1.2.13 flair-0.11.3 ftfy-6.1.1 huggingface-hub-0.12.1 hyperopt-0.2.7 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 mpld3-0.3 overrides-3.1.0 pptree-3.1 py4j-0.10.9.7 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-2.1.0 tokenizers-0.13.2 transformers-4.26.1 wikipedia-api-0.5.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgcDJP7mcO2h"
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import ColumnCorpus\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, FlairEmbeddings, CharacterEmbeddings\n",
        "from typing import List"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj40_Wj05yre"
      },
      "source": [
        "# ASSIGNMENT 1\n",
        "\n",
        "Please inspect the data we provided (train, dev and test sets available): 'content/drive/My Drive/Colab Notebooks/2023-ILTAPP/datasets/sigmorphon2019/'\n",
        "\n",
        "+ TODO Try to spot the differences with the Lab 04-ner-training-tagging_TODO.ipynb.\n",
        "+ TODO: define the columns required to instantiate the ColumnCorpus and the tag we want to predict.\n",
        "+ TODO: use downsampling to speed up training.\n",
        "+ TODO: print the total number of tags to predict in the corpus and check how this changes as you change the downsampling.\n",
        "+ TODO: experiment with Classic Embeddings, CharacterEmbeddings and Flair Embeddings for improving accuracy performance (also with number of epochs).\n",
        "  + HINT: Do not combine more than two types of embeddings to avoid running out RAM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wvgqo8ACc7DK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7ea6eb4-ba18-4a65-9b06-fb134a08d88a"
      },
      "source": [
        "text_path= \"/content/drive/MyDrive/NLP_Applications_1/DATA/2023-ILTAPP-20230203T201734Z-001/2023-ILTAPP/datasets/sigmorphon2019\"\n",
        "\n",
        "\n",
        "#  get the corpus into a ColumnCorpus object\n",
        "from flair.data import Corpus\n",
        "from flair.datasets import ColumnCorpus\n",
        "\n",
        "columns = {0:'text', 1:'pos', 2:'lemma'}\n",
        "\n",
        "corpus: Corpus = ColumnCorpus(text_path, columns,\n",
        "                              train_file='en-ewt-ud-ses-true-train.tsv',\n",
        "                              test_file='en-ewt-ud-ses-true-test.tsv',\n",
        "                              dev_file='en-ewt-ud-ses-true-dev.tsv').downsample(0.3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:18:35,594 Reading data from /content/drive/MyDrive/NLP_Applications_1/DATA/2023-ILTAPP-20230203T201734Z-001/2023-ILTAPP/datasets/sigmorphon2019\n",
            "2023-02-21 16:18:35,597 Train: /content/drive/MyDrive/NLP_Applications_1/DATA/2023-ILTAPP-20230203T201734Z-001/2023-ILTAPP/datasets/sigmorphon2019/en-ewt-ud-ses-true-train.tsv\n",
            "2023-02-21 16:18:35,599 Dev: /content/drive/MyDrive/NLP_Applications_1/DATA/2023-ILTAPP-20230203T201734Z-001/2023-ILTAPP/datasets/sigmorphon2019/en-ewt-ud-ses-true-dev.tsv\n",
            "2023-02-21 16:18:35,601 Test: /content/drive/MyDrive/NLP_Applications_1/DATA/2023-ILTAPP-20230203T201734Z-001/2023-ILTAPP/datasets/sigmorphon2019/en-ewt-ud-ses-true-test.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkY2N8eFd50c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "608e39ae-302c-4670-dfe1-090a61376896"
      },
      "source": [
        "#what tag do we want to predict?\n",
        "label_type = 'lemma'\n",
        "\n",
        "# make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_label_dictionary(label_type=label_type)\n",
        "print(tag_dictionary.idx2item)\n",
        "# print total number of tags\n",
        "print(len(tag_dictionary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:19:06,200 Computing label dictionary. Progress:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3989it [00:00, 24618.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:19:06,404 Dictionary created for label 'lemma' with 142 values: ↓0;d¦+ (seen 47582 times), ↑0¦↓1;d¦+ (seen 4216 times), ↓0;d¦-+ (seen 3426 times), ↓0;abe (seen 1324 times), ↓0;d¦--+ (seen 949 times), ↓0;d¦---+ (seen 700 times), ↓0;d--+b¦+ (seen 384 times), ↑0¦↓-1;d¦+ (seen 324 times), ↓0;d¦-+v+e+ (seen 305 times), ↓0;d¦---+e+ (seen 266 times), ↓0;d¦-+o→+ (seen 232 times), ↓0;d¦--+e+ (seen 200 times), ↓0;d¦--+y+ (seen 190 times), ↑0¦↓1;ai (seen 177 times), ↓0;d¦---+y+ (seen 156 times), ↓0;awe (seen 151 times), ↓0;d-+b¦--+ (seen 123 times), ↓0;d¦-+y+ (seen 113 times), ↓0;d¦--+o+ (seen 81 times), ↓0;d¦----+ (seen 68 times)\n",
            "[b'<unk>', b'\\xe2\\x86\\x930;d\\xc2\\xa6+', b'\\xe2\\x86\\x910\\xc2\\xa6\\xe2\\x86\\x931;d\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+', b'\\xe2\\x86\\x930;abe', b'\\xe2\\x86\\x930;d\\xc2\\xa6--+', b'\\xe2\\x86\\x930;d\\xc2\\xa6---+', b'\\xe2\\x86\\x930;d--+b\\xc2\\xa6+', b'\\xe2\\x86\\x910\\xc2\\xa6\\xe2\\x86\\x93-1;d\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+v+e+', b'\\xe2\\x86\\x930;d\\xc2\\xa6---+e+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+o\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;d\\xc2\\xa6--+e+', b'\\xe2\\x86\\x930;d\\xc2\\xa6--+y+', b'\\xe2\\x86\\x910\\xc2\\xa6\\xe2\\x86\\x931;ai', b'\\xe2\\x86\\x930;d\\xc2\\xa6---+y+', b'\\xe2\\x86\\x930;awe', b'\\xe2\\x86\\x930;d-+b\\xc2\\xa6--+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+y+', b'\\xe2\\x86\\x930;d\\xc2\\xa6--+o+', b'\\xe2\\x86\\x930;d\\xc2\\xa6----+', b'\\xe2\\x86\\x930;d+s\\xc2\\xa6-+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+k\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+e\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;d-+w+i\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+e\\xe2\\x86\\x92-+l+', b'\\xe2\\x86\\x930;d-+h+a\\xc2\\xa6+', b\"\\xe2\\x86\\x930;d+'\\xc2\\xa6+\", b'\\xe2\\x86\\x930;d\\xc2\\xa6--+a\\xe2\\x86\\x92+e+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+d+', b'\\xe2\\x86\\x930;ago', b'\\xe2\\x86\\x930;d\\xe2\\x86\\x92-+o\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+a\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;d\\xc2\\xa6+e\\xe2\\x86\\x92-+', b'\\xe2\\x86\\x930;d\\xc2\\xa6+n+', b'\\xe2\\x86\\x930;d\\xe2\\x86\\x92--+i\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-----+i+n+k+', b'\\xe2\\x86\\x930;d\\xe2\\x86\\x92-+i\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+i\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;d\\xc2\\xa6--+a+v+e+', b'\\xe2\\x86\\x910\\xc2\\xa6\\xe2\\x86\\x931\\xc2\\xa6\\xe2\\x86\\x912\\xc2\\xa6\\xe2\\x86\\x933;d\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+i+l+l+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+i\\xe2\\x86\\x92\\xe2\\x86\\x92+', b'\\xe2\\x86\\x910\\xc2\\xa6\\xe2\\x86\\x931\\xc2\\xa6\\xe2\\x86\\x912\\xc2\\xa6\\xe2\\x86\\x93-2;d\\xc2\\xa6+', b'\\xe2\\x86\\x930;d-+w+o+u+l\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6+o\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;a\"', b'\\xe2\\x86\\x930;d\\xc2\\xa6+s+', b\"\\xe2\\x86\\x930;d-+'\\xc2\\xa6+\", b'\\xe2\\x86\\x930;d\\xc2\\xa6+e\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+e+a\\xe2\\x86\\x92-+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+f\\xe2\\x86\\x92-+', b'\\xe2\\x86\\x910\\xc2\\xa6\\xe2\\x86\\x931\\xc2\\xa6\\xe2\\x86\\x915\\xc2\\xa6\\xe2\\x86\\x936;d\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+e+', b'\\xe2\\x86\\x930;d\\xc2\\xa6--+e+e+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-\\xe2\\x86\\x92-+', b'\\xe2\\x86\\x930;d+h+a\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6--\\xe2\\x86\\x92+a+k+', b'\\xe2\\x86\\x930;d\\xc2\\xa6--+i+n\\xe2\\x86\\x92--+', b'\\xe2\\x86\\x910\\xc2\\xa6\\xe2\\x86\\x931\\xc2\\xa6\\xe2\\x86\\x912\\xc2\\xa6\\xe2\\x86\\x93-1;d\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6---+f+', b'\\xe2\\x86\\x930;a-', b'\\xe2\\x86\\x930;ahave', b'\\xe2\\x86\\x930;d\\xc2\\xa6-\\xe2\\x86\\x92---+y+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-\\xe2\\x86\\x92--+e+', b\"\\xe2\\x86\\x930;a''\", b'\\xe2\\x86\\x930;ato', b'\\xe2\\x86\\x910\\xc2\\xa6\\xe2\\x86\\x931\\xc2\\xa6\\xe2\\x86\\x916\\xc2\\xa6\\xe2\\x86\\x93-6;d\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6+f+', b'\\xe2\\x86\\x910\\xc2\\xa6\\xe2\\x86\\x931\\xc2\\xa6\\xe2\\x86\\x913\\xc2\\xa6\\xe2\\x86\\x934;d\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6+t+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+r+', b'\\xe2\\x86\\x930;d\\xc2\\xa6+a\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;d\\xc2\\xa6--\\xe2\\x86\\x92-+a+l+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+u\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;d\\xc2\\xa6--+a+n\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;aof', b'\\xe2\\x86\\x930;d\\xc2\\xa6--\\xe2\\x86\\x92-+a+k+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+i+t+h+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+e\\xe2\\x86\\x92+a+u+s+e+', b'\\xe2\\x86\\x930;d\\xc2\\xa6+l\\xe2\\x86\\x92+', b'\\xe2\\x86\\x910\\xc2\\xa6\\xe2\\x86\\x931\\xc2\\xa6\\xe2\\x86\\x914\\xc2\\xa6\\xe2\\x86\\x935;d\\xc2\\xa6+', b'\\xe2\\x86\\x930;d+w+o+u+l\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+o\\xe2\\x86\\x92\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;d\\xc2\\xa6+e+', b'\\xe2\\x86\\x930;d\\xc2\\xa6+o\\xe2\\x86\\x92\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+o+', b'\\xe2\\x86\\x930;d\\xc2\\xa6---\\xe2\\x86\\x92+c+h+', b'\\xe2\\x86\\x930;d\\xc2\\xa6+o\\xe2\\x86\\x92\\xe2\\x86\\x92-+', b'\\xe2\\x86\\x910\\xc2\\xa6\\xe2\\x86\\x931\\xc2\\xa6\\xe2\\x86\\x914\\xc2\\xa6\\xe2\\x86\\x93-4;d\\xc2\\xa6+', b'\\xe2\\x86\\x930;d-+b\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xe2\\x86\\x92-\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6+o+r+', b'\\xe2\\x86\\x930;d\\xc2\\xa6+e+n+e+f+i+t+', b'\\xe2\\x86\\x930;d\\xc2\\xa6+a\\xe2\\x86\\x92\\xe2\\x86\\x92\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-----+e+e+k+', b'\\xe2\\x86\\x930;d\\xe2\\x86\\x92-+a\\xc2\\xa6+', b'\\xe2\\x86\\x930\\xc2\\xa6\\xe2\\x86\\x911\\xc2\\xa6\\xe2\\x86\\x932;d\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-\\xe2\\x86\\x92+l\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+o\\xe2\\x86\\x92\\xe2\\x86\\x92-\\xe2\\x86\\x92\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;d\\xc2\\xa6+a\\xe2\\x86\\x92\\xe2\\x86\\x92-\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;d\\xc2\\xa6--\\xe2\\x86\\x92+a+r+', b'\\xe2\\x86\\x930;d\\xc2\\xa6--+o+o\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;d\\xc2\\xa6+o\\xe2\\x86\\x92+g+h+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+n+', b'\\xe2\\x86\\x930;d\\xc2\\xa6---\\xe2\\x86\\x92-+t+', b'\\xe2\\x86\\x910\\xc2\\xa6\\xe2\\x86\\x933;d\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6--+o+u+s\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+i+n+g+', b'\\xe2\\x86\\x930;d\\xc2\\xa6+e+r\\xe2\\x86\\x92+u+s+', b'\\xe2\\x86\\x930;d\\xe2\\x86\\x92-+i+t+h\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6+i+t+h+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+g+', b'\\xe2\\x86\\x910\\xc2\\xa6\\xe2\\x86\\x931\\xc2\\xa6\\xe2\\x86\\x914\\xc2\\xa6\\xe2\\x86\\x93-3;d\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-\\xe2\\x86\\x92+u\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+i+t+h\\xe2\\x86\\x92+u+t+', b'\\xe2\\x86\\x910\\xc2\\xa6\\xe2\\x86\\x931\\xc2\\xa6\\xe2\\x86\\x913\\xc2\\xa6\\xe2\\x86\\x93-3;d\\xc2\\xa6+', b'\\xe2\\x86\\x910\\xc2\\xa6\\xe2\\x86\\x931\\xc2\\xa6\\xe2\\x86\\x912\\xc2\\xa6\\xe2\\x86\\x933\\xc2\\xa6\\xe2\\x86\\x91-3\\xc2\\xa6\\xe2\\x86\\x93-2;d\\xc2\\xa6+', b'\\xe2\\x86\\x930;d-\\xc2\\xa6+f+', b'\\xe2\\x86\\x930\\xc2\\xa6\\xe2\\x86\\x914\\xc2\\xa6\\xe2\\x86\\x93-3;d\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xe2\\x86\\x92\\xe2\\x86\\x92+r\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-\\xe2\\x86\\x92\\xe2\\x86\\x92\\xe2\\x86\\x92\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;d+e\\xc2\\xa6-+', b'\\xe2\\x86\\x930;d\\xc2\\xa6--+i\\xe2\\x86\\x92+e+', b'\\xe2\\x86\\x930;d+a\\xc2\\xa6+d+', b'\\xe2\\x86\\x930;d\\xe2\\x86\\x92-\\xe2\\x86\\x92+i\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+r\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930;d\\xc2\\xa6+.+', b'\\xe2\\x86\\x930;d\\xe2\\x86\\x92+e\\xc2\\xa6-+', b'\\xe2\\x86\\x910\\xc2\\xa6\\xe2\\x86\\x934;d\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xe2\\x86\\x92\\xe2\\x86\\x92-\\xe2\\x86\\x92+a\\xc2\\xa6+', b'\\xe2\\x86\\x930;d+c\\xc2\\xa6+n+', b'\\xe2\\x86\\x910\\xc2\\xa6\\xe2\\x86\\x931\\xc2\\xa6\\xe2\\x86\\x91-3\\xc2\\xa6\\xe2\\x86\\x93-1;d\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xe2\\x86\\x92+h\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6+u+e+', b'\\xe2\\x86\\x930;d\\xc2\\xa6+o+', b'\\xe2\\x86\\x930;d+w+i\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-\\xe2\\x86\\x92\\xe2\\x86\\x92+', b'\\xe2\\x86\\x930\\xc2\\xa6\\xe2\\x86\\x914\\xc2\\xa6\\xe2\\x86\\x93-4;d\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xe2\\x86\\x92--\\xc2\\xa6+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-\\xe2\\x86\\x92+d+', b'\\xe2\\x86\\x930;d\\xc2\\xa6-+n\\xe2\\x86\\x92+']\n",
            "142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB172bjKeBlA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75b9c21e-3a34-4677-f997-3456d71817cb"
      },
      "source": [
        "# play with embeddings and check performance; which one is the best combination?\n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "\n",
        "    CharacterEmbeddings(),\n",
        "    #WordEmbeddings('en'),\n",
        "    FlairEmbeddings('en-forward', chars_per_chunk=128),\n",
        "   # FlairEmbeddings('en-backward', chars_per_chunk=128),\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:23:10,039 https://flair.informatik.hu-berlin.de/resources/characters/common_characters not found in cache, downloading to /tmp/tmpjz477hh0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2887/2887 [00:00<00:00, 992781.47B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:23:10,428 copying /tmp/tmpjz477hh0 to cache at /root/.flair/datasets/common_characters\n",
            "2023-02-21 16:23:10,434 removing temp file /tmp/tmpjz477hh0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:23:19,582 https://flair.informatik.hu-berlin.de/resources/embeddings/flair/news-forward-0.4.1.pt not found in cache, downloading to /tmp/tmpn2u0fxzw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 73034624/73034624 [00:03<00:00, 18426501.18B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:23:23,937 copying /tmp/tmpn2u0fxzw to cache at /root/.flair/embeddings/news-forward-0.4.1.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:23:24,009 removing temp file /tmp/tmpn2u0fxzw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-NkS26pePfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d44d14a-f868-4657-fbb1-ba4b04382aeb"
      },
      "source": [
        "# initialize sequence tagger\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
        "                                        embeddings=embeddings,\n",
        "                                        tag_dictionary=tag_dictionary,\n",
        "                                        tag_type=label_type\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:23:38,695 SequenceTagger predicts: Dictionary with 142 tags: <unk>, ↓0;d¦+, ↑0¦↓1;d¦+, ↓0;d¦-+, ↓0;abe, ↓0;d¦--+, ↓0;d¦---+, ↓0;d--+b¦+, ↑0¦↓-1;d¦+, ↓0;d¦-+v+e+, ↓0;d¦---+e+, ↓0;d¦-+o→+, ↓0;d¦--+e+, ↓0;d¦--+y+, ↑0¦↓1;ai, ↓0;d¦---+y+, ↓0;awe, ↓0;d-+b¦--+, ↓0;d¦-+y+, ↓0;d¦--+o+, ↓0;d¦----+, ↓0;d+s¦-+, ↓0;d¦-+k→+, ↓0;d¦-+e→+, ↓0;d-+w+i¦+, ↓0;d¦-+e→-+l+, ↓0;d-+h+a¦+, ↓0;d+'¦+, ↓0;d¦--+a→+e+, ↓0;d¦-+d+, ↓0;ago, ↓0;d→-+o¦+, ↓0;d¦-+a→+, ↓0;d¦+e→-+, ↓0;d¦+n+, ↓0;d→--+i¦+, ↓0;d¦-----+i+n+k+, ↓0;d→-+i¦+, ↓0;d¦-+i→+, ↓0;d¦--+a+v+e+, ↑0¦↓1¦↑2¦↓3;d¦+, ↓0;d¦-+i+l+l+, ↓0;d¦-+i→→+, ↑0¦↓1¦↑2¦↓-2;d¦+, ↓0;d-+w+o+u+l¦+, ↓0;d¦+o→+, ↓0;a\", ↓0;d¦+s+, ↓0;d-+'¦+, ↓0;d¦+e→+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt3MGEkZeX2b"
      },
      "source": [
        "# initialize trainer\n",
        "from flair.trainers import ModelTrainer\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23a9h5ipeetW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a17cb827-5a02-4762-c036-b1925642e214"
      },
      "source": [
        "trainer.train('/content/drive/MyDrive/NLP_Applications_1/DATA/2023-ILTAPP-20230203T201734Z-001/2023-ILTAPP/datasets/sigmorphon2019', mini_batch_size=16, train_with_dev=False, max_epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:24:55,278 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:24:55,280 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): CharacterEmbeddings(\n",
            "      (char_embedding): Embedding(275, 25)\n",
            "      (char_rnn): LSTM(25, 25, bidirectional=True)\n",
            "    )\n",
            "    (list_embedding_1): FlairEmbeddings(\n",
            "      (lm): LanguageModel(\n",
            "        (drop): Dropout(p=0.05, inplace=False)\n",
            "        (encoder): Embedding(300, 100)\n",
            "        (rnn): LSTM(100, 2048)\n",
            "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=2098, out_features=2098, bias=True)\n",
            "  (rnn): LSTM(2098, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=144, bias=True)\n",
            "  (loss_function): ViterbiLoss()\n",
            "  (crf): CRF()\n",
            ")\"\n",
            "2023-02-21 16:24:55,283 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:24:55,286 Corpus: \"Corpus: 3989 train + 499 dev + 499 test sentences\"\n",
            "2023-02-21 16:24:55,290 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:24:55,292 Parameters:\n",
            "2023-02-21 16:24:55,293  - learning_rate: \"0.100000\"\n",
            "2023-02-21 16:24:55,294  - mini_batch_size: \"16\"\n",
            "2023-02-21 16:24:55,295  - patience: \"3\"\n",
            "2023-02-21 16:24:55,296  - anneal_factor: \"0.5\"\n",
            "2023-02-21 16:24:55,297  - max_epochs: \"20\"\n",
            "2023-02-21 16:24:55,299  - shuffle: \"True\"\n",
            "2023-02-21 16:24:55,300  - train_with_dev: \"False\"\n",
            "2023-02-21 16:24:55,301  - batch_growth_annealing: \"False\"\n",
            "2023-02-21 16:24:55,302 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:24:55,303 Model training base path: \"/content/drive/MyDrive/NLP_Applications_1/DATA/2023-ILTAPP-20230203T201734Z-001/2023-ILTAPP/datasets/sigmorphon2019\"\n",
            "2023-02-21 16:24:55,304 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:24:55,305 Device: cuda:0\n",
            "2023-02-21 16:24:55,306 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:24:55,307 Embeddings storage mode: cpu\n",
            "2023-02-21 16:24:55,308 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:25:04,333 epoch 1 - iter 25/250 - loss 2.03533526 - samples/sec: 44.36 - lr: 0.100000\n",
            "2023-02-21 16:25:13,280 epoch 1 - iter 50/250 - loss 1.67728911 - samples/sec: 44.73 - lr: 0.100000\n",
            "2023-02-21 16:25:22,207 epoch 1 - iter 75/250 - loss 1.49618929 - samples/sec: 44.83 - lr: 0.100000\n",
            "2023-02-21 16:25:29,109 epoch 1 - iter 100/250 - loss 1.38611648 - samples/sec: 58.00 - lr: 0.100000\n",
            "2023-02-21 16:25:38,497 epoch 1 - iter 125/250 - loss 1.28864655 - samples/sec: 42.63 - lr: 0.100000\n",
            "2023-02-21 16:25:47,521 epoch 1 - iter 150/250 - loss 1.21507969 - samples/sec: 44.36 - lr: 0.100000\n",
            "2023-02-21 16:25:55,153 epoch 1 - iter 175/250 - loss 1.14712501 - samples/sec: 52.47 - lr: 0.100000\n",
            "2023-02-21 16:26:03,745 epoch 1 - iter 200/250 - loss 1.09178130 - samples/sec: 46.58 - lr: 0.100000\n",
            "2023-02-21 16:26:12,120 epoch 1 - iter 225/250 - loss 1.04392862 - samples/sec: 47.79 - lr: 0.100000\n",
            "2023-02-21 16:26:21,001 epoch 1 - iter 250/250 - loss 1.00050391 - samples/sec: 45.07 - lr: 0.100000\n",
            "2023-02-21 16:26:21,003 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:26:21,005 EPOCH 1 done: loss 1.0005 - lr 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  5.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:26:27,073 Evaluating as a multi-label problem: False\n",
            "2023-02-21 16:26:27,120 DEV : loss 0.5385197997093201 - f1-score (micro avg)  0.8697\n",
            "2023-02-21 16:26:27,171 BAD EPOCHS (no improvement): 0\n",
            "2023-02-21 16:26:27,178 saving best model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:26:27,499 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:26:33,959 epoch 2 - iter 25/250 - loss 0.56508093 - samples/sec: 62.02 - lr: 0.100000\n",
            "2023-02-21 16:26:39,731 epoch 2 - iter 50/250 - loss 0.56305160 - samples/sec: 69.35 - lr: 0.100000\n",
            "2023-02-21 16:26:46,657 epoch 2 - iter 75/250 - loss 0.55405841 - samples/sec: 57.80 - lr: 0.100000\n",
            "2023-02-21 16:26:52,116 epoch 2 - iter 100/250 - loss 0.54498202 - samples/sec: 73.36 - lr: 0.100000\n",
            "2023-02-21 16:26:57,531 epoch 2 - iter 125/250 - loss 0.53575875 - samples/sec: 73.94 - lr: 0.100000\n",
            "2023-02-21 16:27:04,106 epoch 2 - iter 150/250 - loss 0.52586863 - samples/sec: 60.88 - lr: 0.100000\n",
            "2023-02-21 16:27:09,575 epoch 2 - iter 175/250 - loss 0.52122953 - samples/sec: 73.19 - lr: 0.100000\n",
            "2023-02-21 16:27:15,531 epoch 2 - iter 200/250 - loss 0.50932850 - samples/sec: 67.22 - lr: 0.100000\n",
            "2023-02-21 16:27:21,632 epoch 2 - iter 225/250 - loss 0.50166269 - samples/sec: 65.63 - lr: 0.100000\n",
            "2023-02-21 16:27:26,880 epoch 2 - iter 250/250 - loss 0.49458740 - samples/sec: 76.29 - lr: 0.100000\n",
            "2023-02-21 16:27:26,883 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:27:26,887 EPOCH 2 done: loss 0.4946 - lr 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:03<00:00,  8.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:27:30,893 Evaluating as a multi-label problem: False\n",
            "2023-02-21 16:27:30,962 DEV : loss 0.33010420203208923 - f1-score (micro avg)  0.9169\n",
            "2023-02-21 16:27:31,031 BAD EPOCHS (no improvement): 0\n",
            "2023-02-21 16:27:31,038 saving best model\n",
            "2023-02-21 16:27:31,462 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:27:37,038 epoch 3 - iter 25/250 - loss 0.40566825 - samples/sec: 71.90 - lr: 0.100000\n",
            "2023-02-21 16:27:42,360 epoch 3 - iter 50/250 - loss 0.42132042 - samples/sec: 75.24 - lr: 0.100000\n",
            "2023-02-21 16:27:48,917 epoch 3 - iter 75/250 - loss 0.41425573 - samples/sec: 61.04 - lr: 0.100000\n",
            "2023-02-21 16:27:54,745 epoch 3 - iter 100/250 - loss 0.40899350 - samples/sec: 68.70 - lr: 0.100000\n",
            "2023-02-21 16:28:01,546 epoch 3 - iter 125/250 - loss 0.40156524 - samples/sec: 58.86 - lr: 0.100000\n",
            "2023-02-21 16:28:07,223 epoch 3 - iter 150/250 - loss 0.39687754 - samples/sec: 70.55 - lr: 0.100000\n",
            "2023-02-21 16:28:12,382 epoch 3 - iter 175/250 - loss 0.39092810 - samples/sec: 77.60 - lr: 0.100000\n",
            "2023-02-21 16:28:19,426 epoch 3 - iter 200/250 - loss 0.38567877 - samples/sec: 56.86 - lr: 0.100000\n",
            "2023-02-21 16:28:25,776 epoch 3 - iter 225/250 - loss 0.38186539 - samples/sec: 63.07 - lr: 0.100000\n",
            "2023-02-21 16:28:32,130 epoch 3 - iter 250/250 - loss 0.37910362 - samples/sec: 63.00 - lr: 0.100000\n",
            "2023-02-21 16:28:32,133 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:28:32,136 EPOCH 3 done: loss 0.3791 - lr 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:03<00:00, 10.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:28:35,176 Evaluating as a multi-label problem: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:28:35,225 DEV : loss 0.2550539970397949 - f1-score (micro avg)  0.9329\n",
            "2023-02-21 16:28:35,284 BAD EPOCHS (no improvement): 0\n",
            "2023-02-21 16:28:35,291 saving best model\n",
            "2023-02-21 16:28:35,623 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:28:41,052 epoch 4 - iter 25/250 - loss 0.32276829 - samples/sec: 73.91 - lr: 0.100000\n",
            "2023-02-21 16:28:48,069 epoch 4 - iter 50/250 - loss 0.32850738 - samples/sec: 57.05 - lr: 0.100000\n",
            "2023-02-21 16:28:53,562 epoch 4 - iter 75/250 - loss 0.33292078 - samples/sec: 72.88 - lr: 0.100000\n",
            "2023-02-21 16:28:59,262 epoch 4 - iter 100/250 - loss 0.32686463 - samples/sec: 70.26 - lr: 0.100000\n",
            "2023-02-21 16:29:04,798 epoch 4 - iter 125/250 - loss 0.32498041 - samples/sec: 72.39 - lr: 0.100000\n",
            "2023-02-21 16:29:10,236 epoch 4 - iter 150/250 - loss 0.32011790 - samples/sec: 73.62 - lr: 0.100000\n",
            "2023-02-21 16:29:16,942 epoch 4 - iter 175/250 - loss 0.31811432 - samples/sec: 59.70 - lr: 0.100000\n",
            "2023-02-21 16:29:23,069 epoch 4 - iter 200/250 - loss 0.31938609 - samples/sec: 65.35 - lr: 0.100000\n",
            "2023-02-21 16:29:29,481 epoch 4 - iter 225/250 - loss 0.31694245 - samples/sec: 62.44 - lr: 0.100000\n",
            "2023-02-21 16:29:35,046 epoch 4 - iter 250/250 - loss 0.31411252 - samples/sec: 71.98 - lr: 0.100000\n",
            "2023-02-21 16:29:35,048 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:29:35,053 EPOCH 4 done: loss 0.3141 - lr 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:03<00:00, 10.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:29:38,146 Evaluating as a multi-label problem: False\n",
            "2023-02-21 16:29:38,191 DEV : loss 0.21091659367084503 - f1-score (micro avg)  0.944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:29:38,238 BAD EPOCHS (no improvement): 0\n",
            "2023-02-21 16:29:38,247 saving best model\n",
            "2023-02-21 16:29:38,573 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:29:45,406 epoch 5 - iter 25/250 - loss 0.27009724 - samples/sec: 58.60 - lr: 0.100000\n",
            "2023-02-21 16:29:50,890 epoch 5 - iter 50/250 - loss 0.28007537 - samples/sec: 73.06 - lr: 0.100000\n",
            "2023-02-21 16:29:56,541 epoch 5 - iter 75/250 - loss 0.28451641 - samples/sec: 70.86 - lr: 0.100000\n",
            "2023-02-21 16:30:02,627 epoch 5 - iter 100/250 - loss 0.28658515 - samples/sec: 65.82 - lr: 0.100000\n",
            "2023-02-21 16:30:07,763 epoch 5 - iter 125/250 - loss 0.28650466 - samples/sec: 77.95 - lr: 0.100000\n",
            "2023-02-21 16:30:14,573 epoch 5 - iter 150/250 - loss 0.28478662 - samples/sec: 58.79 - lr: 0.100000\n",
            "2023-02-21 16:30:20,086 epoch 5 - iter 175/250 - loss 0.28434331 - samples/sec: 72.64 - lr: 0.100000\n",
            "2023-02-21 16:30:25,643 epoch 5 - iter 200/250 - loss 0.28105197 - samples/sec: 72.04 - lr: 0.100000\n",
            "2023-02-21 16:30:32,491 epoch 5 - iter 225/250 - loss 0.28071779 - samples/sec: 58.49 - lr: 0.100000\n",
            "2023-02-21 16:30:37,588 epoch 5 - iter 250/250 - loss 0.27899784 - samples/sec: 78.57 - lr: 0.100000\n",
            "2023-02-21 16:30:37,590 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:30:37,595 EPOCH 5 done: loss 0.2790 - lr 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:03<00:00,  9.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:30:41,020 Evaluating as a multi-label problem: False\n",
            "2023-02-21 16:30:41,093 DEV : loss 0.18944858014583588 - f1-score (micro avg)  0.9471\n",
            "2023-02-21 16:30:41,163 BAD EPOCHS (no improvement): 0\n",
            "2023-02-21 16:30:41,171 saving best model\n",
            "2023-02-21 16:30:41,728 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:30:47,884 epoch 6 - iter 25/250 - loss 0.27059757 - samples/sec: 65.16 - lr: 0.100000\n",
            "2023-02-21 16:30:53,795 epoch 6 - iter 50/250 - loss 0.26189877 - samples/sec: 67.71 - lr: 0.100000\n",
            "2023-02-21 16:31:00,527 epoch 6 - iter 75/250 - loss 0.25474554 - samples/sec: 59.48 - lr: 0.100000\n",
            "2023-02-21 16:31:05,653 epoch 6 - iter 100/250 - loss 0.25031771 - samples/sec: 78.13 - lr: 0.100000\n",
            "2023-02-21 16:31:11,550 epoch 6 - iter 125/250 - loss 0.25465388 - samples/sec: 67.90 - lr: 0.100000\n",
            "2023-02-21 16:31:17,409 epoch 6 - iter 150/250 - loss 0.25209027 - samples/sec: 68.39 - lr: 0.100000\n",
            "2023-02-21 16:31:23,406 epoch 6 - iter 175/250 - loss 0.25364368 - samples/sec: 66.76 - lr: 0.100000\n",
            "2023-02-21 16:31:29,852 epoch 6 - iter 200/250 - loss 0.25201097 - samples/sec: 62.11 - lr: 0.100000\n",
            "2023-02-21 16:31:34,887 epoch 6 - iter 225/250 - loss 0.25103812 - samples/sec: 79.52 - lr: 0.100000\n",
            "2023-02-21 16:31:41,250 epoch 6 - iter 250/250 - loss 0.25431553 - samples/sec: 62.93 - lr: 0.100000\n",
            "2023-02-21 16:31:41,253 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:31:41,257 EPOCH 6 done: loss 0.2543 - lr 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:03<00:00,  8.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:31:44,982 Evaluating as a multi-label problem: False\n",
            "2023-02-21 16:31:45,034 DEV : loss 0.18367673456668854 - f1-score (micro avg)  0.9511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:31:45,083 BAD EPOCHS (no improvement): 0\n",
            "2023-02-21 16:31:45,089 saving best model\n",
            "2023-02-21 16:31:45,415 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:31:50,583 epoch 7 - iter 25/250 - loss 0.24622824 - samples/sec: 77.52 - lr: 0.100000\n",
            "2023-02-21 16:31:56,504 epoch 7 - iter 50/250 - loss 0.24126191 - samples/sec: 67.62 - lr: 0.100000\n",
            "2023-02-21 16:32:02,103 epoch 7 - iter 75/250 - loss 0.24688537 - samples/sec: 71.57 - lr: 0.100000\n",
            "2023-02-21 16:32:07,585 epoch 7 - iter 100/250 - loss 0.24334711 - samples/sec: 73.02 - lr: 0.100000\n",
            "2023-02-21 16:32:14,525 epoch 7 - iter 125/250 - loss 0.24001147 - samples/sec: 57.68 - lr: 0.100000\n",
            "2023-02-21 16:32:20,643 epoch 7 - iter 150/250 - loss 0.23915017 - samples/sec: 65.43 - lr: 0.100000\n",
            "2023-02-21 16:32:27,160 epoch 7 - iter 175/250 - loss 0.23770304 - samples/sec: 61.42 - lr: 0.100000\n",
            "2023-02-21 16:32:32,413 epoch 7 - iter 200/250 - loss 0.23559222 - samples/sec: 76.30 - lr: 0.100000\n",
            "2023-02-21 16:32:37,596 epoch 7 - iter 225/250 - loss 0.23270095 - samples/sec: 77.24 - lr: 0.100000\n",
            "2023-02-21 16:32:43,852 epoch 7 - iter 250/250 - loss 0.23343216 - samples/sec: 63.99 - lr: 0.100000\n",
            "2023-02-21 16:32:43,856 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:32:43,859 EPOCH 7 done: loss 0.2334 - lr 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:02<00:00, 11.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:32:46,796 Evaluating as a multi-label problem: False\n",
            "2023-02-21 16:32:46,840 DEV : loss 0.16604416072368622 - f1-score (micro avg)  0.9559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:32:46,887 BAD EPOCHS (no improvement): 0\n",
            "2023-02-21 16:32:46,896 saving best model\n",
            "2023-02-21 16:32:47,213 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:32:52,239 epoch 8 - iter 25/250 - loss 0.21596353 - samples/sec: 79.77 - lr: 0.100000\n",
            "2023-02-21 16:32:58,551 epoch 8 - iter 50/250 - loss 0.20979188 - samples/sec: 63.42 - lr: 0.100000\n",
            "2023-02-21 16:33:04,082 epoch 8 - iter 75/250 - loss 0.21545429 - samples/sec: 72.39 - lr: 0.100000\n",
            "2023-02-21 16:33:09,327 epoch 8 - iter 100/250 - loss 0.21885591 - samples/sec: 76.33 - lr: 0.100000\n",
            "2023-02-21 16:33:15,247 epoch 8 - iter 125/250 - loss 0.21692189 - samples/sec: 67.65 - lr: 0.100000\n",
            "2023-02-21 16:33:20,308 epoch 8 - iter 150/250 - loss 0.21924886 - samples/sec: 79.12 - lr: 0.100000\n",
            "2023-02-21 16:33:27,297 epoch 8 - iter 175/250 - loss 0.21644525 - samples/sec: 57.28 - lr: 0.100000\n",
            "2023-02-21 16:33:33,485 epoch 8 - iter 200/250 - loss 0.21643886 - samples/sec: 64.68 - lr: 0.100000\n",
            "2023-02-21 16:33:40,600 epoch 8 - iter 225/250 - loss 0.21636898 - samples/sec: 56.27 - lr: 0.100000\n",
            "2023-02-21 16:33:46,220 epoch 8 - iter 250/250 - loss 0.21570818 - samples/sec: 71.29 - lr: 0.100000\n",
            "2023-02-21 16:33:46,222 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:33:46,225 EPOCH 8 done: loss 0.2157 - lr 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:04<00:00,  6.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:33:51,137 Evaluating as a multi-label problem: False\n",
            "2023-02-21 16:33:51,179 DEV : loss 0.16101936995983124 - f1-score (micro avg)  0.9597\n",
            "2023-02-21 16:33:51,227 BAD EPOCHS (no improvement): 0\n",
            "2023-02-21 16:33:51,233 saving best model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:33:51,580 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:33:58,259 epoch 9 - iter 25/250 - loss 0.22896452 - samples/sec: 60.00 - lr: 0.100000\n",
            "2023-02-21 16:34:04,358 epoch 9 - iter 50/250 - loss 0.21492418 - samples/sec: 65.65 - lr: 0.100000\n",
            "2023-02-21 16:34:10,473 epoch 9 - iter 75/250 - loss 0.21226569 - samples/sec: 65.49 - lr: 0.100000\n",
            "2023-02-21 16:34:16,254 epoch 9 - iter 100/250 - loss 0.20640573 - samples/sec: 69.28 - lr: 0.100000\n",
            "2023-02-21 16:34:21,717 epoch 9 - iter 125/250 - loss 0.20837741 - samples/sec: 73.28 - lr: 0.100000\n",
            "2023-02-21 16:34:28,751 epoch 9 - iter 150/250 - loss 0.21046494 - samples/sec: 56.91 - lr: 0.100000\n",
            "2023-02-21 16:34:34,032 epoch 9 - iter 175/250 - loss 0.20877147 - samples/sec: 75.82 - lr: 0.100000\n",
            "2023-02-21 16:34:40,671 epoch 9 - iter 200/250 - loss 0.20988110 - samples/sec: 60.30 - lr: 0.100000\n",
            "2023-02-21 16:34:45,914 epoch 9 - iter 225/250 - loss 0.21120889 - samples/sec: 76.48 - lr: 0.100000\n",
            "2023-02-21 16:34:50,696 epoch 9 - iter 250/250 - loss 0.20863703 - samples/sec: 83.75 - lr: 0.100000\n",
            "2023-02-21 16:34:50,698 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:34:50,702 EPOCH 9 done: loss 0.2086 - lr 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:04<00:00,  7.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:34:54,968 Evaluating as a multi-label problem: False\n",
            "2023-02-21 16:34:55,046 DEV : loss 0.16022558510303497 - f1-score (micro avg)  0.9574\n",
            "2023-02-21 16:34:55,123 BAD EPOCHS (no improvement): 1\n",
            "2023-02-21 16:34:55,138 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:35:00,728 epoch 10 - iter 25/250 - loss 0.19088693 - samples/sec: 71.65 - lr: 0.100000\n",
            "2023-02-21 16:35:06,904 epoch 10 - iter 50/250 - loss 0.20023066 - samples/sec: 64.83 - lr: 0.100000\n",
            "2023-02-21 16:35:13,207 epoch 10 - iter 75/250 - loss 0.20309713 - samples/sec: 63.57 - lr: 0.100000\n",
            "2023-02-21 16:35:18,792 epoch 10 - iter 100/250 - loss 0.20547640 - samples/sec: 71.68 - lr: 0.100000\n",
            "2023-02-21 16:35:24,840 epoch 10 - iter 125/250 - loss 0.20232619 - samples/sec: 66.20 - lr: 0.100000\n",
            "2023-02-21 16:35:30,211 epoch 10 - iter 150/250 - loss 0.19935582 - samples/sec: 74.56 - lr: 0.100000\n",
            "2023-02-21 16:35:35,734 epoch 10 - iter 175/250 - loss 0.19719839 - samples/sec: 72.47 - lr: 0.100000\n",
            "2023-02-21 16:35:41,648 epoch 10 - iter 200/250 - loss 0.19826524 - samples/sec: 67.78 - lr: 0.100000\n",
            "2023-02-21 16:35:47,618 epoch 10 - iter 225/250 - loss 0.19702067 - samples/sec: 67.06 - lr: 0.100000\n",
            "2023-02-21 16:35:55,038 epoch 10 - iter 250/250 - loss 0.19538746 - samples/sec: 53.96 - lr: 0.100000\n",
            "2023-02-21 16:35:55,041 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:35:55,044 EPOCH 10 done: loss 0.1954 - lr 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:03<00:00,  9.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:35:58,289 Evaluating as a multi-label problem: False\n",
            "2023-02-21 16:35:58,333 DEV : loss 0.14543043076992035 - f1-score (micro avg)  0.961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:35:58,384 BAD EPOCHS (no improvement): 0\n",
            "2023-02-21 16:35:58,393 saving best model\n",
            "2023-02-21 16:35:58,726 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:36:03,828 epoch 11 - iter 25/250 - loss 0.18486460 - samples/sec: 78.57 - lr: 0.100000\n",
            "2023-02-21 16:36:10,460 epoch 11 - iter 50/250 - loss 0.18662684 - samples/sec: 60.38 - lr: 0.100000\n",
            "2023-02-21 16:36:15,868 epoch 11 - iter 75/250 - loss 0.18121551 - samples/sec: 74.08 - lr: 0.100000\n",
            "2023-02-21 16:36:21,580 epoch 11 - iter 100/250 - loss 0.18431550 - samples/sec: 70.10 - lr: 0.100000\n",
            "2023-02-21 16:36:27,955 epoch 11 - iter 125/250 - loss 0.18458124 - samples/sec: 62.85 - lr: 0.100000\n",
            "2023-02-21 16:36:34,202 epoch 11 - iter 150/250 - loss 0.18601205 - samples/sec: 64.08 - lr: 0.100000\n",
            "2023-02-21 16:36:40,907 epoch 11 - iter 175/250 - loss 0.18776317 - samples/sec: 59.71 - lr: 0.100000\n",
            "2023-02-21 16:36:46,175 epoch 11 - iter 200/250 - loss 0.18681182 - samples/sec: 75.99 - lr: 0.100000\n",
            "2023-02-21 16:36:51,887 epoch 11 - iter 225/250 - loss 0.18603901 - samples/sec: 70.08 - lr: 0.100000\n",
            "2023-02-21 16:36:58,120 epoch 11 - iter 250/250 - loss 0.18595090 - samples/sec: 64.25 - lr: 0.100000\n",
            "2023-02-21 16:36:58,122 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:36:58,127 EPOCH 11 done: loss 0.1860 - lr 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:02<00:00, 11.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:37:00,974 Evaluating as a multi-label problem: False\n",
            "2023-02-21 16:37:01,023 DEV : loss 0.14295707643032074 - f1-score (micro avg)  0.9632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:37:01,075 BAD EPOCHS (no improvement): 0\n",
            "2023-02-21 16:37:01,083 saving best model\n",
            "2023-02-21 16:37:01,392 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:37:07,683 epoch 12 - iter 25/250 - loss 0.16537555 - samples/sec: 63.79 - lr: 0.100000\n",
            "2023-02-21 16:37:14,259 epoch 12 - iter 50/250 - loss 0.16869713 - samples/sec: 60.90 - lr: 0.100000\n",
            "2023-02-21 16:37:19,149 epoch 12 - iter 75/250 - loss 0.16895150 - samples/sec: 81.90 - lr: 0.100000\n",
            "2023-02-21 16:37:25,068 epoch 12 - iter 100/250 - loss 0.16609688 - samples/sec: 67.64 - lr: 0.100000\n",
            "2023-02-21 16:37:31,240 epoch 12 - iter 125/250 - loss 0.17254803 - samples/sec: 64.86 - lr: 0.100000\n",
            "2023-02-21 16:37:37,743 epoch 12 - iter 150/250 - loss 0.17202756 - samples/sec: 61.60 - lr: 0.100000\n",
            "2023-02-21 16:37:43,503 epoch 12 - iter 175/250 - loss 0.17341008 - samples/sec: 69.53 - lr: 0.100000\n",
            "2023-02-21 16:37:48,748 epoch 12 - iter 200/250 - loss 0.17313873 - samples/sec: 76.33 - lr: 0.100000\n",
            "2023-02-21 16:37:55,636 epoch 12 - iter 225/250 - loss 0.17527730 - samples/sec: 58.12 - lr: 0.100000\n",
            "2023-02-21 16:38:00,862 epoch 12 - iter 250/250 - loss 0.17564552 - samples/sec: 76.62 - lr: 0.100000\n",
            "2023-02-21 16:38:00,865 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:38:00,868 EPOCH 12 done: loss 0.1756 - lr 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:02<00:00, 11.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:38:03,762 Evaluating as a multi-label problem: False\n",
            "2023-02-21 16:38:03,807 DEV : loss 0.136047825217247 - f1-score (micro avg)  0.9668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:38:03,859 BAD EPOCHS (no improvement): 0\n",
            "2023-02-21 16:38:03,866 saving best model\n",
            "2023-02-21 16:38:04,185 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:38:10,681 epoch 13 - iter 25/250 - loss 0.18884983 - samples/sec: 61.74 - lr: 0.100000\n",
            "2023-02-21 16:38:16,955 epoch 13 - iter 50/250 - loss 0.17936834 - samples/sec: 63.82 - lr: 0.100000\n",
            "2023-02-21 16:38:23,684 epoch 13 - iter 75/250 - loss 0.17591934 - samples/sec: 59.51 - lr: 0.100000\n",
            "2023-02-21 16:38:28,911 epoch 13 - iter 100/250 - loss 0.17149322 - samples/sec: 76.66 - lr: 0.100000\n",
            "2023-02-21 16:38:34,247 epoch 13 - iter 125/250 - loss 0.16863851 - samples/sec: 75.04 - lr: 0.100000\n",
            "2023-02-21 16:38:40,980 epoch 13 - iter 150/250 - loss 0.17035387 - samples/sec: 59.50 - lr: 0.100000\n",
            "2023-02-21 16:38:47,101 epoch 13 - iter 175/250 - loss 0.16893083 - samples/sec: 65.41 - lr: 0.100000\n",
            "2023-02-21 16:38:53,278 epoch 13 - iter 200/250 - loss 0.17012230 - samples/sec: 64.80 - lr: 0.100000\n",
            "2023-02-21 16:38:58,190 epoch 13 - iter 225/250 - loss 0.16832110 - samples/sec: 81.53 - lr: 0.100000\n",
            "2023-02-21 16:39:03,380 epoch 13 - iter 250/250 - loss 0.16972049 - samples/sec: 77.14 - lr: 0.100000\n",
            "2023-02-21 16:39:03,384 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:39:03,387 EPOCH 13 done: loss 0.1697 - lr 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:04<00:00,  7.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:39:07,873 Evaluating as a multi-label problem: False\n",
            "2023-02-21 16:39:07,943 DEV : loss 0.13325029611587524 - f1-score (micro avg)  0.9619\n",
            "2023-02-21 16:39:08,019 BAD EPOCHS (no improvement): 1\n",
            "2023-02-21 16:39:08,029 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:39:13,746 epoch 14 - iter 25/250 - loss 0.16757688 - samples/sec: 70.04 - lr: 0.100000\n",
            "2023-02-21 16:39:19,259 epoch 14 - iter 50/250 - loss 0.15938418 - samples/sec: 72.62 - lr: 0.100000\n",
            "2023-02-21 16:39:26,186 epoch 14 - iter 75/250 - loss 0.16217824 - samples/sec: 57.79 - lr: 0.100000\n",
            "2023-02-21 16:39:31,822 epoch 14 - iter 100/250 - loss 0.16653250 - samples/sec: 71.03 - lr: 0.100000\n",
            "2023-02-21 16:39:38,704 epoch 14 - iter 125/250 - loss 0.16815099 - samples/sec: 58.18 - lr: 0.100000\n",
            "2023-02-21 16:39:44,680 epoch 14 - iter 150/250 - loss 0.16823578 - samples/sec: 66.99 - lr: 0.100000\n",
            "2023-02-21 16:39:50,928 epoch 14 - iter 175/250 - loss 0.16743169 - samples/sec: 64.08 - lr: 0.100000\n",
            "2023-02-21 16:39:57,210 epoch 14 - iter 200/250 - loss 0.16810075 - samples/sec: 63.73 - lr: 0.100000\n",
            "2023-02-21 16:40:02,617 epoch 14 - iter 225/250 - loss 0.16436482 - samples/sec: 74.06 - lr: 0.100000\n",
            "2023-02-21 16:40:09,645 epoch 14 - iter 250/250 - loss 0.16436321 - samples/sec: 56.97 - lr: 0.100000\n",
            "2023-02-21 16:40:09,648 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:40:09,650 EPOCH 14 done: loss 0.1644 - lr 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:03<00:00, 10.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:40:12,781 Evaluating as a multi-label problem: False\n",
            "2023-02-21 16:40:12,827 DEV : loss 0.13514719903469086 - f1-score (micro avg)  0.9647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:40:12,877 BAD EPOCHS (no improvement): 2\n",
            "2023-02-21 16:40:12,886 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:40:18,710 epoch 15 - iter 25/250 - loss 0.16059349 - samples/sec: 68.75 - lr: 0.100000\n",
            "2023-02-21 16:40:25,862 epoch 15 - iter 50/250 - loss 0.15337311 - samples/sec: 55.99 - lr: 0.100000\n",
            "2023-02-21 16:40:31,494 epoch 15 - iter 75/250 - loss 0.15467104 - samples/sec: 71.10 - lr: 0.100000\n",
            "2023-02-21 16:40:38,784 epoch 15 - iter 100/250 - loss 0.16137744 - samples/sec: 54.92 - lr: 0.100000\n",
            "2023-02-21 16:40:45,544 epoch 15 - iter 125/250 - loss 0.16000197 - samples/sec: 59.22 - lr: 0.100000\n",
            "2023-02-21 16:40:51,561 epoch 15 - iter 150/250 - loss 0.16002782 - samples/sec: 66.54 - lr: 0.100000\n",
            "2023-02-21 16:40:57,835 epoch 15 - iter 175/250 - loss 0.15967838 - samples/sec: 63.81 - lr: 0.100000\n",
            "2023-02-21 16:41:03,368 epoch 15 - iter 200/250 - loss 0.15943091 - samples/sec: 72.38 - lr: 0.100000\n",
            "2023-02-21 16:41:10,293 epoch 15 - iter 225/250 - loss 0.15871432 - samples/sec: 57.82 - lr: 0.100000\n",
            "2023-02-21 16:41:15,955 epoch 15 - iter 250/250 - loss 0.15854911 - samples/sec: 70.73 - lr: 0.100000\n",
            "2023-02-21 16:41:15,957 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:41:15,962 EPOCH 15 done: loss 0.1585 - lr 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:03<00:00, 10.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:41:19,204 Evaluating as a multi-label problem: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:41:19,258 DEV : loss 0.13064870238304138 - f1-score (micro avg)  0.966\n",
            "2023-02-21 16:41:19,312 BAD EPOCHS (no improvement): 3\n",
            "2023-02-21 16:41:19,320 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:41:25,760 epoch 16 - iter 25/250 - loss 0.15844651 - samples/sec: 62.20 - lr: 0.100000\n",
            "2023-02-21 16:41:31,310 epoch 16 - iter 50/250 - loss 0.15579196 - samples/sec: 72.14 - lr: 0.100000\n",
            "2023-02-21 16:41:37,991 epoch 16 - iter 75/250 - loss 0.15251239 - samples/sec: 59.92 - lr: 0.100000\n",
            "2023-02-21 16:41:44,199 epoch 16 - iter 100/250 - loss 0.15915119 - samples/sec: 64.50 - lr: 0.100000\n",
            "2023-02-21 16:41:50,242 epoch 16 - iter 125/250 - loss 0.15447666 - samples/sec: 66.24 - lr: 0.100000\n",
            "2023-02-21 16:41:57,281 epoch 16 - iter 150/250 - loss 0.15583735 - samples/sec: 56.88 - lr: 0.100000\n",
            "2023-02-21 16:42:02,779 epoch 16 - iter 175/250 - loss 0.15461149 - samples/sec: 72.81 - lr: 0.100000\n",
            "2023-02-21 16:42:10,393 epoch 16 - iter 200/250 - loss 0.15545630 - samples/sec: 52.56 - lr: 0.100000\n",
            "2023-02-21 16:42:16,692 epoch 16 - iter 225/250 - loss 0.15308244 - samples/sec: 63.57 - lr: 0.100000\n",
            "2023-02-21 16:42:22,272 epoch 16 - iter 250/250 - loss 0.15161419 - samples/sec: 71.77 - lr: 0.100000\n",
            "2023-02-21 16:42:22,281 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:42:22,283 EPOCH 16 done: loss 0.1516 - lr 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:04<00:00,  6.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:42:26,973 Evaluating as a multi-label problem: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:42:27,026 DEV : loss 0.12230341136455536 - f1-score (micro avg)  0.9695\n",
            "2023-02-21 16:42:27,076 BAD EPOCHS (no improvement): 0\n",
            "2023-02-21 16:42:27,086 saving best model\n",
            "2023-02-21 16:42:27,674 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:42:33,393 epoch 17 - iter 25/250 - loss 0.14287549 - samples/sec: 70.08 - lr: 0.100000\n",
            "2023-02-21 16:42:39,715 epoch 17 - iter 50/250 - loss 0.13753753 - samples/sec: 63.33 - lr: 0.100000\n",
            "2023-02-21 16:42:46,229 epoch 17 - iter 75/250 - loss 0.13999390 - samples/sec: 61.52 - lr: 0.100000\n",
            "2023-02-21 16:42:52,796 epoch 17 - iter 100/250 - loss 0.14294945 - samples/sec: 60.97 - lr: 0.100000\n",
            "2023-02-21 16:43:01,296 epoch 17 - iter 125/250 - loss 0.14097130 - samples/sec: 47.10 - lr: 0.100000\n",
            "2023-02-21 16:43:07,800 epoch 17 - iter 150/250 - loss 0.14170993 - samples/sec: 61.54 - lr: 0.100000\n",
            "2023-02-21 16:43:14,655 epoch 17 - iter 175/250 - loss 0.14531158 - samples/sec: 58.40 - lr: 0.100000\n",
            "2023-02-21 16:43:20,588 epoch 17 - iter 200/250 - loss 0.14533171 - samples/sec: 67.47 - lr: 0.100000\n",
            "2023-02-21 16:43:26,176 epoch 17 - iter 225/250 - loss 0.14681767 - samples/sec: 71.67 - lr: 0.100000\n",
            "2023-02-21 16:43:33,051 epoch 17 - iter 250/250 - loss 0.14723784 - samples/sec: 58.27 - lr: 0.100000\n",
            "2023-02-21 16:43:33,054 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:43:33,060 EPOCH 17 done: loss 0.1472 - lr 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:03<00:00,  9.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:43:36,379 Evaluating as a multi-label problem: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:43:36,433 DEV : loss 0.1285804659128189 - f1-score (micro avg)  0.9682\n",
            "2023-02-21 16:43:36,489 BAD EPOCHS (no improvement): 1\n",
            "2023-02-21 16:43:36,496 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:43:43,634 epoch 18 - iter 25/250 - loss 0.14019843 - samples/sec: 56.11 - lr: 0.100000\n",
            "2023-02-21 16:43:49,867 epoch 18 - iter 50/250 - loss 0.13684872 - samples/sec: 64.23 - lr: 0.100000\n",
            "2023-02-21 16:43:55,211 epoch 18 - iter 75/250 - loss 0.13444452 - samples/sec: 74.91 - lr: 0.100000\n",
            "2023-02-21 16:44:02,332 epoch 18 - iter 100/250 - loss 0.13794680 - samples/sec: 56.26 - lr: 0.100000\n",
            "2023-02-21 16:44:08,778 epoch 18 - iter 125/250 - loss 0.13703591 - samples/sec: 62.10 - lr: 0.100000\n",
            "2023-02-21 16:44:15,776 epoch 18 - iter 150/250 - loss 0.13937700 - samples/sec: 57.20 - lr: 0.100000\n",
            "2023-02-21 16:44:21,630 epoch 18 - iter 175/250 - loss 0.13912917 - samples/sec: 68.40 - lr: 0.100000\n",
            "2023-02-21 16:44:27,880 epoch 18 - iter 200/250 - loss 0.14021039 - samples/sec: 64.05 - lr: 0.100000\n",
            "2023-02-21 16:44:33,700 epoch 18 - iter 225/250 - loss 0.13887504 - samples/sec: 68.86 - lr: 0.100000\n",
            "2023-02-21 16:44:38,585 epoch 18 - iter 250/250 - loss 0.13867483 - samples/sec: 81.96 - lr: 0.100000\n",
            "2023-02-21 16:44:38,588 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:44:38,591 EPOCH 18 done: loss 0.1387 - lr 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:03<00:00,  8.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:44:42,436 Evaluating as a multi-label problem: False\n",
            "2023-02-21 16:44:42,514 DEV : loss 0.12219937145709991 - f1-score (micro avg)  0.969\n",
            "2023-02-21 16:44:42,592 BAD EPOCHS (no improvement): 2\n",
            "2023-02-21 16:44:42,605 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:44:49,683 epoch 19 - iter 25/250 - loss 0.15295085 - samples/sec: 56.57 - lr: 0.100000\n",
            "2023-02-21 16:44:55,781 epoch 19 - iter 50/250 - loss 0.15556412 - samples/sec: 65.63 - lr: 0.100000\n",
            "2023-02-21 16:45:02,027 epoch 19 - iter 75/250 - loss 0.15451942 - samples/sec: 64.14 - lr: 0.100000\n",
            "2023-02-21 16:45:07,706 epoch 19 - iter 100/250 - loss 0.14857377 - samples/sec: 70.49 - lr: 0.100000\n",
            "2023-02-21 16:45:14,392 epoch 19 - iter 125/250 - loss 0.14789456 - samples/sec: 59.88 - lr: 0.100000\n",
            "2023-02-21 16:45:20,523 epoch 19 - iter 150/250 - loss 0.14539125 - samples/sec: 65.30 - lr: 0.100000\n",
            "2023-02-21 16:45:26,034 epoch 19 - iter 175/250 - loss 0.14541679 - samples/sec: 72.65 - lr: 0.100000\n",
            "2023-02-21 16:45:32,743 epoch 19 - iter 200/250 - loss 0.14571144 - samples/sec: 59.69 - lr: 0.100000\n",
            "2023-02-21 16:45:39,069 epoch 19 - iter 225/250 - loss 0.14397764 - samples/sec: 63.28 - lr: 0.100000\n",
            "2023-02-21 16:45:46,169 epoch 19 - iter 250/250 - loss 0.14142099 - samples/sec: 56.39 - lr: 0.100000\n",
            "2023-02-21 16:45:46,172 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:45:46,174 EPOCH 19 done: loss 0.1414 - lr 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:03<00:00, 10.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:45:49,292 Evaluating as a multi-label problem: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:45:49,347 DEV : loss 0.12511852383613586 - f1-score (micro avg)  0.9697\n",
            "2023-02-21 16:45:49,397 BAD EPOCHS (no improvement): 0\n",
            "2023-02-21 16:45:49,406 saving best model\n",
            "2023-02-21 16:45:49,761 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:45:55,143 epoch 20 - iter 25/250 - loss 0.13131750 - samples/sec: 74.50 - lr: 0.100000\n",
            "2023-02-21 16:46:02,467 epoch 20 - iter 50/250 - loss 0.12684090 - samples/sec: 54.65 - lr: 0.100000\n",
            "2023-02-21 16:46:08,060 epoch 20 - iter 75/250 - loss 0.13137319 - samples/sec: 71.58 - lr: 0.100000\n",
            "2023-02-21 16:46:14,030 epoch 20 - iter 100/250 - loss 0.13281474 - samples/sec: 67.07 - lr: 0.100000\n",
            "2023-02-21 16:46:20,341 epoch 20 - iter 125/250 - loss 0.13411723 - samples/sec: 63.46 - lr: 0.100000\n",
            "2023-02-21 16:46:25,781 epoch 20 - iter 150/250 - loss 0.13454362 - samples/sec: 73.60 - lr: 0.100000\n",
            "2023-02-21 16:46:32,965 epoch 20 - iter 175/250 - loss 0.13230223 - samples/sec: 55.73 - lr: 0.100000\n",
            "2023-02-21 16:46:38,746 epoch 20 - iter 200/250 - loss 0.13461941 - samples/sec: 69.24 - lr: 0.100000\n",
            "2023-02-21 16:46:45,648 epoch 20 - iter 225/250 - loss 0.13586823 - samples/sec: 58.00 - lr: 0.100000\n",
            "2023-02-21 16:46:52,223 epoch 20 - iter 250/250 - loss 0.13507948 - samples/sec: 60.93 - lr: 0.100000\n",
            "2023-02-21 16:46:52,226 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:46:52,231 EPOCH 20 done: loss 0.1351 - lr 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:03<00:00, 10.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:46:55,280 Evaluating as a multi-label problem: False\n",
            "2023-02-21 16:46:55,327 DEV : loss 0.129611074924469 - f1-score (micro avg)  0.9702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:46:55,380 BAD EPOCHS (no improvement): 0\n",
            "2023-02-21 16:46:55,389 saving best model\n",
            "2023-02-21 16:46:56,084 ----------------------------------------------------------------------------------------------------\n",
            "2023-02-21 16:46:56,091 loading file /content/drive/MyDrive/NLP_Applications_1/DATA/2023-ILTAPP-20230203T201734Z-001/2023-ILTAPP/datasets/sigmorphon2019/best-model.pt\n",
            "2023-02-21 16:46:56,578 SequenceTagger predicts: Dictionary with 144 tags: <unk>, ↓0;d¦+, ↑0¦↓1;d¦+, ↓0;d¦-+, ↓0;abe, ↓0;d¦--+, ↓0;d¦---+, ↓0;d--+b¦+, ↑0¦↓-1;d¦+, ↓0;d¦-+v+e+, ↓0;d¦---+e+, ↓0;d¦-+o→+, ↓0;d¦--+e+, ↓0;d¦--+y+, ↑0¦↓1;ai, ↓0;d¦---+y+, ↓0;awe, ↓0;d-+b¦--+, ↓0;d¦-+y+, ↓0;d¦--+o+, ↓0;d¦----+, ↓0;d+s¦-+, ↓0;d¦-+k→+, ↓0;d¦-+e→+, ↓0;d-+w+i¦+, ↓0;d¦-+e→-+l+, ↓0;d-+h+a¦+, ↓0;d+'¦+, ↓0;d¦--+a→+e+, ↓0;d¦-+d+, ↓0;ago, ↓0;d→-+o¦+, ↓0;d¦-+a→+, ↓0;d¦+e→-+, ↓0;d¦+n+, ↓0;d→--+i¦+, ↓0;d¦-----+i+n+k+, ↓0;d→-+i¦+, ↓0;d¦-+i→+, ↓0;d¦--+a+v+e+, ↑0¦↓1¦↑2¦↓3;d¦+, ↓0;d¦-+i+l+l+, ↓0;d¦-+i→→+, ↑0¦↓1¦↑2¦↓-2;d¦+, ↓0;d-+w+o+u+l¦+, ↓0;d¦+o→+, ↓0;a\", ↓0;d¦+s+, ↓0;d-+'¦+, ↓0;d¦+e→+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:07<00:00,  4.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 16:47:03,809 Evaluating as a multi-label problem: False\n",
            "2023-02-21 16:47:03,862 0.9684\t0.9684\t0.9684\t0.9684\n",
            "2023-02-21 16:47:03,864 \n",
            "Results:\n",
            "- F-score (micro) 0.9684\n",
            "- F-score (macro) 0.5261\n",
            "- Accuracy 0.9684\n",
            "\n",
            "By class:\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "                 ↓0;d¦+     0.9892    0.9825    0.9858      6042\n",
            "              ↑0¦↓1;d¦+     0.9022    0.9727    0.9361       550\n",
            "                ↓0;d¦-+     0.8725    0.9683    0.9179       410\n",
            "                 ↓0;abe     1.0000    0.9869    0.9934       153\n",
            "               ↓0;d¦--+     0.8968    0.9113    0.9040       124\n",
            "              ↓0;d¦---+     0.7379    0.8636    0.7958        88\n",
            "             ↑0¦↓-1;d¦+     0.7794    0.8983    0.8346        59\n",
            "            ↓0;d¦-+v+e+     1.0000    1.0000    1.0000        46\n",
            "             ↓0;d--+b¦+     1.0000    1.0000    1.0000        44\n",
            "            ↓0;d¦---+e+     0.9032    0.7778    0.8358        36\n",
            "             ↓0;d¦-+o→+     1.0000    1.0000    1.0000        31\n",
            "            ↓0;d¦---+y+     0.9286    0.9286    0.9286        28\n",
            "                 ↓0;awe     1.0000    1.0000    1.0000        24\n",
            "             ↓0;d¦--+y+     1.0000    0.9130    0.9545        23\n",
            "             ↓0;d¦--+e+     1.0000    1.0000    1.0000        19\n",
            "               ↑0¦↓1;ai     1.0000    1.0000    1.0000        17\n",
            "             ↓0;d¦----+     0.8333    0.2632    0.4000        19\n",
            "             ↓0;d¦--+o+     1.0000    1.0000    1.0000        10\n",
            "            ↓0;d-+b¦--+     1.0000    1.0000    1.0000         8\n",
            "              ↓0;d¦-+d+     0.8750    1.0000    0.9333         7\n",
            "            ↓0;d-+w+i¦+     1.0000    0.8571    0.9231         7\n",
            "                  ↓0;a\"     0.5000    0.1111    0.1818         9\n",
            "               ↓0;d¦+n+     1.0000    1.0000    1.0000         5\n",
            "          ↓0;d¦-+e→-+l+     1.0000    1.0000    1.0000         5\n",
            "             ↓0;d→-+o¦+     0.8000    1.0000    0.8889         4\n",
            "          ↓0;d¦-+i+l+l+     1.0000    1.0000    1.0000         4\n",
            "              ↓0;d¦-+y+     1.0000    1.0000    1.0000         4\n",
            "              ↓0;d+s¦-+     1.0000    1.0000    1.0000         4\n",
            "             ↓0;d¦-+i→+     0.5000    0.6667    0.5714         3\n",
            "            ↓0;d→--+i¦+     1.0000    0.7500    0.8571         4\n",
            "              ↓0;d¦+o→+     1.0000    0.7500    0.8571         4\n",
            "             ↓0;d¦-+k→+     1.0000    1.0000    1.0000         3\n",
            "          ↓0;d¦--+a→+e+     1.0000    1.0000    1.0000         3\n",
            "             ↓0;d¦-+a→+     1.0000    0.5000    0.6667         4\n",
            "               ↓0;d+'¦+     1.0000    1.0000    1.0000         2\n",
            "            ↓0;d-+h+a¦+     1.0000    1.0000    1.0000         2\n",
            "         ↓0;d¦--+a+v+e+     1.0000    1.0000    1.0000         2\n",
            "        ↓0;d-+w+o+u+l¦+     1.0000    1.0000    1.0000         2\n",
            "             ↓0;d→-+i¦+     1.0000    1.0000    1.0000         2\n",
            "            ↓0;d¦-+o→→+     0.0000    0.0000    0.0000         3\n",
            "          ↓0;d¦-+e+a→-+     1.0000    0.5000    0.6667         2\n",
            "            ↓0;d¦-+f→-+     0.0000    0.0000    0.0000         3\n",
            "      ↓0;d¦-----+i+n+k+     1.0000    1.0000    1.0000         1\n",
            "              ↓0;d-+b¦+     0.0000    0.0000    0.0000         2\n",
            "               ↓0;d¦+s+     0.0000    0.0000    0.0000         2\n",
            "              ↓0;d¦+a→+     0.0000    0.0000    0.0000         2\n",
            "                  ↓0;a'     0.0000    0.0000    0.0000         2\n",
            "             ↓0;d¦-+e→+     1.0000    1.0000    1.0000         1\n",
            "              ↓0;d¦-+e+     0.0000    0.0000    0.0000         1\n",
            "          ↓0;d¦---→-+t+     0.0000    0.0000    0.0000         2\n",
            "          ↓0;d¦-→---+y+     1.0000    1.0000    1.0000         1\n",
            "           ↓0;d¦--+e+e+     1.0000    1.0000    1.0000         1\n",
            "             ↓0;d¦+e→-+     1.0000    1.0000    1.0000         1\n",
            "           ↓0;d¦--+l+l+     0.0000    0.0000    0.0000         1\n",
            "           ↓0;d-+s¦-+s+     0.0000    0.0000    0.0000         1\n",
            "               ↓0;d-¦-+     0.0000    0.0000    0.0000         1\n",
            "            ↓0;d¦-→+n→+     0.0000    0.0000    0.0000         1\n",
            "              ↓0;d-+s¦+     0.0000    0.0000    0.0000         1\n",
            "       ↑0¦↓1¦↑2¦↓-2;d¦+     0.0000    0.0000    0.0000         1\n",
            "                 ↓0;a''     0.0000    0.0000    0.0000         1\n",
            "           ↓0¦↑1¦↓2;d¦+     0.0000    0.0000    0.0000         1\n",
            "            ↓0;d¦-→→→→+     0.0000    0.0000    0.0000         1\n",
            "            ↓0;d¦-→→+e+     0.0000    0.0000    0.0000         1\n",
            "          ↓0¦↑4¦↓-4;d¦+     0.0000    0.0000    0.0000         1\n",
            "            ↓0;d¦-+u+s+     0.0000    0.0000    0.0000         1\n",
            "             ↓0;d¦+n→→+     0.0000    0.0000    0.0000         1\n",
            "                ↓0;afor     0.0000    0.0000    0.0000         1\n",
            "↑0¦↓1¦↑5¦↓6¦↑-5¦↓-4;d¦+     0.0000    0.0000    0.0000         1\n",
            "                 ↓0;ato     0.0000    0.0000    0.0000         1\n",
            "       ↑0¦↓1¦↑2¦↓-1;d¦+     0.0000    0.0000    0.0000         1\n",
            "            ↓0;d¦---+f+     0.0000    0.0000    0.0000         1\n",
            "          ↓0;d¦--+o+o→+     0.0000    0.0000    0.0000         1\n",
            "                ↓0;athe     0.0000    0.0000    0.0000         1\n",
            "           ↓0;d¦-+i→+e+     0.0000    0.0000    0.0000         1\n",
            "               ↓0;d+o¦+     0.0000    0.0000    0.0000         1\n",
            "         ↓0;d¦--→-+a+k+     0.0000    0.0000    0.0000         1\n",
            "        ↓0;d¦--+i+n→--+     0.0000    0.0000    0.0000         1\n",
            "          ↓0;d¦--+a+n→+     0.0000    0.0000    0.0000         1\n",
            "\n",
            "               accuracy                         0.9684      7860\n",
            "              macro avg     0.5451    0.5205    0.5261      7860\n",
            "           weighted avg     0.9645    0.9684    0.9655      7860\n",
            "\n",
            "2023-02-21 16:47:03,866 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_score': 0.9684478371501273,\n",
              " 'dev_score_history': [0.8697375615090214,\n",
              "  0.9168944778567524,\n",
              "  0.9328868234007655,\n",
              "  0.9439584472389284,\n",
              "  0.9471022416621104,\n",
              "  0.9510661563696009,\n",
              "  0.9558501913613997,\n",
              "  0.9596774193548387,\n",
              "  0.9573537452159651,\n",
              "  0.9610442864953527,\n",
              "  0.9632312739201749,\n",
              "  0.9667851284855112,\n",
              "  0.961864406779661,\n",
              "  0.9647348277747403,\n",
              "  0.9659650082012028,\n",
              "  0.9695188627665391,\n",
              "  0.9681519956260252,\n",
              "  0.9689721159103335,\n",
              "  0.9696555494805905,\n",
              "  0.9702022963367961],\n",
              " 'train_loss_history': [1.0005039109526812,\n",
              "  0.4945873994613341,\n",
              "  0.37910362244022516,\n",
              "  0.3141125241301643,\n",
              "  0.2789978361980388,\n",
              "  0.2543155323251964,\n",
              "  0.2334321607786623,\n",
              "  0.2157081805995122,\n",
              "  0.2086370251310654,\n",
              "  0.19538745860692858,\n",
              "  0.18595090382611726,\n",
              "  0.17564551980019125,\n",
              "  0.16972049185576157,\n",
              "  0.16436320507525048,\n",
              "  0.15854910670852052,\n",
              "  0.15161419079830507,\n",
              "  0.14723784443369325,\n",
              "  0.13867482848914783,\n",
              "  0.14142098506791698,\n",
              "  0.13507947665562178],\n",
              " 'dev_loss_history': [0.5385197997093201,\n",
              "  0.33010420203208923,\n",
              "  0.2550539970397949,\n",
              "  0.21091659367084503,\n",
              "  0.18944858014583588,\n",
              "  0.18367673456668854,\n",
              "  0.16604416072368622,\n",
              "  0.16101936995983124,\n",
              "  0.16022558510303497,\n",
              "  0.14543043076992035,\n",
              "  0.14295707643032074,\n",
              "  0.136047825217247,\n",
              "  0.13325029611587524,\n",
              "  0.13514719903469086,\n",
              "  0.13064870238304138,\n",
              "  0.12230341136455536,\n",
              "  0.1285804659128189,\n",
              "  0.12219937145709991,\n",
              "  0.12511852383613586,\n",
              "  0.129611074924469]}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCuoAb_ID9sO"
      },
      "source": [
        "# ASSIGNMENT 2\n",
        "\n",
        "In this assignment we will be using the trained model in the previous step to tag some texts.\n",
        "\n",
        "**NOTE**: If you use the Basque corpus, you can get a document to process in Basque from a newspaper: https://www.berria.eus/\n",
        "\n",
        "+ TODO: Pick a document of your choice and run the following Flair components:\n",
        "  + Tokenize and segment the document into sentences.\n",
        "  + Instiantiate a SequenceTagger with the generated lemmatizer model.\n",
        "  + Lemmatize the sentences and print the results (ideally saving the predictions into a list of Sentence objects)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilU4atxwpEXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97c8a5a4-b660-43dd-bbf1-fec2bc27b1d1"
      },
      "source": [
        "from flair.data import Sentence\n",
        "\n",
        "my_text=\"An unsuccessful attempt was made to pass an amendment to the Constitution of New Jersey in 1927. The legislature twice passed a proposal, subject to a popular vote, intended to increase the length of the terms of its members and the governor, with the text approved by the state attorney general. Then, it was realized that though the legislature intended that members of its lower house, the General Assembly, be elected biennially (once in two years), the text actually read that they were to be chosen biannually (twice a year). The press was considerably amused by this. \"\n",
        "tokenized_sentence = Sentence(my_text, use_tokenizer=True)\n",
        "tagger = SequenceTagger.load('/content/drive/MyDrive/NLP_Applications_1/DATA/2023-ILTAPP-20230203T201734Z-001/2023-ILTAPP/datasets/sigmorphon2019/final-model.pt')\n",
        "print(tagger.predict(tokenized_sentence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 17:05:05,968 loading file /content/drive/MyDrive/NLP_Applications_1/DATA/2023-ILTAPP-20230203T201734Z-001/2023-ILTAPP/datasets/sigmorphon2019/final-model.pt\n",
            "2023-02-21 17:05:06,450 SequenceTagger predicts: Dictionary with 144 tags: <unk>, ↓0;d¦+, ↑0¦↓1;d¦+, ↓0;d¦-+, ↓0;abe, ↓0;d¦--+, ↓0;d¦---+, ↓0;d--+b¦+, ↑0¦↓-1;d¦+, ↓0;d¦-+v+e+, ↓0;d¦---+e+, ↓0;d¦-+o→+, ↓0;d¦--+e+, ↓0;d¦--+y+, ↑0¦↓1;ai, ↓0;d¦---+y+, ↓0;awe, ↓0;d-+b¦--+, ↓0;d¦-+y+, ↓0;d¦--+o+, ↓0;d¦----+, ↓0;d+s¦-+, ↓0;d¦-+k→+, ↓0;d¦-+e→+, ↓0;d-+w+i¦+, ↓0;d¦-+e→-+l+, ↓0;d-+h+a¦+, ↓0;d+'¦+, ↓0;d¦--+a→+e+, ↓0;d¦-+d+, ↓0;ago, ↓0;d→-+o¦+, ↓0;d¦-+a→+, ↓0;d¦+e→-+, ↓0;d¦+n+, ↓0;d→--+i¦+, ↓0;d¦-----+i+n+k+, ↓0;d→-+i¦+, ↓0;d¦-+i→+, ↓0;d¦--+a+v+e+, ↑0¦↓1¦↑2¦↓3;d¦+, ↓0;d¦-+i+l+l+, ↓0;d¦-+i→→+, ↑0¦↓1¦↑2¦↓-2;d¦+, ↓0;d-+w+o+u+l¦+, ↓0;d¦+o→+, ↓0;a\", ↓0;d¦+s+, ↓0;d-+'¦+, ↓0;d¦+e→+\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J--3iN8xGy56"
      },
      "source": [
        "# ASSIGNMENT 3\n",
        "\n",
        "As you can see in the previous step, the model we trained predicts this weird lemma_rules based on the minimum script edits generated by the get_ses_affixes.py script (in the resources folder). In order to obtain the real lemma we need to decode the lemma_rule using the function _apply_lemma_rule(form, lemma_rule) below.\n",
        "\n",
        "+ This function takes as parameters the original word we want to lemmatize and the lemma_rule predicted by our lemmatizer model.\n",
        "\n",
        "  ```\n",
        "  decoded_lemma = _apply_lemma_rule('partidua', '↓0;d¦-+')\n",
        "  print(decoded_lemma)\n",
        "  'partidu'\n",
        "  ```\n",
        "Ideally you may have saved the previous predictions in a list of Sentence objects. Being that the case, you need to:\n",
        "\n",
        "+ TODO: Iterate over the sentences to extract the lemma prediction (or lemma_rule) which will be used, together with the word, as input to obtain the decoded lemma.\n",
        "+ TODO: print the results with the decoded lemma.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uqbq7RKBwvk"
      },
      "source": [
        "def _apply_lemma_rule(form, lemma_rule):\n",
        "    if ';' not in lemma_rule:\n",
        "        raise ValueError('lemma_rule %r for form %r missing semicolon' % (lemma_rule, form))\n",
        "    casing, rule = lemma_rule.split(\";\", 1)\n",
        "    if rule.startswith(\"a\"):\n",
        "        lemma = rule[1:]\n",
        "    else:\n",
        "        form = form.lower()\n",
        "        rules, rule_sources = rule[1:].split(\"¦\"), []\n",
        "        assert len(rules) == 2\n",
        "        for rule in rules:\n",
        "            source, i = 0, 0\n",
        "            while i < len(rule):\n",
        "                if rule[i] == \"→\" or rule[i] == \"-\":\n",
        "                    source += 1\n",
        "                else:\n",
        "                    assert rule[i] == \"+\"\n",
        "                    i += 1\n",
        "                i += 1\n",
        "            rule_sources.append(source)\n",
        "#_apply_lemma_rule(form, lemma_rule):\n",
        "        try:\n",
        "            lemma, form_offset = \"\", 0\n",
        "            for i in range(2):\n",
        "                j, offset = 0, (0 if i == 0 else len(form) - rule_sources[1])\n",
        "                while j < len(rules[i]):\n",
        "                    if rules[i][j] == \"→\":\n",
        "                        lemma += form[offset]\n",
        "                        offset += 1\n",
        "                    elif rules[i][j] == \"-\":\n",
        "                        offset += 1\n",
        "                    else:\n",
        "                        assert (rules[i][j] == \"+\")\n",
        "                        lemma += rules[i][j + 1]\n",
        "                        j += 1\n",
        "                    j += 1\n",
        "                    # print(lemma)\n",
        "                if i == 0:\n",
        "                    lemma += form[rule_sources[0]: len(form) - rule_sources[1]]\n",
        "        except:\n",
        "            lemma = lemma\n",
        "\n",
        "    for rule in casing.split(\"¦\"):\n",
        "        if rule == \"↓0\": continue  # The lemma is lowercased initially\n",
        "        if not rule: continue  # Empty lemma might generate empty casing rule\n",
        "        case, offset = rule[0], int(rule[1:])\n",
        "        lemma = lemma[:offset] + (lemma[offset:].upper() if case == \"↑\" else lemma[offset:].lower())\n",
        "    return lemma\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5G3Ghuzoe8k",
        "outputId": "f966cd4b-27a0-4c0c-8bec-fb0b67d4e770"
      },
      "source": [
        "!pip install syntok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: syntok in /usr/local/lib/python3.8/dist-packages (1.4.4)\n",
            "Requirement already satisfied: regex>2016 in /usr/local/lib/python3.8/dist-packages (from syntok) (2022.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn-T4fUhGhVE",
        "outputId": "d5b8f322-2793-4605-8da2-450831e41cc6"
      },
      "source": [
        "\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "from syntok.tokenizer import Tokenizer\n",
        "import syntok.segmenter as segmenter\n",
        "\n",
        "file = open('/content/drive/MyDrive/NLP_Applications_1/DATA/2023-ILTAPP-20230203T201734Z-001/2023-ILTAPP/resources/guardian.txt', encoding='utf-8')\n",
        "input_txt = file.read()\n",
        "sentence_tokens = segmenter.analyze(input_txt)\n",
        "\n",
        "tokenized_sent = []\n",
        "\n",
        "for elements in sentence_tokens:\n",
        "  for sent in elements:\n",
        "    words = []\n",
        "    for word in sent:\n",
        "      words.append(word.value)\n",
        "    tokenized_sent.append(Sentence(\" \".join(words)))\n",
        "\n",
        "#lemmatizer model for prediction\n",
        "lemmatizer_model = SequenceTagger.load('/content/drive/MyDrive/NLP_Applications_1/DATA/2023-ILTAPP-20230203T201734Z-001/2023-ILTAPP/datasets/sigmorphon2019/best-model.pt')\n",
        "lemmatizer_model.predict(tokenized_sent)\n",
        "\n",
        "\n",
        "text_output = []\n",
        "for sentence in tokenized_sent:\n",
        "  \n",
        "  for stem in sentence.get_labels('lemma'):\n",
        "    print(stem)\n",
        "    decoded_text = _apply_lemma_rule(stem.data_point.text, stem.value)\n",
        "    text_output.append(f\"{stem.data_point.text}\\t{decoded_text}\\n\")\n",
        "  text_output.append(\"\\n\")\n",
        "\n",
        "final_file = open('/content/drive/MyDrive/NLP_Applications_1/DATA/2023-ILTAPP-20230203T201734Z-001/2023-ILTAPP/resources/English-lemmatized.txt', mode='w', encoding='utf-8')\n",
        "final_file.write(\"\".join(text_output))\n",
        "print(\"\".join(text_output))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 17:39:31,638 loading file /content/drive/MyDrive/NLP_Applications_1/DATA/2023-ILTAPP-20230203T201734Z-001/2023-ILTAPP/datasets/sigmorphon2019/best-model.pt\n",
            "2023-02-21 17:39:32,137 SequenceTagger predicts: Dictionary with 144 tags: <unk>, ↓0;d¦+, ↑0¦↓1;d¦+, ↓0;d¦-+, ↓0;abe, ↓0;d¦--+, ↓0;d¦---+, ↓0;d--+b¦+, ↑0¦↓-1;d¦+, ↓0;d¦-+v+e+, ↓0;d¦---+e+, ↓0;d¦-+o→+, ↓0;d¦--+e+, ↓0;d¦--+y+, ↑0¦↓1;ai, ↓0;d¦---+y+, ↓0;awe, ↓0;d-+b¦--+, ↓0;d¦-+y+, ↓0;d¦--+o+, ↓0;d¦----+, ↓0;d+s¦-+, ↓0;d¦-+k→+, ↓0;d¦-+e→+, ↓0;d-+w+i¦+, ↓0;d¦-+e→-+l+, ↓0;d-+h+a¦+, ↓0;d+'¦+, ↓0;d¦--+a→+e+, ↓0;d¦-+d+, ↓0;ago, ↓0;d→-+o¦+, ↓0;d¦-+a→+, ↓0;d¦+e→-+, ↓0;d¦+n+, ↓0;d→--+i¦+, ↓0;d¦-----+i+n+k+, ↓0;d→-+i¦+, ↓0;d¦-+i→+, ↓0;d¦--+a+v+e+, ↑0¦↓1¦↑2¦↓3;d¦+, ↓0;d¦-+i+l+l+, ↓0;d¦-+i→→+, ↑0¦↓1¦↑2¦↓-2;d¦+, ↓0;d-+w+o+u+l¦+, ↓0;d¦+o→+, ↓0;a\", ↓0;d¦+s+, ↓0;d-+'¦+, ↓0;d¦+e→+\n",
            "Token[0]: \"Twelve\" → ↓0;d¦+ (1.0)\n",
            "Token[1]: \"years\" → ↓0;d¦-+ (1.0)\n",
            "Token[2]: \"after\" → ↓0;d¦+ (0.9999)\n",
            "Token[3]: \"the\" → ↓0;d¦+ (0.9425)\n",
            "Token[4]: \"fall\" → ↓0;d¦+ (0.9992)\n",
            "Token[5]: \"of\" → ↓0;d¦+ (0.9863)\n",
            "Token[6]: \"the\" → ↓0;d¦+ (0.9992)\n",
            "Token[7]: \"Taliban\" → ↑0¦↓1;d¦+ (0.9987)\n",
            "Token[8]: \",\" → ↓0;d¦+ (0.966)\n",
            "Token[9]: \"Afghanistan\" → ↑0¦↓1;d¦+ (0.9979)\n",
            "Token[10]: \"is\" → ↓0;abe (0.9999)\n",
            "Token[11]: \"heading\" → ↓0;d¦---+ (0.9906)\n",
            "Token[12]: \"for\" → ↓0;d¦+ (1.0)\n",
            "Token[13]: \"a\" → ↓0;d¦+ (1.0)\n",
            "Token[14]: \"near\" → ↓0;d¦+ (1.0)\n",
            "Token[15]: \"record\" → ↓0;d¦+ (0.9999)\n",
            "Token[16]: \"opium\" → ↓0;d¦+ (0.9911)\n",
            "Token[17]: \"crop\" → ↓0;d¦+ (0.9994)\n",
            "Token[18]: \"as\" → ↓0;d¦+ (0.9968)\n",
            "Token[19]: \"instability\" → ↓0;d¦+ (0.9966)\n",
            "Token[20]: \"pushes\" → ↓0;d¦-+ (0.7643)\n",
            "Token[21]: \"up\" → ↓0;d¦+ (1.0)\n",
            "Token[22]: \"the\" → ↓0;d¦+ (1.0)\n",
            "Token[23]: \"amount\" → ↓0;d¦+ (0.9997)\n",
            "Token[24]: \"of\" → ↓0;d¦+ (1.0)\n",
            "Token[25]: \"land\" → ↓0;d¦+ (0.946)\n",
            "Token[26]: \"planted\" → ↓0;d¦--+ (0.9133)\n",
            "Token[27]: \"with\" → ↓0;d¦+ (1.0)\n",
            "Token[28]: \"illegal\" → ↓0;d¦+ (0.9986)\n",
            "Token[29]: \"but\" → ↓0;d¦+ (1.0)\n",
            "Token[30]: \"lucrative\" → ↓0;d¦+ (0.9998)\n",
            "Token[31]: \"poppies\" → ↓0;d¦---+y+ (0.986)\n",
            "Token[32]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[33]: \"according\" → ↓0;d¦---+ (0.9994)\n",
            "Token[34]: \"to\" → ↓0;d¦+ (0.9999)\n",
            "Token[35]: \"a\" → ↓0;d¦+ (1.0)\n",
            "Token[36]: \"bleak\" → ↓0;d¦+ (0.9974)\n",
            "Token[37]: \"UN\" → ↑0¦↓-1;d¦+ (0.9203)\n",
            "Token[38]: \"report\" → ↓0;d¦+ (0.9968)\n",
            "Token[39]: \".\" → ↓0;d¦+ (0.9999)\n",
            "Token[0]: \"The\" → ↓0;d¦+ (1.0)\n",
            "Token[1]: \"rapid\" → ↓0;d¦+ (0.9959)\n",
            "Token[2]: \"growth\" → ↓0;d¦+ (0.9762)\n",
            "Token[3]: \"of\" → ↓0;d¦+ (1.0)\n",
            "Token[4]: \"poppy\" → ↓0;d¦+ (0.9988)\n",
            "Token[5]: \"farming\" → ↓0;d¦+ (0.6675)\n",
            "Token[6]: \"as\" → ↓0;d¦+ (0.998)\n",
            "Token[7]: \"western\" → ↓0;d¦+ (0.9999)\n",
            "Token[8]: \"troops\" → ↓0;d¦-+ (0.9959)\n",
            "Token[9]: \"head\" → ↓0;d¦+ (0.8767)\n",
            "Token[10]: \"home\" → ↓0;d¦+ (0.9999)\n",
            "Token[11]: \"reflects\" → ↓0;d¦-+ (0.9905)\n",
            "Token[12]: \"particularly\" → ↓0;d¦+ (1.0)\n",
            "Token[13]: \"badly\" → ↓0;d¦+ (1.0)\n",
            "Token[14]: \"on\" → ↓0;d¦+ (0.9994)\n",
            "Token[15]: \"Britain\" → ↑0¦↓1;d¦+ (0.9998)\n",
            "Token[16]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[17]: \"which\" → ↓0;d¦+ (0.9999)\n",
            "Token[18]: \"was\" → ↓0;abe (0.9997)\n",
            "Token[19]: \"designated\" → ↓0;d¦-+ (0.9966)\n",
            "Token[20]: \"\"\" → ↓0;d¦+ (0.9999)\n",
            "Token[21]: \"lead\" → ↓0;d¦+ (0.9956)\n",
            "Token[22]: \"nation\" → ↓0;d¦+ (0.9866)\n",
            "Token[23]: \"\"\" → ↓0;d¦+ (0.9998)\n",
            "Token[24]: \"for\" → ↓0;d¦+ (1.0)\n",
            "Token[25]: \"counter\" → ↓0;d¦+ (0.9994)\n",
            "Token[26]: \"narcotics\" → ↓0;d¦+ (0.5499)\n",
            "Token[27]: \"work\" → ↓0;d¦+ (0.9856)\n",
            "Token[28]: \"over\" → ↓0;d¦+ (1.0)\n",
            "Token[29]: \"a\" → ↓0;d¦+ (1.0)\n",
            "Token[30]: \"decade\" → ↓0;d¦+ (0.9238)\n",
            "Token[31]: \"ago\" → ↓0;d¦+ (0.9999)\n",
            "Token[32]: \".\" → ↓0;d¦+ (0.9999)\n",
            "Token[0]: \"\"\" → ↓0;d¦+ (1.0)\n",
            "Token[1]: \"Poppy\" → ↑0¦↓1;d¦+ (0.7247)\n",
            "Token[2]: \"cultivation\" → ↓0;d¦+ (0.9881)\n",
            "Token[3]: \"is\" → ↓0;abe (0.9999)\n",
            "Token[4]: \"not\" → ↓0;d¦+ (1.0)\n",
            "Token[5]: \"only\" → ↓0;d¦+ (1.0)\n",
            "Token[6]: \"expected\" → ↓0;d¦--+ (0.9379)\n",
            "Token[7]: \"to\" → ↓0;d¦+ (1.0)\n",
            "Token[8]: \"expand\" → ↓0;d¦+ (0.9989)\n",
            "Token[9]: \"in\" → ↓0;d¦+ (1.0)\n",
            "Token[10]: \"areas\" → ↓0;d¦-+ (0.9949)\n",
            "Token[11]: \"where\" → ↓0;d¦+ (0.9999)\n",
            "Token[12]: \"it\" → ↓0;d¦+ (1.0)\n",
            "Token[13]: \"already\" → ↓0;d¦+ (1.0)\n",
            "Token[14]: \"existed\" → ↓0;d¦--+ (0.9651)\n",
            "Token[15]: \"in\" → ↓0;d¦+ (1.0)\n",
            "Token[16]: \"2012\" → ↓0;d¦+ (0.9998)\n",
            "Token[17]: \"…\" → ↓0;d¦+ (0.9845)\n",
            "Token[18]: \"but\" → ↓0;d¦+ (0.9999)\n",
            "Token[19]: \"also\" → ↓0;d¦+ (0.9999)\n",
            "Token[20]: \"in\" → ↓0;d¦+ (1.0)\n",
            "Token[21]: \"new\" → ↓0;d¦+ (0.9997)\n",
            "Token[22]: \"areas\" → ↓0;d¦-+ (0.9925)\n",
            "Token[23]: \"or\" → ↓0;d¦+ (1.0)\n",
            "Token[24]: \"areas\" → ↓0;d¦-+ (0.9995)\n",
            "Token[25]: \"where\" → ↓0;d¦+ (1.0)\n",
            "Token[26]: \"poppy\" → ↓0;d¦+ (0.9996)\n",
            "Token[27]: \"cultivation\" → ↓0;d¦+ (0.9965)\n",
            "Token[28]: \"was\" → ↓0;abe (0.9995)\n",
            "Token[29]: \"stopped\" → ↓0;d¦---+ (0.9626)\n",
            "Token[30]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[31]: \"\"\" → ↓0;d¦+ (0.9999)\n",
            "Token[32]: \"the\" → ↓0;d¦+ (0.9999)\n",
            "Token[33]: \"Afghanistan\" → ↑0¦↓1;d¦+ (0.9309)\n",
            "Token[34]: \"Opium\" → ↑0¦↓1;d¦+ (0.9945)\n",
            "Token[35]: \"Winter\" → ↑0¦↓1;d¦+ (0.9623)\n",
            "Token[36]: \"Risk\" → ↑0¦↓1;d¦+ (0.8827)\n",
            "Token[37]: \"Assessment\" → ↑0¦↓1;d¦+ (0.963)\n",
            "Token[38]: \"found\" → ↓0;d→--+i¦+ (0.7515)\n",
            "Token[39]: \".\" → ↓0;d¦+ (1.0)\n",
            "Token[0]: \"The\" → ↓0;d¦+ (1.0)\n",
            "Token[1]: \"growth\" → ↓0;d¦+ (0.9972)\n",
            "Token[2]: \"in\" → ↓0;d¦+ (0.9999)\n",
            "Token[3]: \"opium\" → ↓0;d¦+ (0.9626)\n",
            "Token[4]: \"cultivation\" → ↓0;d¦+ (0.9092)\n",
            "Token[5]: \"reflects\" → ↓0;d¦-+ (0.9994)\n",
            "Token[6]: \"both\" → ↓0;d¦+ (1.0)\n",
            "Token[7]: \"spreading\" → ↓0;d¦---+ (0.9819)\n",
            "Token[8]: \"instability\" → ↓0;d¦+ (0.9994)\n",
            "Token[9]: \"and\" → ↓0;d¦+ (0.9997)\n",
            "Token[10]: \"concerns\" → ↓0;d¦-+ (0.9999)\n",
            "Token[11]: \"about\" → ↓0;d¦+ (0.9997)\n",
            "Token[12]: \"the\" → ↓0;d¦+ (1.0)\n",
            "Token[13]: \"future\" → ↓0;d¦+ (0.9992)\n",
            "Token[14]: \".\" → ↓0;d¦+ (1.0)\n",
            "Token[0]: \"Farmers\" → ↓0;d¦-+ (0.9989)\n",
            "Token[1]: \"are\" → ↓0;d--+b¦+ (1.0)\n",
            "Token[2]: \"more\" → ↓0;d¦+ (0.9999)\n",
            "Token[3]: \"likely\" → ↓0;d¦+ (1.0)\n",
            "Token[4]: \"to\" → ↓0;d¦+ (1.0)\n",
            "Token[5]: \"plant\" → ↓0;d¦+ (0.9984)\n",
            "Token[6]: \"the\" → ↓0;d¦+ (1.0)\n",
            "Token[7]: \"deadly\" → ↓0;d¦+ (0.9984)\n",
            "Token[8]: \"crop\" → ↓0;d¦+ (0.9986)\n",
            "Token[9]: \"in\" → ↓0;d¦+ (1.0)\n",
            "Token[10]: \"areas\" → ↓0;d¦-+ (0.9994)\n",
            "Token[11]: \"of\" → ↓0;d¦+ (1.0)\n",
            "Token[12]: \"high\" → ↓0;d¦+ (1.0)\n",
            "Token[13]: \"violence\" → ↓0;d¦+ (0.9969)\n",
            "Token[14]: \"or\" → ↓0;d¦+ (1.0)\n",
            "Token[15]: \"where\" → ↓0;d¦+ (1.0)\n",
            "Token[16]: \"they\" → ↓0;d¦+ (1.0)\n",
            "Token[17]: \"have\" → ↓0;d¦+ (0.9999)\n",
            "Token[18]: \"not\" → ↓0;d¦+ (0.9999)\n",
            "Token[19]: \"received\" → ↓0;d¦-+ (0.9998)\n",
            "Token[20]: \"any\" → ↓0;d¦+ (1.0)\n",
            "Token[21]: \"agricultural\" → ↓0;d¦+ (0.9987)\n",
            "Token[22]: \"aid\" → ↓0;d¦+ (0.9988)\n",
            "Token[23]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[24]: \"the\" → ↓0;d¦+ (1.0)\n",
            "Token[25]: \"report\" → ↓0;d¦+ (0.9992)\n",
            "Token[26]: \"said\" → ↓0;d¦--+y+ (0.9996)\n",
            "Token[27]: \".\" → ↓0;d¦+ (1.0)\n",
            "Token[0]: \"Opium\" → ↓0;d¦+ (0.732)\n",
            "Token[1]: \"traders\" → ↓0;d¦-+ (0.9985)\n",
            "Token[2]: \"are\" → ↓0;d--+b¦+ (0.9999)\n",
            "Token[3]: \"often\" → ↓0;d¦+ (0.9901)\n",
            "Token[4]: \"happy\" → ↓0;d¦+ (0.9999)\n",
            "Token[5]: \"to\" → ↓0;d¦+ (1.0)\n",
            "Token[6]: \"provide\" → ↓0;d¦+ (0.9999)\n",
            "Token[7]: \"seeds\" → ↓0;d¦-+ (0.9982)\n",
            "Token[8]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[9]: \"fertilisers\" → ↓0;d¦-+ (0.9983)\n",
            "Token[10]: \"and\" → ↓0;d¦+ (0.9998)\n",
            "Token[11]: \"even\" → ↓0;d¦+ (0.9995)\n",
            "Token[12]: \"advance\" → ↓0;d¦+ (0.9999)\n",
            "Token[13]: \"payments\" → ↓0;d¦-+ (0.9999)\n",
            "Token[14]: \"to\" → ↓0;d¦+ (1.0)\n",
            "Token[15]: \"encourage\" → ↓0;d¦+ (0.9988)\n",
            "Token[16]: \"crops\" → ↓0;d¦-+ (0.9978)\n",
            "Token[17]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[18]: \"leaving\" → ↓0;d¦---+e+ (0.9858)\n",
            "Token[19]: \"farmers\" → ↓0;d¦-+ (0.9996)\n",
            "Token[20]: \"who\" → ↓0;d¦+ (1.0)\n",
            "Token[21]: \"do\" → ↓0;d¦+ (0.9999)\n",
            "Token[22]: \"not\" → ↓0;d¦+ (0.9968)\n",
            "Token[23]: \"have\" → ↓0;d¦+ (1.0)\n",
            "Token[24]: \"western\" → ↓0;d¦+ (0.9994)\n",
            "Token[25]: \"or\" → ↓0;d¦+ (1.0)\n",
            "Token[26]: \"government\" → ↓0;d¦+ (0.9996)\n",
            "Token[27]: \"agricultural\" → ↓0;d¦+ (0.9988)\n",
            "Token[28]: \"help\" → ↓0;d¦+ (0.9998)\n",
            "Token[29]: \"very\" → ↓0;d¦+ (1.0)\n",
            "Token[30]: \"vulnerable\" → ↓0;d¦+ (0.9995)\n",
            "Token[31]: \"to\" → ↓0;d¦+ (0.9999)\n",
            "Token[32]: \"their\" → ↓0;d¦--+y+ (0.9995)\n",
            "Token[33]: \"inducements\" → ↓0;d¦-+ (0.9995)\n",
            "Token[34]: \".\" → ↓0;d¦+ (1.0)\n",
            "Token[0]: \"At\" → ↓0;d¦+ (0.9999)\n",
            "Token[1]: \"the\" → ↓0;d¦+ (1.0)\n",
            "Token[2]: \"same\" → ↓0;d¦+ (1.0)\n",
            "Token[3]: \"time\" → ↓0;d¦+ (0.9991)\n",
            "Token[4]: \"the\" → ↓0;d¦+ (0.9915)\n",
            "Token[5]: \"more\" → ↓0;d¦+ (1.0)\n",
            "Token[6]: \"powerful\" → ↓0;d¦+ (0.9964)\n",
            "Token[7]: \"figures\" → ↓0;d¦-+ (0.9981)\n",
            "Token[8]: \"in\" → ↓0;d¦+ (1.0)\n",
            "Token[9]: \"the\" → ↓0;d¦+ (0.9765)\n",
            "Token[10]: \"drugs\" → ↓0;d¦-+ (0.8849)\n",
            "Token[11]: \"trade\" → ↓0;d¦+ (0.9951)\n",
            "Token[12]: \",\" → ↓0;d¦+ (0.9572)\n",
            "Token[13]: \"from\" → ↓0;d¦+ (0.9972)\n",
            "Token[14]: \"traffickers\" → ↓0;d¦-+ (0.999)\n",
            "Token[15]: \"to\" → ↓0;d¦+ (1.0)\n",
            "Token[16]: \"corrupt\" → ↓0;d¦+ (0.9991)\n",
            "Token[17]: \"government\" → ↓0;d¦+ (0.9928)\n",
            "Token[18]: \"officials\" → ↓0;d¦-+ (0.9999)\n",
            "Token[19]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[20]: \"who\" → ↓0;d¦+ (0.9999)\n",
            "Token[21]: \"take\" → ↓0;d¦+ (0.8841)\n",
            "Token[22]: \"over\" → ↓0;d¦+ (0.9999)\n",
            "Token[23]: \"half\" → ↓0;d¦+ (1.0)\n",
            "Token[24]: \"the\" → ↓0;d¦+ (1.0)\n",
            "Token[25]: \"profit\" → ↓0;d¦+ (0.9991)\n",
            "Token[26]: \"from\" → ↓0;d¦+ (0.9989)\n",
            "Token[27]: \"each\" → ↓0;d¦+ (0.9999)\n",
            "Token[28]: \"kilo\" → ↓0;d¦+ (0.9996)\n",
            "Token[29]: \"of\" → ↓0;d¦+ (1.0)\n",
            "Token[30]: \"opium\" → ↓0;d¦+ (0.9783)\n",
            "Token[31]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[32]: \"have\" → ↓0;d¦+ (0.999)\n",
            "Token[33]: \"shrinking\" → ↓0;d¦---+ (0.8876)\n",
            "Token[34]: \"opportunities\" → ↓0;d¦---+y+ (0.9836)\n",
            "Token[35]: \"to\" → ↓0;d¦+ (1.0)\n",
            "Token[36]: \"earn\" → ↓0;d¦+ (0.9993)\n",
            "Token[37]: \"money\" → ↓0;d¦+ (0.9542)\n",
            "Token[38]: \"from\" → ↓0;d¦+ (1.0)\n",
            "Token[39]: \"Nato\" → ↑0¦↓1;d¦+ (0.999)\n",
            "Token[40]: \"or\" → ↓0;d¦+ (1.0)\n",
            "Token[41]: \"international\" → ↓0;d¦+ (0.9996)\n",
            "Token[42]: \"aid\" → ↓0;d¦+ (0.9995)\n",
            "Token[43]: \"contracts\" → ↓0;d¦-+ (0.9999)\n",
            "Token[44]: \"–\" → ↓0;d¦+ (0.539)\n",
            "Token[45]: \"and\" → ↓0;d¦+ (0.9995)\n",
            "Token[46]: \"may\" → ↓0;d¦+ (1.0)\n",
            "Token[47]: \"be\" → ↓0;d¦+ (0.9999)\n",
            "Token[48]: \"preparing\" → ↓0;d¦---+e+ (0.8419)\n",
            "Token[49]: \"a\" → ↓0;d¦+ (1.0)\n",
            "Token[50]: \"war\" → ↓0;d¦+ (0.9998)\n",
            "Token[51]: \"chest\" → ↓0;d¦+ (0.9952)\n",
            "Token[52]: \"for\" → ↓0;d¦+ (0.9999)\n",
            "Token[53]: \"upcoming\" → ↓0;d¦---+e+ (0.6112)\n",
            "Token[54]: \"presidential\" → ↓0;d¦+ (0.9323)\n",
            "Token[55]: \"and\" → ↓0;d¦+ (1.0)\n",
            "Token[56]: \"parliamentary\" → ↓0;d¦+ (0.9854)\n",
            "Token[57]: \"elections\" → ↓0;d¦-+ (0.9995)\n",
            "Token[58]: \".\" → ↓0;d¦+ (1.0)\n",
            "Token[0]: \"\"\" → ↓0;d¦+ (1.0)\n",
            "Token[1]: \"Opium\" → ↑0¦↓1;d¦+ (0.9474)\n",
            "Token[2]: \"cultivation\" → ↓0;d¦+ (0.9776)\n",
            "Token[3]: \"is\" → ↓0;abe (0.9997)\n",
            "Token[4]: \"up\" → ↓0;d¦+ (0.999)\n",
            "Token[5]: \"for\" → ↓0;d¦+ (1.0)\n",
            "Token[6]: \"the\" → ↓0;d¦+ (0.9993)\n",
            "Token[7]: \"third\" → ↓0;d¦+ (1.0)\n",
            "Token[8]: \"successive\" → ↓0;d¦+ (0.9999)\n",
            "Token[9]: \"year\" → ↓0;d¦+ (0.7352)\n",
            "Token[10]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[11]: \"and\" → ↓0;d¦+ (0.9995)\n",
            "Token[12]: \"production\" → ↓0;d¦+ (0.9993)\n",
            "Token[13]: \"is\" → ↓0;abe (0.9999)\n",
            "Token[14]: \"heading\" → ↓0;d¦---+ (0.9714)\n",
            "Token[15]: \"towards\" → ↓0;d¦+ (0.9616)\n",
            "Token[16]: \"record\" → ↓0;d¦+ (0.9998)\n",
            "Token[17]: \"levels\" → ↓0;d¦-+ (0.9997)\n",
            "Token[18]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[19]: \"\"\" → ↓0;d¦+ (0.9995)\n",
            "Token[20]: \"said\" → ↓0;d¦--+y+ (0.9994)\n",
            "Token[21]: \"Jean\" → ↑0¦↓1;d¦+ (0.9923)\n",
            "Token[22]: \"Luc\" → ↑0¦↓1;d¦+ (0.9998)\n",
            "Token[23]: \"Lemahieu\" → ↑0¦↓1;d¦+ (0.9982)\n",
            "Token[24]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[25]: \"Afghanistan\" → ↑0¦↓1;d¦+ (0.9802)\n",
            "Token[26]: \"head\" → ↓0;d¦+ (0.8737)\n",
            "Token[27]: \"of\" → ↓0;d¦+ (1.0)\n",
            "Token[28]: \"the\" → ↓0;d¦+ (0.9995)\n",
            "Token[29]: \"UN\" → ↑0¦↓-1;d¦+ (0.9255)\n",
            "Token[30]: \"Office\" → ↑0¦↓1;d¦+ (0.7749)\n",
            "Token[31]: \"on\" → ↓0;d¦+ (0.9999)\n",
            "Token[32]: \"Drugs\" → ↑0¦↓1;d¦+ (0.9123)\n",
            "Token[33]: \"and\" → ↓0;d¦+ (0.9896)\n",
            "Token[34]: \"Crime\" → ↑0¦↓1;d¦+ (0.9939)\n",
            "Token[35]: \".\" → ↓0;d¦+ (0.9997)\n",
            "Token[0]: \"\"\" → ↓0;d¦+ (1.0)\n",
            "Token[1]: \"People\" → ↓0;d¦+ (0.9928)\n",
            "Token[2]: \"are\" → ↓0;d--+b¦+ (0.9999)\n",
            "Token[3]: \"hedging\" → ↓0;d¦---+ (0.7824)\n",
            "Token[4]: \"against\" → ↓0;d¦+ (0.9996)\n",
            "Token[5]: \"an\" → ↓0;d¦-+ (1.0)\n",
            "Token[6]: \"insecure\" → ↓0;d¦+ (0.9991)\n",
            "Token[7]: \"future\" → ↓0;d¦+ (0.9999)\n",
            "Token[8]: \"both\" → ↓0;d¦+ (1.0)\n",
            "Token[9]: \"politically\" → ↓0;d¦+ (1.0)\n",
            "Token[10]: \"and\" → ↓0;d¦+ (0.9961)\n",
            "Token[11]: \"economically\" → ↓0;d¦+ (0.9992)\n",
            "Token[12]: \".\" → ↓0;d¦+ (1.0)\n",
            "Token[13]: \"\"\" → ↓0;d¦+ (0.9997)\n",
            "Token[0]: \"Just\" → ↓0;d¦+ (0.9977)\n",
            "Token[1]: \"14\" → ↓0;d¦+ (1.0)\n",
            "Token[2]: \"of\" → ↓0;d¦+ (0.9998)\n",
            "Token[3]: \"Afghanistan\" → ↑0¦↓1;d¦+ (0.9999)\n",
            "Token[4]: \"'\" → ↓0;d¦+ (0.8131)\n",
            "Token[5]: \"s\" → ↓0;d¦-+ (0.8802)\n",
            "Token[6]: \"34\" → ↓0;d¦+ (1.0)\n",
            "Token[7]: \"provinces\" → ↓0;d¦-+ (0.9983)\n",
            "Token[8]: \"are\" → ↓0;d--+b¦+ (0.9993)\n",
            "Token[9]: \"now\" → ↓0;d¦+ (1.0)\n",
            "Token[10]: \"\"\" → ↓0;d¦+ (0.9993)\n",
            "Token[11]: \"poppy\" → ↓0;d¦+ (0.9948)\n",
            "Token[12]: \"free\" → ↓0;d¦+ (0.9968)\n",
            "Token[13]: \"\"\" → ↓0;d¦+ (0.9998)\n",
            "Token[14]: \",\" → ↓0;d¦+ (0.9999)\n",
            "Token[15]: \"down\" → ↓0;d¦+ (0.5596)\n",
            "Token[16]: \"from\" → ↓0;d¦+ (1.0)\n",
            "Token[17]: \"20\" → ↓0;d¦+ (0.9998)\n",
            "Token[18]: \"in\" → ↓0;d¦+ (1.0)\n",
            "Token[19]: \"2010\" → ↓0;d¦+ (0.9864)\n",
            "Token[20]: \".\" → ↓0;d¦+ (1.0)\n",
            "Token[0]: \"In\" → ↓0;d¦+ (1.0)\n",
            "Token[1]: \"three\" → ↓0;d¦+ (1.0)\n",
            "Token[2]: \"provinces\" → ↓0;d¦-+ (0.9997)\n",
            "Token[3]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[4]: \"the\" → ↓0;d¦+ (0.9815)\n",
            "Token[5]: \"spring\" → ↓0;d¦+ (0.9968)\n",
            "Token[6]: \"sowing\" → ↓0;d¦+ (0.6933)\n",
            "Token[7]: \"was\" → ↓0;abe (0.9995)\n",
            "Token[8]: \"the\" → ↓0;d¦+ (1.0)\n",
            "Token[9]: \"first\" → ↓0;d¦+ (0.9999)\n",
            "Token[10]: \"time\" → ↓0;d¦+ (0.9999)\n",
            "Token[11]: \"this\" → ↓0;d¦+ (0.9999)\n",
            "Token[12]: \"decade\" → ↓0;d¦+ (0.9978)\n",
            "Token[13]: \"that\" → ↓0;d¦+ (0.9999)\n",
            "Token[14]: \"farmers\" → ↓0;d¦-+ (0.998)\n",
            "Token[15]: \"had\" → ↓0;d¦-+v+e+ (0.9999)\n",
            "Token[16]: \"risked\" → ↓0;d¦--+ (0.6759)\n",
            "Token[17]: \"an\" → ↓0;d¦-+ (1.0)\n",
            "Token[18]: \"attempt\" → ↓0;d¦+ (0.9811)\n",
            "Token[19]: \"at\" → ↓0;d¦+ (1.0)\n",
            "Token[20]: \"growing\" → ↓0;d¦---+ (0.9919)\n",
            "Token[21]: \"opium\" → ↓0;d¦+ (0.9964)\n",
            "Token[22]: \".\" → ↓0;d¦+ (1.0)\n",
            "Token[0]: \"The\" → ↓0;d¦+ (1.0)\n",
            "Token[1]: \"only\" → ↓0;d¦+ (1.0)\n",
            "Token[2]: \"figures\" → ↓0;d¦-+ (0.996)\n",
            "Token[3]: \"showing\" → ↓0;d¦---+ (0.9987)\n",
            "Token[4]: \"a\" → ↓0;d¦+ (0.9994)\n",
            "Token[5]: \"fall\" → ↓0;d¦+ (0.9997)\n",
            "Token[6]: \"in\" → ↓0;d¦+ (0.9998)\n",
            "Token[7]: \"cultivation\" → ↓0;d¦+ (0.9875)\n",
            "Token[8]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[9]: \"for\" → ↓0;d¦+ (1.0)\n",
            "Token[10]: \"western\" → ↓0;d¦+ (0.9996)\n",
            "Token[11]: \"Herat\" → ↑0¦↓1;d¦+ (0.9982)\n",
            "Token[12]: \"province\" → ↓0;d¦+ (0.6841)\n",
            "Token[13]: \",\" → ↓0;d¦+ (0.9998)\n",
            "Token[14]: \"may\" → ↓0;d¦+ (1.0)\n",
            "Token[15]: \"actually\" → ↓0;d¦+ (1.0)\n",
            "Token[16]: \"be\" → ↓0;d¦+ (0.9999)\n",
            "Token[17]: \"due\" → ↓0;d¦+ (0.9996)\n",
            "Token[18]: \"to\" → ↓0;d¦+ (0.9999)\n",
            "Token[19]: \"a\" → ↓0;d¦+ (0.9999)\n",
            "Token[20]: \"statistics\" → ↓0;d¦-+ (0.8457)\n",
            "Token[21]: \"blip\" → ↓0;d¦+ (0.9999)\n",
            "Token[22]: \".\" → ↓0;d¦+ (1.0)\n",
            "Token[0]: \"The\" → ↓0;d¦+ (1.0)\n",
            "Token[1]: \"UN\" → ↑0¦↓-1;d¦+ (0.9912)\n",
            "Token[2]: \"was\" → ↓0;abe (0.9999)\n",
            "Token[3]: \"forced\" → ↓0;d¦-+ (0.9995)\n",
            "Token[4]: \"to\" → ↓0;d¦+ (1.0)\n",
            "Token[5]: \"use\" → ↓0;d¦+ (0.9999)\n",
            "Token[6]: \"external\" → ↓0;d¦+ (0.9995)\n",
            "Token[7]: \"data\" → ↓0;d¦+ (0.9792)\n",
            "Token[8]: \"last\" → ↓0;d¦+ (0.9999)\n",
            "Token[9]: \"year\" → ↓0;d¦+ (0.9953)\n",
            "Token[10]: \"instead\" → ↓0;d¦+ (0.9993)\n",
            "Token[11]: \"of\" → ↓0;d¦+ (0.8884)\n",
            "Token[12]: \"the\" → ↓0;d¦+ (0.9998)\n",
            "Token[13]: \"satellite\" → ↓0;d¦+ (0.9842)\n",
            "Token[14]: \"images\" → ↓0;d¦-+ (0.9984)\n",
            "Token[15]: \"that\" → ↓0;d¦+ (1.0)\n",
            "Token[16]: \"are\" → ↓0;d--+b¦+ (0.999)\n",
            "Token[17]: \"usually\" → ↓0;d¦+ (1.0)\n",
            "Token[18]: \"the\" → ↓0;d¦+ (0.9993)\n",
            "Token[19]: \"basis\" → ↓0;d¦+ (0.9519)\n",
            "Token[20]: \"of\" → ↓0;d¦+ (1.0)\n",
            "Token[21]: \"poppy\" → ↓0;d¦+ (0.9981)\n",
            "Token[22]: \"growing\" → ↓0;d¦+ (0.6258)\n",
            "Token[23]: \"calculations\" → ↓0;d¦-+ (0.9991)\n",
            "Token[24]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[25]: \"and\" → ↓0;d¦+ (1.0)\n",
            "Token[26]: \"local\" → ↓0;d¦+ (0.9998)\n",
            "Token[27]: \"officials\" → ↓0;d¦-+ (0.9998)\n",
            "Token[28]: \"protested\" → ↓0;d¦--+ (0.9745)\n",
            "Token[29]: \"heavily\" → ↓0;d¦+ (1.0)\n",
            "Token[30]: \"that\" → ↓0;d¦+ (0.9999)\n",
            "Token[31]: \"the\" → ↓0;d¦+ (0.9998)\n",
            "Token[32]: \"opium\" → ↓0;d¦+ (0.9967)\n",
            "Token[33]: \"crop\" → ↓0;d¦+ (0.9993)\n",
            "Token[34]: \"there\" → ↓0;d¦+ (0.9995)\n",
            "Token[35]: \"had\" → ↓0;d¦-+v+e+ (1.0)\n",
            "Token[36]: \"been\" → ↓0;d¦--+ (0.9995)\n",
            "Token[37]: \"overestimated\" → ↓0;d¦-+ (0.9922)\n",
            "Token[38]: \".\" → ↓0;d¦+ (1.0)\n",
            "Token[0]: \"If\" → ↓0;d¦+ (1.0)\n",
            "Token[1]: \"this\" → ↓0;d¦+ (0.9995)\n",
            "Token[2]: \"year\" → ↓0;d¦+ (0.9997)\n",
            "Token[3]: \"'\" → ↓0;d¦+ (0.969)\n",
            "Token[4]: \"s\" → ↓0;d¦+ (0.9825)\n",
            "Token[5]: \"poppy\" → ↓0;d¦+ (0.9971)\n",
            "Token[6]: \"fields\" → ↓0;d¦-+ (0.9989)\n",
            "Token[7]: \"are\" → ↓0;d--+b¦+ (0.9998)\n",
            "Token[8]: \"harvested\" → ↓0;d¦--+ (0.9915)\n",
            "Token[9]: \"without\" → ↓0;d¦+ (0.9997)\n",
            "Token[10]: \"disruption\" → ↓0;d¦+ (0.9992)\n",
            "Token[11]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[12]: \"the\" → ↓0;d¦+ (0.9999)\n",
            "Token[13]: \"country\" → ↓0;d¦+ (0.9773)\n",
            "Token[14]: \"would\" → ↓0;d¦+ (0.9998)\n",
            "Token[15]: \"likely\" → ↓0;d¦+ (1.0)\n",
            "Token[16]: \"regain\" → ↓0;d¦+ (0.9974)\n",
            "Token[17]: \"its\" → ↓0;d¦+ (0.8867)\n",
            "Token[18]: \"status\" → ↓0;d¦+ (0.8779)\n",
            "Token[19]: \"as\" → ↓0;d¦+ (0.9982)\n",
            "Token[20]: \"producer\" → ↓0;d¦+ (0.9955)\n",
            "Token[21]: \"of\" → ↓0;d¦+ (0.9999)\n",
            "Token[22]: \"90\" → ↓0;d¦+ (1.0)\n",
            "Token[23]: \"%\" → ↓0;d¦+ (0.9987)\n",
            "Token[24]: \"of\" → ↓0;d¦+ (1.0)\n",
            "Token[25]: \"the\" → ↓0;d¦+ (1.0)\n",
            "Token[26]: \"world\" → ↓0;d¦+ (0.9998)\n",
            "Token[27]: \"'\" → ↓0;d¦+ (0.9894)\n",
            "Token[28]: \"s\" → ↓0;d¦-+ (0.8567)\n",
            "Token[29]: \"opium\" → ↓0;d¦+ (0.9887)\n",
            "Token[30]: \".\" → ↓0;d¦+ (1.0)\n",
            "Token[0]: \"Afghanistan\" → ↑0¦↓1;d¦+ (0.9994)\n",
            "Token[1]: \"'\" → ↓0;d¦+ (0.5385)\n",
            "Token[2]: \"s\" → ↓0;d¦+ (0.8129)\n",
            "Token[3]: \"share\" → ↓0;d¦+ (0.9999)\n",
            "Token[4]: \"of\" → ↓0;d¦+ (0.9999)\n",
            "Token[5]: \"the\" → ↓0;d¦+ (1.0)\n",
            "Token[6]: \"deadly\" → ↓0;d¦+ (0.9986)\n",
            "Token[7]: \"market\" → ↓0;d¦+ (0.9952)\n",
            "Token[8]: \"slipped\" → ↓0;d¦---+ (0.9875)\n",
            "Token[9]: \"to\" → ↓0;d¦+ (1.0)\n",
            "Token[10]: \"around\" → ↓0;d¦+ (1.0)\n",
            "Token[11]: \"75\" → ↓0;d¦+ (0.9978)\n",
            "Token[12]: \"%\" → ↓0;d¦+ (0.9963)\n",
            "Token[13]: \"after\" → ↓0;d¦+ (0.9999)\n",
            "Token[14]: \"bad\" → ↓0;d¦+ (0.9993)\n",
            "Token[15]: \"weather\" → ↓0;d¦+ (0.6726)\n",
            "Token[16]: \"and\" → ↓0;d¦+ (0.9849)\n",
            "Token[17]: \"a\" → ↓0;d¦+ (1.0)\n",
            "Token[18]: \"blight\" → ↓0;d¦+ (0.9998)\n",
            "Token[19]: \"slashed\" → ↓0;d¦--+ (0.9874)\n",
            "Token[20]: \"production\" → ↓0;d¦+ (0.9975)\n",
            "Token[21]: \"over\" → ↓0;d¦+ (1.0)\n",
            "Token[22]: \"the\" → ↓0;d¦+ (1.0)\n",
            "Token[23]: \"past\" → ↓0;d¦+ (1.0)\n",
            "Token[24]: \"two\" → ↓0;d¦+ (1.0)\n",
            "Token[25]: \"years\" → ↓0;d¦-+ (0.9995)\n",
            "Token[26]: \".\" → ↓0;d¦+ (1.0)\n",
            "Token[0]: \"But\" → ↓0;d¦+ (1.0)\n",
            "Token[1]: \"the\" → ↓0;d¦+ (1.0)\n",
            "Token[2]: \"decline\" → ↓0;d¦+ (0.9997)\n",
            "Token[3]: \"in\" → ↓0;d¦+ (1.0)\n",
            "Token[4]: \"opium\" → ↓0;d¦+ (0.9153)\n",
            "Token[5]: \"production\" → ↓0;d¦+ (0.9012)\n",
            "Token[6]: \"also\" → ↓0;d¦+ (0.9997)\n",
            "Token[7]: \"drove\" → ↓0;d¦+ (0.3395)\n",
            "Token[8]: \"up\" → ↓0;d¦+ (0.9998)\n",
            "Token[9]: \"prices\" → ↓0;d¦-+ (0.9992)\n",
            "Token[10]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[11]: \"to\" → ↓0;d¦+ (1.0)\n",
            "Token[12]: \"a\" → ↓0;d¦+ (0.9998)\n",
            "Token[13]: \"record\" → ↓0;d¦+ (0.9999)\n",
            "Token[14]: \"$\" → ↓0;d¦+ (0.9999)\n",
            "Token[15]: \"300\" → ↓0;d¦+ (1.0)\n",
            "Token[16]: \"a\" → ↓0;d¦+ (0.9997)\n",
            "Token[17]: \"kilogramme\" → ↓0;d¦+ (0.9973)\n",
            "Token[18]: \".\" → ↓0;d¦+ (1.0)\n",
            "Token[0]: \"Prices\" → ↓0;d¦-+ (0.9986)\n",
            "Token[1]: \"have\" → ↓0;d¦+ (0.9948)\n",
            "Token[2]: \"now\" → ↓0;d¦+ (0.9998)\n",
            "Token[3]: \"slipped\" → ↓0;d¦---+ (0.9757)\n",
            "Token[4]: \"by\" → ↓0;d¦+ (1.0)\n",
            "Token[5]: \"over\" → ↓0;d¦+ (0.9957)\n",
            "Token[6]: \"$\" → ↓0;d¦+ (0.9998)\n",
            "Token[7]: \"100\" → ↓0;d¦+ (0.9999)\n",
            "Token[8]: \"but\" → ↓0;d¦+ (1.0)\n",
            "Token[9]: \"are\" → ↓0;d--+b¦+ (0.9999)\n",
            "Token[10]: \"still\" → ↓0;d¦+ (0.9978)\n",
            "Token[11]: \"far\" → ↓0;d¦+ (0.9971)\n",
            "Token[12]: \"above\" → ↓0;d¦+ (0.9999)\n",
            "Token[13]: \"historic\" → ↓0;d¦+ (0.9987)\n",
            "Token[14]: \"levels\" → ↓0;d¦-+ (0.9998)\n",
            "Token[15]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[16]: \"helping\" → ↓0;d¦---+ (0.9912)\n",
            "Token[17]: \"tempt\" → ↓0;d¦+ (0.8669)\n",
            "Token[18]: \"more\" → ↓0;d¦+ (0.9999)\n",
            "Token[19]: \"farmers\" → ↓0;d¦-+ (0.9992)\n",
            "Token[20]: \"to\" → ↓0;d¦+ (1.0)\n",
            "Token[21]: \"turn\" → ↓0;d¦+ (0.9999)\n",
            "Token[22]: \"land\" → ↓0;d¦+ (0.988)\n",
            "Token[23]: \"over\" → ↓0;d¦+ (0.9993)\n",
            "Token[24]: \"to\" → ↓0;d¦+ (1.0)\n",
            "Token[25]: \"poppy\" → ↓0;d¦+ (0.999)\n",
            "Token[26]: \".\" → ↓0;d¦+ (1.0)\n",
            "Token[0]: \"It\" → ↓0;d¦+ (1.0)\n",
            "Token[1]: \"seems\" → ↓0;d¦-+ (0.9992)\n",
            "Token[2]: \"unlikely\" → ↓0;d¦+ (0.9998)\n",
            "Token[3]: \"that\" → ↓0;d¦+ (0.9999)\n",
            "Token[4]: \"the\" → ↓0;d¦+ (0.995)\n",
            "Token[5]: \"poor\" → ↓0;d¦+ (0.9994)\n",
            "Token[6]: \"harvests\" → ↓0;d¦-+ (0.9999)\n",
            "Token[7]: \"of\" → ↓0;d¦+ (0.991)\n",
            "Token[8]: \"the\" → ↓0;d¦+ (1.0)\n",
            "Token[9]: \"last\" → ↓0;d¦+ (0.9999)\n",
            "Token[10]: \"year\" → ↓0;d¦+ (0.9965)\n",
            "Token[11]: \"will\" → ↓0;d¦+ (0.9999)\n",
            "Token[12]: \"be\" → ↓0;d¦+ (1.0)\n",
            "Token[13]: \"repeated\" → ↓0;d¦-+ (0.9153)\n",
            "Token[14]: \";\" → ↓0;d¦+ (1.0)\n",
            "Token[15]: \"there\" → ↓0;d¦+ (1.0)\n",
            "Token[16]: \"have\" → ↓0;d¦+ (0.9996)\n",
            "Token[17]: \"been\" → ↓0;d¦--+ (0.9776)\n",
            "Token[18]: \"no\" → ↓0;d¦+ (1.0)\n",
            "Token[19]: \"reports\" → ↓0;d¦-+ (0.9999)\n",
            "Token[20]: \"of\" → ↓0;d¦+ (0.9996)\n",
            "Token[21]: \"blight\" → ↓0;d¦+ (0.9992)\n",
            "Token[22]: \"and\" → ↓0;d¦+ (0.9995)\n",
            "Token[23]: \"the\" → ↓0;d¦+ (0.9999)\n",
            "Token[24]: \"exceptionally\" → ↓0;d¦+ (1.0)\n",
            "Token[25]: \"bitter\" → ↓0;d¦+ (0.9995)\n",
            "Token[26]: \"winter\" → ↓0;d¦+ (0.9989)\n",
            "Token[27]: \"of\" → ↓0;d¦+ (1.0)\n",
            "Token[28]: \"2011-12\" → ↓0;d¦+ (0.9988)\n",
            "Token[29]: \"was\" → ↓0;abe (0.9997)\n",
            "Token[30]: \"followed\" → ↓0;d¦--+ (0.9921)\n",
            "Token[31]: \"this\" → ↓0;d¦+ (1.0)\n",
            "Token[32]: \"year\" → ↓0;d¦+ (0.9789)\n",
            "Token[33]: \"by\" → ↓0;d¦+ (0.9999)\n",
            "Token[34]: \"a\" → ↓0;d¦+ (0.9999)\n",
            "Token[35]: \"milder\" → ↓0;d¦+ (0.9981)\n",
            "Token[36]: \"one\" → ↓0;d¦+ (0.9999)\n",
            "Token[37]: \",\" → ↓0;d¦+ (0.9999)\n",
            "Token[38]: \"creating\" → ↓0;d¦---+e+ (0.808)\n",
            "Token[39]: \"expectations\" → ↓0;d¦-+ (0.9997)\n",
            "Token[40]: \"of\" → ↓0;d¦+ (1.0)\n",
            "Token[41]: \"a\" → ↓0;d¦+ (1.0)\n",
            "Token[42]: \"large\" → ↓0;d¦+ (0.9999)\n",
            "Token[43]: \"crop\" → ↓0;d¦+ (0.9996)\n",
            "Token[44]: \".\" → ↓0;d¦+ (0.9999)\n",
            "Token[0]: \"The\" → ↓0;d¦+ (1.0)\n",
            "Token[1]: \"increase\" → ↓0;d¦+ (0.9999)\n",
            "Token[2]: \"has\" → ↓0;d¦-+v+e+ (1.0)\n",
            "Token[3]: \"come\" → ↓0;d¦+ (0.739)\n",
            "Token[4]: \"despite\" → ↓0;d¦+ (1.0)\n",
            "Token[5]: \"a\" → ↓0;d¦+ (1.0)\n",
            "Token[6]: \"marked\" → ↓0;d¦--+ (0.5368)\n",
            "Token[7]: \"improvement\" → ↓0;d¦+ (0.9998)\n",
            "Token[8]: \"in\" → ↓0;d¦+ (0.9999)\n",
            "Token[9]: \"Afghanistan\" → ↑0¦↓1;d¦+ (0.9995)\n",
            "Token[10]: \"'\" → ↓0;d¦+ (0.6782)\n",
            "Token[11]: \"s\" → ↓0;d¦-+ (0.8239)\n",
            "Token[12]: \"specialised\" → ↓0;d¦-+ (0.9406)\n",
            "Token[13]: \"counter\" → ↓0;d¦+ (0.9998)\n",
            "Token[14]: \"narcotics\" → ↓0;d¦+ (0.5307)\n",
            "Token[15]: \"units\" → ↓0;d¦-+ (0.9982)\n",
            "Token[16]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[17]: \"Lemahieu\" → ↑0¦↓1;d¦+ (0.9682)\n",
            "Token[18]: \"said\" → ↓0;d¦--+y+ (0.9995)\n",
            "Token[19]: \".\" → ↓0;d¦+ (1.0)\n",
            "Token[0]: \"Fear\" → ↓0;d¦+ (0.9981)\n",
            "Token[1]: \"of\" → ↓0;d¦+ (0.9999)\n",
            "Token[2]: \"eradication\" → ↓0;d¦+ (0.9967)\n",
            "Token[3]: \"has\" → ↓0;d¦-+v+e+ (1.0)\n",
            "Token[4]: \"become\" → ↓0;d¦+ (0.8763)\n",
            "Token[5]: \"a\" → ↓0;d¦+ (1.0)\n",
            "Token[6]: \"far\" → ↓0;d¦+ (0.9998)\n",
            "Token[7]: \"more\" → ↓0;d¦+ (1.0)\n",
            "Token[8]: \"significant\" → ↓0;d¦+ (0.9995)\n",
            "Token[9]: \"reason\" → ↓0;d¦+ (0.9617)\n",
            "Token[10]: \"for\" → ↓0;d¦+ (1.0)\n",
            "Token[11]: \"farmers\" → ↓0;d¦-+ (0.9985)\n",
            "Token[12]: \"to\" → ↓0;d¦+ (1.0)\n",
            "Token[13]: \"stick\" → ↓0;d¦+ (0.9993)\n",
            "Token[14]: \"to\" → ↓0;d¦+ (1.0)\n",
            "Token[15]: \"legal\" → ↓0;d¦+ (0.9988)\n",
            "Token[16]: \"crops\" → ↓0;d¦-+ (0.9994)\n",
            "Token[17]: \"than\" → ↓0;d¦+ (0.9988)\n",
            "Token[18]: \"in\" → ↓0;d¦+ (1.0)\n",
            "Token[19]: \"the\" → ↓0;d¦+ (1.0)\n",
            "Token[20]: \"past\" → ↓0;d¦+ (0.9987)\n",
            "Token[21]: \",\" → ↓0;d¦+ (0.9999)\n",
            "Token[22]: \"the\" → ↓0;d¦+ (1.0)\n",
            "Token[23]: \"report\" → ↓0;d¦+ (0.9994)\n",
            "Token[24]: \"found\" → ↓0;d¦-+ (0.4312)\n",
            "Token[25]: \".\" → ↓0;d¦+ (0.9999)\n",
            "Token[0]: \"But\" → ↓0;d¦+ (1.0)\n",
            "Token[1]: \"overall\" → ↓0;d¦+ (0.9998)\n",
            "Token[2]: \"the\" → ↓0;d¦+ (0.9999)\n",
            "Token[3]: \"government\" → ↓0;d¦+ (0.9463)\n",
            "Token[4]: \"and\" → ↓0;d¦+ (1.0)\n",
            "Token[5]: \"aid\" → ↓0;d¦+ (0.9965)\n",
            "Token[6]: \"community\" → ↓0;d¦+ (0.9405)\n",
            "Token[7]: \"has\" → ↓0;d¦-+v+e+ (0.9999)\n",
            "Token[8]: \"not\" → ↓0;d¦+ (0.9999)\n",
            "Token[9]: \"prioritised\" → ↓0;d¦-+ (0.93)\n",
            "Token[10]: \"efforts\" → ↓0;d¦-+ (0.9996)\n",
            "Token[11]: \"to\" → ↓0;d¦+ (1.0)\n",
            "Token[12]: \"cut\" → ↓0;d¦+ (0.9998)\n",
            "Token[13]: \"back\" → ↓0;d¦+ (0.9999)\n",
            "Token[14]: \"a\" → ↓0;d¦+ (0.9999)\n",
            "Token[15]: \"crop\" → ↓0;d¦+ (0.9999)\n",
            "Token[16]: \"and\" → ↓0;d¦+ (0.9999)\n",
            "Token[17]: \"trade\" → ↓0;d¦+ (0.9995)\n",
            "Token[18]: \"that\" → ↓0;d¦+ (1.0)\n",
            "Token[19]: \"feeds\" → ↓0;d¦-+ (0.9882)\n",
            "Token[20]: \"global\" → ↓0;d¦+ (0.9999)\n",
            "Token[21]: \"markets\" → ↓0;d¦-+ (0.9996)\n",
            "Token[22]: \"for\" → ↓0;d¦+ (1.0)\n",
            "Token[23]: \"heroin\" → ↓0;d¦+ (0.8986)\n",
            "Token[24]: \",\" → ↓0;d¦+ (0.9995)\n",
            "Token[25]: \"Lemahieu\" → ↑0¦↓1;d¦+ (0.9611)\n",
            "Token[26]: \"said\" → ↓0;d¦--+y+ (0.9997)\n",
            "Token[27]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[28]: \"despite\" → ↓0;d¦+ (0.9996)\n",
            "Token[29]: \"its\" → ↓0;d¦+ (0.9138)\n",
            "Token[30]: \"corrosive\" → ↓0;d¦+ (0.9998)\n",
            "Token[31]: \"effect\" → ↓0;d¦+ (0.9988)\n",
            "Token[32]: \"on\" → ↓0;d¦+ (0.9993)\n",
            "Token[33]: \"security\" → ↓0;d¦+ (0.9998)\n",
            "Token[34]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[35]: \"corruption\" → ↓0;d¦+ (0.9994)\n",
            "Token[36]: \"and\" → ↓0;d¦+ (0.9989)\n",
            "Token[37]: \"trust\" → ↓0;d¦+ (0.9984)\n",
            "Token[38]: \"in\" → ↓0;d¦+ (0.9999)\n",
            "Token[39]: \"Kabul\" → ↑0¦↓1;d¦+ (0.9996)\n",
            "Token[40]: \".\" → ↓0;d¦+ (1.0)\n",
            "Token[0]: \"Typical\" → ↓0;d¦+ (0.9996)\n",
            "Token[1]: \"of\" → ↓0;d¦+ (1.0)\n",
            "Token[2]: \"the\" → ↓0;d¦+ (0.9999)\n",
            "Token[3]: \"official\" → ↓0;d¦+ (0.9962)\n",
            "Token[4]: \"neglect\" → ↓0;d¦+ (0.9972)\n",
            "Token[5]: \"are\" → ↓0;d--+b¦+ (0.9996)\n",
            "Token[6]: \"the\" → ↓0;d¦+ (0.9999)\n",
            "Token[7]: \"22\" → ↓0;d¦+ (1.0)\n",
            "Token[8]: \"\"\" → ↓0;d¦+ (0.9921)\n",
            "Token[9]: \"national\" → ↓0;d¦+ (0.9994)\n",
            "Token[10]: \"priority\" → ↓0;d¦+ (0.9998)\n",
            "Token[11]: \"programmes\" → ↓0;d¦-+ (0.9995)\n",
            "Token[12]: \"\"\" → ↓0;d¦+ (0.9996)\n",
            "Token[13]: \"drawn\" → ↓0;d¦-+ (0.9973)\n",
            "Token[14]: \"up\" → ↓0;d¦+ (0.9981)\n",
            "Token[15]: \"by\" → ↓0;d¦+ (1.0)\n",
            "Token[16]: \"Kabul\" → ↑0¦↓1;d¦+ (0.9989)\n",
            "Token[17]: \"to\" → ↓0;d¦+ (1.0)\n",
            "Token[18]: \"focus\" → ↓0;d¦+ (0.9706)\n",
            "Token[19]: \"aid\" → ↓0;d¦+ (0.9974)\n",
            "Token[20]: \"money\" → ↓0;d¦+ (0.7525)\n",
            "Token[21]: \"and\" → ↓0;d¦+ (1.0)\n",
            "Token[22]: \"diplomatic\" → ↓0;d¦+ (0.9997)\n",
            "Token[23]: \"efforts\" → ↓0;d¦-+ (0.9994)\n",
            "Token[24]: \"on\" → ↓0;d¦+ (0.9999)\n",
            "Token[25]: \"its\" → ↓0;d¦+ (0.9844)\n",
            "Token[26]: \"key\" → ↓0;d¦+ (0.9997)\n",
            "Token[27]: \"development\" → ↓0;d¦+ (0.9989)\n",
            "Token[28]: \"concerns\" → ↓0;d¦-+ (0.9998)\n",
            "Token[29]: \"including\" → ↓0;d¦---+e+ (0.6968)\n",
            "Token[30]: \"justice\" → ↓0;d¦+ (0.9993)\n",
            "Token[31]: \"and\" → ↓0;d¦+ (1.0)\n",
            "Token[32]: \"education\" → ↓0;d¦+ (0.9941)\n",
            "Token[33]: \".\" → ↓0;d¦+ (1.0)\n",
            "Token[0]: \"Counter\" → ↓0;d¦+ (0.9999)\n",
            "Token[1]: \"narcotics\" → ↓0;d¦+ (0.6471)\n",
            "Token[2]: \"was\" → ↓0;abe (0.9998)\n",
            "Token[3]: \"not\" → ↓0;d¦+ (1.0)\n",
            "Token[4]: \"one\" → ↓0;d¦+ (0.9999)\n",
            "Token[5]: \"of\" → ↓0;d¦+ (1.0)\n",
            "Token[6]: \"them\" → ↓0;d¦-+y+ (0.9997)\n",
            "Token[7]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[8]: \"nor\" → ↓0;d¦+ (1.0)\n",
            "Token[9]: \"has\" → ↓0;d¦-+v+e+ (1.0)\n",
            "Token[10]: \"it\" → ↓0;d¦+ (0.9999)\n",
            "Token[11]: \"been\" → ↓0;d¦--+ (0.9989)\n",
            "Token[12]: \"put\" → ↓0;d¦+ (0.998)\n",
            "Token[13]: \"at\" → ↓0;d¦+ (1.0)\n",
            "Token[14]: \"the\" → ↓0;d¦+ (1.0)\n",
            "Token[15]: \"heart\" → ↓0;d¦+ (0.9986)\n",
            "Token[16]: \"of\" → ↓0;d¦+ (1.0)\n",
            "Token[17]: \"the\" → ↓0;d¦+ (0.9999)\n",
            "Token[18]: \"other\" → ↓0;d¦+ (0.9995)\n",
            "Token[19]: \"programmes\" → ↓0;d¦-+ (0.9993)\n",
            "Token[20]: \".\" → ↓0;d¦+ (1.0)\n",
            "Token[0]: \"\"\" → ↓0;d¦+ (1.0)\n",
            "Token[1]: \"We\" → ↓0;d¦+ (0.9999)\n",
            "Token[2]: \"need\" → ↓0;d¦+ (0.9983)\n",
            "Token[3]: \"to\" → ↓0;d¦+ (1.0)\n",
            "Token[4]: \"have\" → ↓0;d¦+ (1.0)\n",
            "Token[5]: \"counter\" → ↓0;d¦+ (0.9998)\n",
            "Token[6]: \"narcotics\" → ↓0;d¦-+ (0.8819)\n",
            "Token[7]: \"dealt\" → ↓0;d¦+ (0.4467)\n",
            "Token[8]: \"with\" → ↓0;d¦+ (0.9999)\n",
            "Token[9]: \"seriously\" → ↓0;d¦+ (1.0)\n",
            "Token[10]: \"by\" → ↓0;d¦+ (1.0)\n",
            "Token[11]: \"the\" → ↓0;d¦+ (1.0)\n",
            "Token[12]: \"entire\" → ↓0;d¦+ (0.9999)\n",
            "Token[13]: \"government\" → ↓0;d¦+ (0.9978)\n",
            "Token[14]: \"as\" → ↓0;d¦+ (1.0)\n",
            "Token[15]: \"well\" → ↓0;d¦+ (0.9999)\n",
            "Token[16]: \"as\" → ↓0;d¦+ (0.9265)\n",
            "Token[17]: \"the\" → ↓0;d¦+ (1.0)\n",
            "Token[18]: \"aid\" → ↓0;d¦+ (0.9992)\n",
            "Token[19]: \"community\" → ↓0;d¦+ (0.9671)\n",
            "Token[20]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[21]: \"\"\" → ↓0;d¦+ (0.9975)\n",
            "Token[22]: \"Lemahieu\" → ↑0¦↓1;d¦+ (0.975)\n",
            "Token[23]: \"said\" → ↓0;d¦--+y+ (0.9999)\n",
            "Token[24]: \".\" → ↓0;d¦+ (1.0)\n",
            "Token[0]: \"\"\" → ↓0;d¦+ (0.9907)\n",
            "Token[1]: \"One\" → ↓0;d¦+ (0.9999)\n",
            "Token[2]: \"of\" → ↓0;d¦+ (1.0)\n",
            "Token[3]: \"the\" → ↓0;d¦+ (0.9808)\n",
            "Token[4]: \"big\" → ↓0;d¦+ (0.9993)\n",
            "Token[5]: \"missing\" → ↓0;d¦---+ (0.6731)\n",
            "Token[6]: \"links\" → ↓0;d¦-+ (0.9989)\n",
            "Token[7]: \"here\" → ↓0;d¦+ (0.9998)\n",
            "Token[8]: \"is\" → ↓0;abe (1.0)\n",
            "Token[9]: \"providing\" → ↓0;d¦---+e+ (0.9207)\n",
            "Token[10]: \"for\" → ↓0;d¦+ (1.0)\n",
            "Token[11]: \"the\" → ↓0;d¦+ (1.0)\n",
            "Token[12]: \"communities\" → ↓0;d¦---+y+ (0.9698)\n",
            "Token[13]: \"themselves\" → ↓0;d¦-+ (0.8997)\n",
            "Token[14]: \".\" → ↓0;d¦+ (1.0)\n",
            "Token[15]: \"\"\" → ↓0;d¦+ (0.9999)\n",
            "Token[0]: \"Eradication\" → ↓0;d¦+ (0.9628)\n",
            "Token[1]: \"programmes\" → ↓0;d¦-+ (0.9998)\n",
            "Token[2]: \"that\" → ↓0;d¦+ (1.0)\n",
            "Token[3]: \"do\" → ↓0;d¦+ (0.9998)\n",
            "Token[4]: \"not\" → ↓0;d¦+ (0.9977)\n",
            "Token[5]: \"provide\" → ↓0;d¦+ (0.9995)\n",
            "Token[6]: \"farmers\" → ↓0;d¦-+ (0.9982)\n",
            "Token[7]: \"with\" → ↓0;d¦+ (1.0)\n",
            "Token[8]: \"benefits\" → ↓0;d¦-+ (0.9978)\n",
            "Token[9]: \"such\" → ↓0;d¦+ (0.9994)\n",
            "Token[10]: \"as\" → ↓0;d¦+ (0.9989)\n",
            "Token[11]: \"healthcare\" → ↓0;d¦+ (0.9989)\n",
            "Token[12]: \"and\" → ↓0;d¦+ (0.9584)\n",
            "Token[13]: \"education\" → ↓0;d¦+ (0.9974)\n",
            "Token[14]: \",\" → ↓0;d¦+ (0.9996)\n",
            "Token[15]: \"and\" → ↓0;d¦+ (0.9998)\n",
            "Token[16]: \"support\" → ↓0;d¦+ (0.9993)\n",
            "Token[17]: \"growing\" → ↓0;d¦---+ (0.9676)\n",
            "Token[18]: \"other\" → ↓0;d¦+ (1.0)\n",
            "Token[19]: \"crops\" → ↓0;d¦-+ (0.9995)\n",
            "Token[20]: \"will\" → ↓0;d¦+ (0.9999)\n",
            "Token[21]: \"just\" → ↓0;d¦+ (1.0)\n",
            "Token[22]: \"push\" → ↓0;d¦+ (0.9987)\n",
            "Token[23]: \"the\" → ↓0;d¦+ (1.0)\n",
            "Token[24]: \"Taliban\" → ↑0¦↓1;d¦+ (0.997)\n",
            "Token[25]: \"or\" → ↓0;d¦+ (1.0)\n",
            "Token[26]: \"other\" → ↓0;d¦+ (0.9993)\n",
            "Token[27]: \"insurgent\" → ↓0;d¦+ (0.9948)\n",
            "Token[28]: \"groups\" → ↓0;d¦-+ (0.9998)\n",
            "Token[29]: \"that\" → ↓0;d¦+ (1.0)\n",
            "Token[30]: \"do\" → ↓0;d¦+ (0.9999)\n",
            "Token[31]: \"tolerate\" → ↓0;d¦+ (0.9995)\n",
            "Token[32]: \"or\" → ↓0;d¦+ (1.0)\n",
            "Token[33]: \"encourage\" → ↓0;d¦+ (0.999)\n",
            "Token[34]: \"poppy\" → ↓0;d¦+ (0.9981)\n",
            "Token[35]: \"production\" → ↓0;d¦+ (0.9901)\n",
            "Token[36]: \",\" → ↓0;d¦+ (1.0)\n",
            "Token[37]: \"he\" → ↓0;d¦+ (0.9997)\n",
            "Token[38]: \"added\" → ↓0;d¦--+ (0.9849)\n",
            "Token[39]: \".\" → ↓0;d¦+ (1.0)\n",
            "Twelve\ttwelve\n",
            "years\tyear\n",
            "after\tafter\n",
            "the\tthe\n",
            "fall\tfall\n",
            "of\tof\n",
            "the\tthe\n",
            "Taliban\tTaliban\n",
            ",\t,\n",
            "Afghanistan\tAfghanistan\n",
            "is\tbe\n",
            "heading\thead\n",
            "for\tfor\n",
            "a\ta\n",
            "near\tnear\n",
            "record\trecord\n",
            "opium\topium\n",
            "crop\tcrop\n",
            "as\tas\n",
            "instability\tinstability\n",
            "pushes\tpushe\n",
            "up\tup\n",
            "the\tthe\n",
            "amount\tamount\n",
            "of\tof\n",
            "land\tland\n",
            "planted\tplant\n",
            "with\twith\n",
            "illegal\tillegal\n",
            "but\tbut\n",
            "lucrative\tlucrative\n",
            "poppies\tpoppy\n",
            ",\t,\n",
            "according\taccord\n",
            "to\tto\n",
            "a\ta\n",
            "bleak\tbleak\n",
            "UN\tUn\n",
            "report\treport\n",
            ".\t.\n",
            "\n",
            "The\tthe\n",
            "rapid\trapid\n",
            "growth\tgrowth\n",
            "of\tof\n",
            "poppy\tpoppy\n",
            "farming\tfarming\n",
            "as\tas\n",
            "western\twestern\n",
            "troops\ttroop\n",
            "head\thead\n",
            "home\thome\n",
            "reflects\treflect\n",
            "particularly\tparticularly\n",
            "badly\tbadly\n",
            "on\ton\n",
            "Britain\tBritain\n",
            ",\t,\n",
            "which\twhich\n",
            "was\tbe\n",
            "designated\tdesignate\n",
            "\"\t\"\n",
            "lead\tlead\n",
            "nation\tnation\n",
            "\"\t\"\n",
            "for\tfor\n",
            "counter\tcounter\n",
            "narcotics\tnarcotics\n",
            "work\twork\n",
            "over\tover\n",
            "a\ta\n",
            "decade\tdecade\n",
            "ago\tago\n",
            ".\t.\n",
            "\n",
            "\"\t\"\n",
            "Poppy\tPoppy\n",
            "cultivation\tcultivation\n",
            "is\tbe\n",
            "not\tnot\n",
            "only\tonly\n",
            "expected\texpect\n",
            "to\tto\n",
            "expand\texpand\n",
            "in\tin\n",
            "areas\tarea\n",
            "where\twhere\n",
            "it\tit\n",
            "already\talready\n",
            "existed\texist\n",
            "in\tin\n",
            "2012\t2012\n",
            "…\t…\n",
            "but\tbut\n",
            "also\talso\n",
            "in\tin\n",
            "new\tnew\n",
            "areas\tarea\n",
            "or\tor\n",
            "areas\tarea\n",
            "where\twhere\n",
            "poppy\tpoppy\n",
            "cultivation\tcultivation\n",
            "was\tbe\n",
            "stopped\tstop\n",
            ",\t,\n",
            "\"\t\"\n",
            "the\tthe\n",
            "Afghanistan\tAfghanistan\n",
            "Opium\tOpium\n",
            "Winter\tWinter\n",
            "Risk\tRisk\n",
            "Assessment\tAssessment\n",
            "found\tfind\n",
            ".\t.\n",
            "\n",
            "The\tthe\n",
            "growth\tgrowth\n",
            "in\tin\n",
            "opium\topium\n",
            "cultivation\tcultivation\n",
            "reflects\treflect\n",
            "both\tboth\n",
            "spreading\tspread\n",
            "instability\tinstability\n",
            "and\tand\n",
            "concerns\tconcern\n",
            "about\tabout\n",
            "the\tthe\n",
            "future\tfuture\n",
            ".\t.\n",
            "\n",
            "Farmers\tfarmer\n",
            "are\tbe\n",
            "more\tmore\n",
            "likely\tlikely\n",
            "to\tto\n",
            "plant\tplant\n",
            "the\tthe\n",
            "deadly\tdeadly\n",
            "crop\tcrop\n",
            "in\tin\n",
            "areas\tarea\n",
            "of\tof\n",
            "high\thigh\n",
            "violence\tviolence\n",
            "or\tor\n",
            "where\twhere\n",
            "they\tthey\n",
            "have\thave\n",
            "not\tnot\n",
            "received\treceive\n",
            "any\tany\n",
            "agricultural\tagricultural\n",
            "aid\taid\n",
            ",\t,\n",
            "the\tthe\n",
            "report\treport\n",
            "said\tsay\n",
            ".\t.\n",
            "\n",
            "Opium\topium\n",
            "traders\ttrader\n",
            "are\tbe\n",
            "often\toften\n",
            "happy\thappy\n",
            "to\tto\n",
            "provide\tprovide\n",
            "seeds\tseed\n",
            ",\t,\n",
            "fertilisers\tfertiliser\n",
            "and\tand\n",
            "even\teven\n",
            "advance\tadvance\n",
            "payments\tpayment\n",
            "to\tto\n",
            "encourage\tencourage\n",
            "crops\tcrop\n",
            ",\t,\n",
            "leaving\tleave\n",
            "farmers\tfarmer\n",
            "who\twho\n",
            "do\tdo\n",
            "not\tnot\n",
            "have\thave\n",
            "western\twestern\n",
            "or\tor\n",
            "government\tgovernment\n",
            "agricultural\tagricultural\n",
            "help\thelp\n",
            "very\tvery\n",
            "vulnerable\tvulnerable\n",
            "to\tto\n",
            "their\tthey\n",
            "inducements\tinducement\n",
            ".\t.\n",
            "\n",
            "At\tat\n",
            "the\tthe\n",
            "same\tsame\n",
            "time\ttime\n",
            "the\tthe\n",
            "more\tmore\n",
            "powerful\tpowerful\n",
            "figures\tfigure\n",
            "in\tin\n",
            "the\tthe\n",
            "drugs\tdrug\n",
            "trade\ttrade\n",
            ",\t,\n",
            "from\tfrom\n",
            "traffickers\ttrafficker\n",
            "to\tto\n",
            "corrupt\tcorrupt\n",
            "government\tgovernment\n",
            "officials\tofficial\n",
            ",\t,\n",
            "who\twho\n",
            "take\ttake\n",
            "over\tover\n",
            "half\thalf\n",
            "the\tthe\n",
            "profit\tprofit\n",
            "from\tfrom\n",
            "each\teach\n",
            "kilo\tkilo\n",
            "of\tof\n",
            "opium\topium\n",
            ",\t,\n",
            "have\thave\n",
            "shrinking\tshrink\n",
            "opportunities\topportunity\n",
            "to\tto\n",
            "earn\tearn\n",
            "money\tmoney\n",
            "from\tfrom\n",
            "Nato\tNato\n",
            "or\tor\n",
            "international\tinternational\n",
            "aid\taid\n",
            "contracts\tcontract\n",
            "–\t–\n",
            "and\tand\n",
            "may\tmay\n",
            "be\tbe\n",
            "preparing\tprepare\n",
            "a\ta\n",
            "war\twar\n",
            "chest\tchest\n",
            "for\tfor\n",
            "upcoming\tupcome\n",
            "presidential\tpresidential\n",
            "and\tand\n",
            "parliamentary\tparliamentary\n",
            "elections\telection\n",
            ".\t.\n",
            "\n",
            "\"\t\"\n",
            "Opium\tOpium\n",
            "cultivation\tcultivation\n",
            "is\tbe\n",
            "up\tup\n",
            "for\tfor\n",
            "the\tthe\n",
            "third\tthird\n",
            "successive\tsuccessive\n",
            "year\tyear\n",
            ",\t,\n",
            "and\tand\n",
            "production\tproduction\n",
            "is\tbe\n",
            "heading\thead\n",
            "towards\ttowards\n",
            "record\trecord\n",
            "levels\tlevel\n",
            ",\t,\n",
            "\"\t\"\n",
            "said\tsay\n",
            "Jean\tJean\n",
            "Luc\tLuc\n",
            "Lemahieu\tLemahieu\n",
            ",\t,\n",
            "Afghanistan\tAfghanistan\n",
            "head\thead\n",
            "of\tof\n",
            "the\tthe\n",
            "UN\tUn\n",
            "Office\tOffice\n",
            "on\ton\n",
            "Drugs\tDrugs\n",
            "and\tand\n",
            "Crime\tCrime\n",
            ".\t.\n",
            "\n",
            "\"\t\"\n",
            "People\tpeople\n",
            "are\tbe\n",
            "hedging\thedg\n",
            "against\tagainst\n",
            "an\ta\n",
            "insecure\tinsecure\n",
            "future\tfuture\n",
            "both\tboth\n",
            "politically\tpolitically\n",
            "and\tand\n",
            "economically\teconomically\n",
            ".\t.\n",
            "\"\t\"\n",
            "\n",
            "Just\tjust\n",
            "14\t14\n",
            "of\tof\n",
            "Afghanistan\tAfghanistan\n",
            "'\t'\n",
            "s\t\n",
            "34\t34\n",
            "provinces\tprovince\n",
            "are\tbe\n",
            "now\tnow\n",
            "\"\t\"\n",
            "poppy\tpoppy\n",
            "free\tfree\n",
            "\"\t\"\n",
            ",\t,\n",
            "down\tdown\n",
            "from\tfrom\n",
            "20\t20\n",
            "in\tin\n",
            "2010\t2010\n",
            ".\t.\n",
            "\n",
            "In\tin\n",
            "three\tthree\n",
            "provinces\tprovince\n",
            ",\t,\n",
            "the\tthe\n",
            "spring\tspring\n",
            "sowing\tsowing\n",
            "was\tbe\n",
            "the\tthe\n",
            "first\tfirst\n",
            "time\ttime\n",
            "this\tthis\n",
            "decade\tdecade\n",
            "that\tthat\n",
            "farmers\tfarmer\n",
            "had\thave\n",
            "risked\trisk\n",
            "an\ta\n",
            "attempt\tattempt\n",
            "at\tat\n",
            "growing\tgrow\n",
            "opium\topium\n",
            ".\t.\n",
            "\n",
            "The\tthe\n",
            "only\tonly\n",
            "figures\tfigure\n",
            "showing\tshow\n",
            "a\ta\n",
            "fall\tfall\n",
            "in\tin\n",
            "cultivation\tcultivation\n",
            ",\t,\n",
            "for\tfor\n",
            "western\twestern\n",
            "Herat\tHerat\n",
            "province\tprovince\n",
            ",\t,\n",
            "may\tmay\n",
            "actually\tactually\n",
            "be\tbe\n",
            "due\tdue\n",
            "to\tto\n",
            "a\ta\n",
            "statistics\tstatistic\n",
            "blip\tblip\n",
            ".\t.\n",
            "\n",
            "The\tthe\n",
            "UN\tUn\n",
            "was\tbe\n",
            "forced\tforce\n",
            "to\tto\n",
            "use\tuse\n",
            "external\texternal\n",
            "data\tdata\n",
            "last\tlast\n",
            "year\tyear\n",
            "instead\tinstead\n",
            "of\tof\n",
            "the\tthe\n",
            "satellite\tsatellite\n",
            "images\timage\n",
            "that\tthat\n",
            "are\tbe\n",
            "usually\tusually\n",
            "the\tthe\n",
            "basis\tbasis\n",
            "of\tof\n",
            "poppy\tpoppy\n",
            "growing\tgrowing\n",
            "calculations\tcalculation\n",
            ",\t,\n",
            "and\tand\n",
            "local\tlocal\n",
            "officials\tofficial\n",
            "protested\tprotest\n",
            "heavily\theavily\n",
            "that\tthat\n",
            "the\tthe\n",
            "opium\topium\n",
            "crop\tcrop\n",
            "there\tthere\n",
            "had\thave\n",
            "been\tbe\n",
            "overestimated\toverestimate\n",
            ".\t.\n",
            "\n",
            "If\tif\n",
            "this\tthis\n",
            "year\tyear\n",
            "'\t'\n",
            "s\ts\n",
            "poppy\tpoppy\n",
            "fields\tfield\n",
            "are\tbe\n",
            "harvested\tharvest\n",
            "without\twithout\n",
            "disruption\tdisruption\n",
            ",\t,\n",
            "the\tthe\n",
            "country\tcountry\n",
            "would\twould\n",
            "likely\tlikely\n",
            "regain\tregain\n",
            "its\tits\n",
            "status\tstatus\n",
            "as\tas\n",
            "producer\tproducer\n",
            "of\tof\n",
            "90\t90\n",
            "%\t%\n",
            "of\tof\n",
            "the\tthe\n",
            "world\tworld\n",
            "'\t'\n",
            "s\t\n",
            "opium\topium\n",
            ".\t.\n",
            "\n",
            "Afghanistan\tAfghanistan\n",
            "'\t'\n",
            "s\ts\n",
            "share\tshare\n",
            "of\tof\n",
            "the\tthe\n",
            "deadly\tdeadly\n",
            "market\tmarket\n",
            "slipped\tslip\n",
            "to\tto\n",
            "around\taround\n",
            "75\t75\n",
            "%\t%\n",
            "after\tafter\n",
            "bad\tbad\n",
            "weather\tweather\n",
            "and\tand\n",
            "a\ta\n",
            "blight\tblight\n",
            "slashed\tslash\n",
            "production\tproduction\n",
            "over\tover\n",
            "the\tthe\n",
            "past\tpast\n",
            "two\ttwo\n",
            "years\tyear\n",
            ".\t.\n",
            "\n",
            "But\tbut\n",
            "the\tthe\n",
            "decline\tdecline\n",
            "in\tin\n",
            "opium\topium\n",
            "production\tproduction\n",
            "also\talso\n",
            "drove\tdrove\n",
            "up\tup\n",
            "prices\tprice\n",
            ",\t,\n",
            "to\tto\n",
            "a\ta\n",
            "record\trecord\n",
            "$\t$\n",
            "300\t300\n",
            "a\ta\n",
            "kilogramme\tkilogramme\n",
            ".\t.\n",
            "\n",
            "Prices\tprice\n",
            "have\thave\n",
            "now\tnow\n",
            "slipped\tslip\n",
            "by\tby\n",
            "over\tover\n",
            "$\t$\n",
            "100\t100\n",
            "but\tbut\n",
            "are\tbe\n",
            "still\tstill\n",
            "far\tfar\n",
            "above\tabove\n",
            "historic\thistoric\n",
            "levels\tlevel\n",
            ",\t,\n",
            "helping\thelp\n",
            "tempt\ttempt\n",
            "more\tmore\n",
            "farmers\tfarmer\n",
            "to\tto\n",
            "turn\tturn\n",
            "land\tland\n",
            "over\tover\n",
            "to\tto\n",
            "poppy\tpoppy\n",
            ".\t.\n",
            "\n",
            "It\tit\n",
            "seems\tseem\n",
            "unlikely\tunlikely\n",
            "that\tthat\n",
            "the\tthe\n",
            "poor\tpoor\n",
            "harvests\tharvest\n",
            "of\tof\n",
            "the\tthe\n",
            "last\tlast\n",
            "year\tyear\n",
            "will\twill\n",
            "be\tbe\n",
            "repeated\trepeate\n",
            ";\t;\n",
            "there\tthere\n",
            "have\thave\n",
            "been\tbe\n",
            "no\tno\n",
            "reports\treport\n",
            "of\tof\n",
            "blight\tblight\n",
            "and\tand\n",
            "the\tthe\n",
            "exceptionally\texceptionally\n",
            "bitter\tbitter\n",
            "winter\twinter\n",
            "of\tof\n",
            "2011-12\t2011-12\n",
            "was\tbe\n",
            "followed\tfollow\n",
            "this\tthis\n",
            "year\tyear\n",
            "by\tby\n",
            "a\ta\n",
            "milder\tmilder\n",
            "one\tone\n",
            ",\t,\n",
            "creating\tcreate\n",
            "expectations\texpectation\n",
            "of\tof\n",
            "a\ta\n",
            "large\tlarge\n",
            "crop\tcrop\n",
            ".\t.\n",
            "\n",
            "The\tthe\n",
            "increase\tincrease\n",
            "has\thave\n",
            "come\tcome\n",
            "despite\tdespite\n",
            "a\ta\n",
            "marked\tmark\n",
            "improvement\timprovement\n",
            "in\tin\n",
            "Afghanistan\tAfghanistan\n",
            "'\t'\n",
            "s\t\n",
            "specialised\tspecialise\n",
            "counter\tcounter\n",
            "narcotics\tnarcotics\n",
            "units\tunit\n",
            ",\t,\n",
            "Lemahieu\tLemahieu\n",
            "said\tsay\n",
            ".\t.\n",
            "\n",
            "Fear\tfear\n",
            "of\tof\n",
            "eradication\teradication\n",
            "has\thave\n",
            "become\tbecome\n",
            "a\ta\n",
            "far\tfar\n",
            "more\tmore\n",
            "significant\tsignificant\n",
            "reason\treason\n",
            "for\tfor\n",
            "farmers\tfarmer\n",
            "to\tto\n",
            "stick\tstick\n",
            "to\tto\n",
            "legal\tlegal\n",
            "crops\tcrop\n",
            "than\tthan\n",
            "in\tin\n",
            "the\tthe\n",
            "past\tpast\n",
            ",\t,\n",
            "the\tthe\n",
            "report\treport\n",
            "found\tfoun\n",
            ".\t.\n",
            "\n",
            "But\tbut\n",
            "overall\toverall\n",
            "the\tthe\n",
            "government\tgovernment\n",
            "and\tand\n",
            "aid\taid\n",
            "community\tcommunity\n",
            "has\thave\n",
            "not\tnot\n",
            "prioritised\tprioritise\n",
            "efforts\teffort\n",
            "to\tto\n",
            "cut\tcut\n",
            "back\tback\n",
            "a\ta\n",
            "crop\tcrop\n",
            "and\tand\n",
            "trade\ttrade\n",
            "that\tthat\n",
            "feeds\tfeed\n",
            "global\tglobal\n",
            "markets\tmarket\n",
            "for\tfor\n",
            "heroin\theroin\n",
            ",\t,\n",
            "Lemahieu\tLemahieu\n",
            "said\tsay\n",
            ",\t,\n",
            "despite\tdespite\n",
            "its\tits\n",
            "corrosive\tcorrosive\n",
            "effect\teffect\n",
            "on\ton\n",
            "security\tsecurity\n",
            ",\t,\n",
            "corruption\tcorruption\n",
            "and\tand\n",
            "trust\ttrust\n",
            "in\tin\n",
            "Kabul\tKabul\n",
            ".\t.\n",
            "\n",
            "Typical\ttypical\n",
            "of\tof\n",
            "the\tthe\n",
            "official\tofficial\n",
            "neglect\tneglect\n",
            "are\tbe\n",
            "the\tthe\n",
            "22\t22\n",
            "\"\t\"\n",
            "national\tnational\n",
            "priority\tpriority\n",
            "programmes\tprogramme\n",
            "\"\t\"\n",
            "drawn\tdraw\n",
            "up\tup\n",
            "by\tby\n",
            "Kabul\tKabul\n",
            "to\tto\n",
            "focus\tfocus\n",
            "aid\taid\n",
            "money\tmoney\n",
            "and\tand\n",
            "diplomatic\tdiplomatic\n",
            "efforts\teffort\n",
            "on\ton\n",
            "its\tits\n",
            "key\tkey\n",
            "development\tdevelopment\n",
            "concerns\tconcern\n",
            "including\tinclude\n",
            "justice\tjustice\n",
            "and\tand\n",
            "education\teducation\n",
            ".\t.\n",
            "\n",
            "Counter\tcounter\n",
            "narcotics\tnarcotics\n",
            "was\tbe\n",
            "not\tnot\n",
            "one\tone\n",
            "of\tof\n",
            "them\tthey\n",
            ",\t,\n",
            "nor\tnor\n",
            "has\thave\n",
            "it\tit\n",
            "been\tbe\n",
            "put\tput\n",
            "at\tat\n",
            "the\tthe\n",
            "heart\theart\n",
            "of\tof\n",
            "the\tthe\n",
            "other\tother\n",
            "programmes\tprogramme\n",
            ".\t.\n",
            "\n",
            "\"\t\"\n",
            "We\twe\n",
            "need\tneed\n",
            "to\tto\n",
            "have\thave\n",
            "counter\tcounter\n",
            "narcotics\tnarcotic\n",
            "dealt\tdealt\n",
            "with\twith\n",
            "seriously\tseriously\n",
            "by\tby\n",
            "the\tthe\n",
            "entire\tentire\n",
            "government\tgovernment\n",
            "as\tas\n",
            "well\twell\n",
            "as\tas\n",
            "the\tthe\n",
            "aid\taid\n",
            "community\tcommunity\n",
            ",\t,\n",
            "\"\t\"\n",
            "Lemahieu\tLemahieu\n",
            "said\tsay\n",
            ".\t.\n",
            "\n",
            "\"\t\"\n",
            "One\tone\n",
            "of\tof\n",
            "the\tthe\n",
            "big\tbig\n",
            "missing\tmiss\n",
            "links\tlink\n",
            "here\there\n",
            "is\tbe\n",
            "providing\tprovide\n",
            "for\tfor\n",
            "the\tthe\n",
            "communities\tcommunity\n",
            "themselves\tthemselve\n",
            ".\t.\n",
            "\"\t\"\n",
            "\n",
            "Eradication\teradication\n",
            "programmes\tprogramme\n",
            "that\tthat\n",
            "do\tdo\n",
            "not\tnot\n",
            "provide\tprovide\n",
            "farmers\tfarmer\n",
            "with\twith\n",
            "benefits\tbenefit\n",
            "such\tsuch\n",
            "as\tas\n",
            "healthcare\thealthcare\n",
            "and\tand\n",
            "education\teducation\n",
            ",\t,\n",
            "and\tand\n",
            "support\tsupport\n",
            "growing\tgrow\n",
            "other\tother\n",
            "crops\tcrop\n",
            "will\twill\n",
            "just\tjust\n",
            "push\tpush\n",
            "the\tthe\n",
            "Taliban\tTaliban\n",
            "or\tor\n",
            "other\tother\n",
            "insurgent\tinsurgent\n",
            "groups\tgroup\n",
            "that\tthat\n",
            "do\tdo\n",
            "tolerate\ttolerate\n",
            "or\tor\n",
            "encourage\tencourage\n",
            "poppy\tpoppy\n",
            "production\tproduction\n",
            ",\t,\n",
            "he\the\n",
            "added\tadd\n",
            ".\t.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BONUS ASSIGNMENT 4 \n",
        "\n",
        "+ TODO Obtain manually the distance between two strings using the Levenshtein distance algorithm we presented in class for the following two words:\n",
        "\n",
        "````\n",
        "mellifluous -> mellow\n",
        "````\n",
        "\n",
        "You can write the matrix here using markdown."
      ],
      "metadata": {
        "id": "E5Bvh5AcOCfi"
      }
    }
  ]
}